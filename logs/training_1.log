2025-10-21 14:37:02,400 - INFO - 使用设备: cpu
2025-10-21 14:37:02,400 - INFO - 加载数据...
2025-10-21 14:37:02,400 - INFO - 加载训练数据...
2025-10-21 14:37:06,884 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-10-21 14:37:06,885 - INFO - 训练数据结构:
2025-10-21 14:37:06,885 - INFO - 数据结构分析:
2025-10-21 14:37:06,885 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:37:06,885 - INFO - 
英语相关语言对:
2025-10-21 14:37:06,885 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:37:06,885 - INFO -   en->de: 206112 个样本
2025-10-21 14:37:06,885 - INFO - 
德语相关语言对:
2025-10-21 14:37:06,885 - INFO - 德语->其他语言: ['en']
2025-10-21 14:37:06,885 - INFO -   de->en: 206112 个样本
2025-10-21 14:37:06,885 - INFO - 提取 en->de 的翻译对
2025-10-21 14:37:06,886 - INFO - 找到 en->de: 206112 个样本
2025-10-21 14:37:06,942 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 14:37:06,942 - INFO - 加载验证数据...
2025-10-21 14:37:06,990 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-10-21 14:37:06,990 - INFO - 提取 en->de 的翻译对
2025-10-21 14:37:06,990 - INFO - 找到 en->de: 888 个样本
2025-10-21 14:37:06,991 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 14:37:06,991 - INFO - 加载测试数据...
2025-10-21 14:37:07,095 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-10-21 14:37:07,095 - INFO - 提取 en->de 的翻译对
2025-10-21 14:37:07,095 - INFO - 找到 en->de: 8079 个样本
2025-10-21 14:37:07,098 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 14:37:07,099 - INFO - 开始构建分词器...
2025-10-21 14:37:07,158 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 14:37:08,553 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 14:37:11,311 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 14:37:11,356 - INFO - 数据加载器创建完成
2025-10-21 14:37:11,356 - INFO - 训练集: 206112 个样本
2025-10-21 14:37:11,356 - INFO - 验证集: 888 个样本
2025-10-21 14:37:11,356 - INFO - 测试集: 8079 个样本
2025-10-21 14:37:12,355 - INFO - 源语言词汇表大小: 5000
2025-10-21 14:37:12,356 - INFO - 目标语言词汇表大小: 5000
2025-10-21 14:37:12,356 - INFO - 创建模型...
2025-10-21 14:37:12,490 - INFO - 总参数数: 15,041,416
2025-10-21 14:37:12,490 - INFO - 可训练参数数: 15,041,416
2025-10-21 14:37:12,492 - INFO - 开始训练...
2025-10-21 14:37:29,876 - INFO - Batch 0, Loss: 8.7017, LR: 0.000000
2025-10-21 14:40:03,862 - INFO - Batch 10, Loss: 8.7065, LR: 0.000000
2025-10-21 14:42:34,415 - INFO - Batch 20, Loss: 8.6847, LR: 0.000000
2025-10-21 14:45:04,825 - INFO - Batch 30, Loss: 8.6975, LR: 0.000000
2025-10-21 14:46:32,651 - INFO - 使用设备: cpu
2025-10-21 14:46:32,651 - INFO - 加载数据...
2025-10-21 14:46:32,651 - INFO - 加载训练数据...
2025-10-21 14:46:54,988 - INFO - 使用设备: cpu
2025-10-21 14:46:54,988 - INFO - 加载数据...
2025-10-21 14:46:54,988 - INFO - 加载训练数据...
2025-10-21 14:46:59,420 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-10-21 14:46:59,420 - INFO - 训练数据结构:
2025-10-21 14:46:59,420 - INFO - 数据结构分析:
2025-10-21 14:46:59,420 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:46:59,420 - INFO - 
英语相关语言对:
2025-10-21 14:46:59,420 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:46:59,421 - INFO -   en->de: 206112 个样本
2025-10-21 14:46:59,421 - INFO - 
德语相关语言对:
2025-10-21 14:46:59,421 - INFO - 德语->其他语言: ['en']
2025-10-21 14:46:59,421 - INFO -   de->en: 206112 个样本
2025-10-21 14:46:59,421 - INFO - 提取 en->de 的翻译对
2025-10-21 14:46:59,421 - INFO - 找到 en->de: 206112 个样本
2025-10-21 14:46:59,476 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 14:46:59,476 - INFO - 加载验证数据...
2025-10-21 14:46:59,521 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-10-21 14:46:59,522 - INFO - 提取 en->de 的翻译对
2025-10-21 14:46:59,522 - INFO - 找到 en->de: 888 个样本
2025-10-21 14:46:59,522 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 14:46:59,522 - INFO - 加载测试数据...
2025-10-21 14:46:59,624 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-10-21 14:46:59,624 - INFO - 提取 en->de 的翻译对
2025-10-21 14:46:59,624 - INFO - 找到 en->de: 8079 个样本
2025-10-21 14:46:59,627 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 14:46:59,627 - INFO - 开始构建分词器...
2025-10-21 14:46:59,683 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 14:47:00,959 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 14:47:03,041 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 14:47:03,079 - INFO - 数据加载器创建完成
2025-10-21 14:47:03,080 - INFO - 训练集: 206112 个样本
2025-10-21 14:47:03,080 - INFO - 验证集: 888 个样本
2025-10-21 14:47:03,080 - INFO - 测试集: 8079 个样本
2025-10-21 14:47:04,061 - INFO - 源语言词汇表大小: 5000
2025-10-21 14:47:04,061 - INFO - 目标语言词汇表大小: 5000
2025-10-21 14:47:04,061 - INFO - 创建模型...
2025-10-21 14:47:04,201 - INFO - 总参数数: 15,041,416
2025-10-21 14:47:04,201 - INFO - 可训练参数数: 15,041,416
2025-10-21 14:47:04,202 - INFO - 开始训练...
2025-10-21 14:47:19,969 - INFO - Batch 0, Loss: 8.7017, LR: 0.000000
2025-10-21 14:49:52,055 - INFO - Batch 10, Loss: 8.7059, LR: 0.000000
2025-10-21 14:52:26,063 - INFO - 使用设备: cpu
2025-10-21 14:52:26,063 - INFO - 加载数据...
2025-10-21 14:52:26,063 - INFO - 加载训练数据...
2025-10-21 14:52:39,786 - INFO - 使用设备: cpu
2025-10-21 14:52:39,786 - INFO - 加载数据...
2025-10-21 14:52:39,786 - INFO - 加载训练数据...
2025-10-21 14:52:44,228 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-10-21 14:52:44,228 - INFO - 训练数据结构:
2025-10-21 14:52:44,228 - INFO - 数据结构分析:
2025-10-21 14:52:44,228 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:52:44,228 - INFO - 
英语相关语言对:
2025-10-21 14:52:44,228 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:52:44,229 - INFO -   en->de: 206112 个样本
2025-10-21 14:52:44,229 - INFO - 
德语相关语言对:
2025-10-21 14:52:44,229 - INFO - 德语->其他语言: ['en']
2025-10-21 14:52:44,229 - INFO -   de->en: 206112 个样本
2025-10-21 14:52:44,229 - INFO - 提取 en->de 的翻译对
2025-10-21 14:52:44,229 - INFO - 找到 en->de: 206112 个样本
2025-10-21 14:52:44,285 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 14:52:44,286 - INFO - 加载验证数据...
2025-10-21 14:52:44,331 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-10-21 14:52:44,331 - INFO - 提取 en->de 的翻译对
2025-10-21 14:52:44,331 - INFO - 找到 en->de: 888 个样本
2025-10-21 14:52:44,331 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 14:52:44,331 - INFO - 加载测试数据...
2025-10-21 14:52:44,435 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-10-21 14:52:44,435 - INFO - 提取 en->de 的翻译对
2025-10-21 14:52:44,435 - INFO - 找到 en->de: 8079 个样本
2025-10-21 14:52:44,438 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 14:52:44,438 - INFO - 开始构建分词器...
2025-10-21 14:52:44,497 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 14:52:56,941 - INFO - 使用设备: cpu
2025-10-21 14:52:56,942 - INFO - 加载数据...
2025-10-21 14:52:56,942 - INFO - 加载训练数据...
2025-10-21 14:53:01,437 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-10-21 14:53:01,437 - INFO - 训练数据结构:
2025-10-21 14:53:01,437 - INFO - 数据结构分析:
2025-10-21 14:53:01,437 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:53:01,437 - INFO - 
英语相关语言对:
2025-10-21 14:53:01,437 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:53:01,437 - INFO -   en->de: 206112 个样本
2025-10-21 14:53:01,437 - INFO - 
德语相关语言对:
2025-10-21 14:53:01,438 - INFO - 德语->其他语言: ['en']
2025-10-21 14:53:01,438 - INFO -   de->en: 206112 个样本
2025-10-21 14:53:01,438 - INFO - 提取 en->de 的翻译对
2025-10-21 14:53:01,438 - INFO - 找到 en->de: 206112 个样本
2025-10-21 14:53:01,492 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 14:53:01,493 - INFO - 加载验证数据...
2025-10-21 14:53:01,538 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-10-21 14:53:01,538 - INFO - 提取 en->de 的翻译对
2025-10-21 14:53:01,538 - INFO - 找到 en->de: 888 个样本
2025-10-21 14:53:01,538 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 14:53:01,538 - INFO - 加载测试数据...
2025-10-21 14:53:01,641 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-10-21 14:53:01,641 - INFO - 提取 en->de 的翻译对
2025-10-21 14:53:01,641 - INFO - 找到 en->de: 8079 个样本
2025-10-21 14:53:01,644 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 14:53:01,644 - INFO - 开始构建分词器...
2025-10-21 14:53:01,689 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 14:53:02,897 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 14:53:05,005 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 14:53:05,047 - INFO - 数据加载器创建完成
2025-10-21 14:53:05,047 - INFO - 训练集: 206112 个样本
2025-10-21 14:53:05,047 - INFO - 验证集: 888 个样本
2025-10-21 14:53:05,047 - INFO - 测试集: 8079 个样本
2025-10-21 14:53:06,047 - INFO - 源语言词汇表大小: 5000
2025-10-21 14:53:06,047 - INFO - 目标语言词汇表大小: 5000
2025-10-21 14:53:06,047 - INFO - 创建模型...
2025-10-21 14:53:06,181 - INFO - 总参数数: 15,041,416
2025-10-21 14:53:06,183 - INFO - 可训练参数数: 15,041,416
2025-10-21 14:53:06,184 - INFO - 开始训练...
2025-10-21 14:53:19,798 - INFO - Batch 0, Loss: 8.6371, LR: 0.000000
2025-10-21 14:55:51,825 - INFO - Batch 10, Loss: 8.6392, LR: 0.000000
2025-10-21 14:57:23,788 - INFO - 使用设备: cpu
2025-10-21 14:57:23,788 - INFO - 加载数据...
2025-10-21 14:57:23,788 - INFO - 加载训练数据...
2025-10-21 14:57:28,211 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-10-21 14:57:28,211 - INFO - 训练数据结构:
2025-10-21 14:57:28,212 - INFO - 数据结构分析:
2025-10-21 14:57:28,212 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:57:28,212 - INFO - 
英语相关语言对:
2025-10-21 14:57:28,212 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:57:28,212 - INFO -   en->de: 206112 个样本
2025-10-21 14:57:28,212 - INFO - 
德语相关语言对:
2025-10-21 14:57:28,212 - INFO - 德语->其他语言: ['en']
2025-10-21 14:57:28,212 - INFO -   de->en: 206112 个样本
2025-10-21 14:57:28,212 - INFO - 提取 en->de 的翻译对
2025-10-21 14:57:28,212 - INFO - 找到 en->de: 206112 个样本
2025-10-21 14:57:28,269 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 14:57:28,269 - INFO - 加载验证数据...
2025-10-21 14:57:28,314 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-10-21 14:57:28,314 - INFO - 提取 en->de 的翻译对
2025-10-21 14:57:28,314 - INFO - 找到 en->de: 888 个样本
2025-10-21 14:57:28,315 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 14:57:28,315 - INFO - 加载测试数据...
2025-10-21 14:57:28,417 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-10-21 14:57:28,417 - INFO - 提取 en->de 的翻译对
2025-10-21 14:57:28,417 - INFO - 找到 en->de: 8079 个样本
2025-10-21 14:57:28,420 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 14:57:28,420 - INFO - 开始构建分词器...
2025-10-21 14:57:28,475 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 14:57:29,707 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 14:57:31,740 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 14:57:31,778 - INFO - 数据加载器创建完成
2025-10-21 14:57:31,778 - INFO - 训练集: 206112 个样本
2025-10-21 14:57:31,778 - INFO - 验证集: 888 个样本
2025-10-21 14:57:31,778 - INFO - 测试集: 8079 个样本
2025-10-21 14:57:32,759 - INFO - 源语言词汇表大小: 5000
2025-10-21 14:57:32,760 - INFO - 目标语言词汇表大小: 5000
2025-10-21 14:57:32,760 - INFO - 创建模型...
2025-10-21 14:57:32,898 - INFO - 总参数数: 15,041,416
2025-10-21 14:57:32,899 - INFO - 可训练参数数: 15,041,416
2025-10-21 14:57:32,900 - INFO - 开始训练...
2025-10-21 14:57:49,176 - INFO - Batch 0, Loss: 8.5819, LR: 0.000000
2025-10-21 15:12:03,336 - INFO - 使用设备: cpu
2025-10-21 15:12:03,336 - INFO - 加载数据...
2025-10-21 15:12:03,336 - INFO - 加载训练数据...
2025-10-21 15:12:07,808 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-10-21 15:12:07,809 - INFO - 训练数据结构:
2025-10-21 15:12:07,809 - INFO - 数据结构分析:
2025-10-21 15:12:07,809 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 15:12:07,809 - INFO - 
英语相关语言对:
2025-10-21 15:12:07,809 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 15:12:07,809 - INFO -   en->de: 206112 个样本
2025-10-21 15:12:07,809 - INFO - 
德语相关语言对:
2025-10-21 15:12:07,809 - INFO - 德语->其他语言: ['en']
2025-10-21 15:12:07,809 - INFO -   de->en: 206112 个样本
2025-10-21 15:12:07,809 - INFO - 提取 en->de 的翻译对
2025-10-21 15:12:07,809 - INFO - 找到 en->de: 206112 个样本
2025-10-21 15:12:07,865 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 15:12:07,865 - INFO - 加载验证数据...
2025-10-21 15:12:07,909 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-10-21 15:12:07,910 - INFO - 提取 en->de 的翻译对
2025-10-21 15:12:07,910 - INFO - 找到 en->de: 888 个样本
2025-10-21 15:12:07,910 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 15:12:07,910 - INFO - 加载测试数据...
2025-10-21 15:12:08,011 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-10-21 15:12:08,011 - INFO - 提取 en->de 的翻译对
2025-10-21 15:12:08,011 - INFO - 找到 en->de: 8079 个样本
2025-10-21 15:12:08,014 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 15:12:08,015 - INFO - 开始构建分词器...
2025-10-21 15:12:08,067 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 15:12:09,330 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 15:12:11,376 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 15:12:11,415 - INFO - 数据加载器创建完成
2025-10-21 15:12:11,415 - INFO - 训练集: 206112 个样本
2025-10-21 15:12:11,416 - INFO - 验证集: 888 个样本
2025-10-21 15:12:11,416 - INFO - 测试集: 8079 个样本
2025-10-21 15:12:12,398 - INFO - 源语言词汇表大小: 5000
2025-10-21 15:12:12,398 - INFO - 目标语言词汇表大小: 5000
2025-10-21 15:12:12,398 - INFO - 创建模型...
2025-10-21 15:12:12,555 - INFO - 总参数数: 15,041,416
2025-10-21 15:12:12,555 - INFO - 可训练参数数: 15,041,416
2025-10-21 15:12:12,556 - INFO - 开始训练...
2025-10-21 15:12:26,925 - INFO - Batch 0, Loss: 8.5819, LR: 0.000000
2025-10-21 15:12:51,169 - INFO - 使用设备: cpu
2025-10-21 15:12:51,169 - INFO - 加载数据...
2025-10-21 15:12:51,169 - INFO - 加载训练数据...
2025-10-21 15:12:55,644 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-10-21 15:12:55,644 - INFO - 训练数据结构:
2025-10-21 15:12:55,645 - INFO - 数据结构分析:
2025-10-21 15:12:55,645 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 15:12:55,645 - INFO - 
英语相关语言对:
2025-10-21 15:12:55,645 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 15:12:55,645 - INFO -   en->de: 206112 个样本
2025-10-21 15:12:55,645 - INFO - 
德语相关语言对:
2025-10-21 15:12:55,645 - INFO - 德语->其他语言: ['en']
2025-10-21 15:12:55,645 - INFO -   de->en: 206112 个样本
2025-10-21 15:12:55,645 - INFO - 提取 en->de 的翻译对
2025-10-21 15:12:55,645 - INFO - 找到 en->de: 206112 个样本
2025-10-21 15:12:55,700 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 15:12:55,700 - INFO - 加载验证数据...
2025-10-21 15:12:55,747 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-10-21 15:12:55,747 - INFO - 提取 en->de 的翻译对
2025-10-21 15:12:55,747 - INFO - 找到 en->de: 888 个样本
2025-10-21 15:12:55,748 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 15:12:55,748 - INFO - 加载测试数据...
2025-10-21 15:12:55,849 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-10-21 15:12:55,850 - INFO - 提取 en->de 的翻译对
2025-10-21 15:12:55,850 - INFO - 找到 en->de: 8079 个样本
2025-10-21 15:12:55,853 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 15:12:55,853 - INFO - 开始构建分词器...
2025-10-21 15:12:55,909 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 15:12:57,230 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 15:12:59,386 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 15:12:59,426 - INFO - 数据加载器创建完成
2025-10-21 15:12:59,426 - INFO - 训练集: 206112 个样本
2025-10-21 15:12:59,426 - INFO - 验证集: 888 个样本
2025-10-21 15:12:59,426 - INFO - 测试集: 8079 个样本
2025-10-21 15:13:00,429 - INFO - 源语言词汇表大小: 5000
2025-10-21 15:13:00,429 - INFO - 目标语言词汇表大小: 5000
2025-10-21 15:13:00,429 - INFO - 创建模型...
2025-10-21 15:13:00,565 - INFO - 总参数数: 15,041,416
2025-10-21 15:13:00,565 - INFO - 可训练参数数: 15,041,416
2025-10-21 15:13:00,566 - INFO - 开始训练...
2025-10-21 15:13:14,024 - INFO - Batch 0, Loss: 8.6371, LR: 0.000000
2025-10-21 15:15:49,109 - INFO - Batch 10, Loss: 8.6391, LR: 0.000000
2025-10-21 15:18:18,935 - INFO - Batch 20, Loss: 8.6970, LR: 0.000000
2025-10-21 15:20:53,190 - INFO - Batch 30, Loss: 8.6510, LR: 0.000000
2025-10-21 15:21:29,070 - INFO - 使用设备: cpu
2025-10-21 15:21:29,070 - INFO - 加载数据...
2025-10-21 15:21:29,070 - INFO - 加载训练数据...
2025-10-21 15:21:33,607 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-10-21 15:21:33,607 - INFO - 训练数据结构:
2025-10-21 15:21:33,607 - INFO - 数据结构分析:
2025-10-21 15:21:33,607 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 15:21:33,607 - INFO - 
英语相关语言对:
2025-10-21 15:21:33,607 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 15:21:33,607 - INFO -   en->de: 206112 个样本
2025-10-21 15:21:33,607 - INFO - 
德语相关语言对:
2025-10-21 15:21:33,607 - INFO - 德语->其他语言: ['en']
2025-10-21 15:21:33,608 - INFO -   de->en: 206112 个样本
2025-10-21 15:21:33,608 - INFO - 提取 en->de 的翻译对
2025-10-21 15:21:33,608 - INFO - 找到 en->de: 206112 个样本
2025-10-21 15:21:33,662 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 15:21:33,662 - INFO - 加载验证数据...
2025-10-21 15:21:33,707 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-10-21 15:21:33,707 - INFO - 提取 en->de 的翻译对
2025-10-21 15:21:33,707 - INFO - 找到 en->de: 888 个样本
2025-10-21 15:21:33,707 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 15:21:33,708 - INFO - 加载测试数据...
2025-10-21 15:21:33,809 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-10-21 15:21:33,809 - INFO - 提取 en->de 的翻译对
2025-10-21 15:21:33,809 - INFO - 找到 en->de: 8079 个样本
2025-10-21 15:21:33,811 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 15:21:33,811 - INFO - 开始构建分词器...
2025-10-21 15:21:33,854 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 15:21:35,125 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 15:21:37,142 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 15:21:37,184 - INFO - 数据加载器创建完成
2025-10-21 15:21:37,185 - INFO - 训练集: 206112 个样本
2025-10-21 15:21:37,185 - INFO - 验证集: 888 个样本
2025-10-21 15:21:37,185 - INFO - 测试集: 8079 个样本
2025-10-21 15:21:38,152 - INFO - 源语言词汇表大小: 5000
2025-10-21 15:21:38,152 - INFO - 目标语言词汇表大小: 5000
2025-10-21 15:21:38,152 - INFO - 创建模型...
2025-10-21 15:21:38,293 - INFO - 总参数数: 15,041,416
2025-10-21 15:21:38,293 - INFO - 可训练参数数: 15,041,416
2025-10-21 15:21:38,294 - INFO - 开始训练...
2025-10-21 15:21:52,438 - INFO - Batch 0, Loss: 8.6371, LR: 0.000000
2025-10-21 15:24:20,417 - INFO - Batch 10, Loss: 8.6391, LR: 0.000000
2025-10-21 15:26:52,175 - INFO - Batch 20, Loss: 8.6970, LR: 0.000000
2025-10-21 15:29:21,277 - INFO - Batch 30, Loss: 8.6510, LR: 0.000000
2025-10-21 15:31:52,619 - INFO - Batch 40, Loss: 8.6571, LR: 0.000000
2025-10-21 15:34:22,456 - INFO - Batch 50, Loss: 8.6260, LR: 0.000000
2025-10-21 15:36:55,862 - INFO - Batch 60, Loss: 8.6569, LR: 0.000000
2025-10-21 15:57:02,494 - INFO - 使用设备: cuda:2
2025-10-21 15:57:02,494 - INFO - 加载数据...
2025-10-21 15:57:02,494 - ERROR - 训练数据文件不存在: data/IWSLT2017/iwslt2017_train.pkl
2025-10-21 16:00:33,788 - INFO - 使用设备: cuda:2
2025-10-21 16:00:33,789 - INFO - 加载数据...
2025-10-21 16:00:33,789 - INFO - 加载训练数据...
2025-10-21 16:00:37,637 - INFO - 成功加载 data/IWSLT2017/iwslt2017_train.pkl
2025-10-21 16:00:37,637 - INFO - 训练数据结构:
2025-10-21 16:00:37,637 - INFO - 数据结构分析:
2025-10-21 16:00:37,637 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 16:00:37,637 - INFO - 
英语相关语言对:
2025-10-21 16:00:37,637 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 16:00:37,637 - INFO -   en->de: 206112 个样本
2025-10-21 16:00:37,637 - INFO - 
德语相关语言对:
2025-10-21 16:00:37,637 - INFO - 德语->其他语言: ['en']
2025-10-21 16:00:37,637 - INFO -   de->en: 206112 个样本
2025-10-21 16:00:37,637 - INFO - 提取 en->de 的翻译对
2025-10-21 16:00:37,637 - INFO - 找到 en->de: 206112 个样本
2025-10-21 16:00:37,672 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 16:00:37,672 - INFO - 加载验证数据...
2025-10-21 16:00:37,703 - INFO - 成功加载 data/IWSLT2017/iwslt2017_validation.pkl
2025-10-21 16:00:37,703 - INFO - 提取 en->de 的翻译对
2025-10-21 16:00:37,703 - INFO - 找到 en->de: 888 个样本
2025-10-21 16:00:37,703 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 16:00:37,704 - INFO - 加载测试数据...
2025-10-21 16:00:37,788 - INFO - 成功加载 data/IWSLT2017/iwslt2017_test.pkl
2025-10-21 16:00:37,788 - INFO - 提取 en->de 的翻译对
2025-10-21 16:00:37,788 - INFO - 找到 en->de: 8079 个样本
2025-10-21 16:00:37,789 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 16:00:37,790 - INFO - 开始构建分词器...
2025-10-21 16:00:37,821 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 16:00:39,361 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 16:00:42,367 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 16:00:42,428 - INFO - 数据加载器创建完成
2025-10-21 16:00:42,428 - INFO - 训练集: 206112 个样本
2025-10-21 16:00:42,428 - INFO - 验证集: 888 个样本
2025-10-21 16:00:42,428 - INFO - 测试集: 8079 个样本
2025-10-21 16:00:43,619 - INFO - 源语言词汇表大小: 5000
2025-10-21 16:00:43,619 - INFO - 目标语言词汇表大小: 5000
2025-10-21 16:00:43,619 - INFO - 创建模型...
2025-10-21 16:00:50,311 - INFO - 总参数数: 15,041,416
2025-10-21 16:00:50,311 - INFO - 可训练参数数: 15,041,416
2025-10-21 16:00:50,311 - INFO - 开始训练...
2025-10-21 16:00:54,527 - INFO - Batch 0, Loss: 8.5653, LR: 0.000000
2025-10-21 16:00:55,942 - INFO - Batch 10, Loss: 8.5961, LR: 0.000000
2025-10-21 16:00:57,338 - INFO - Batch 20, Loss: 8.5806, LR: 0.000000
2025-10-21 16:00:58,737 - INFO - Batch 30, Loss: 8.5876, LR: 0.000000
2025-10-21 16:01:00,138 - INFO - Batch 40, Loss: 8.5597, LR: 0.000000
2025-10-21 16:01:01,535 - INFO - Batch 50, Loss: 8.5972, LR: 0.000000
2025-10-21 16:01:02,937 - INFO - Batch 60, Loss: 8.5880, LR: 0.000000
2025-10-21 16:01:04,342 - INFO - Batch 70, Loss: 8.5887, LR: 0.000000
2025-10-21 16:01:05,739 - INFO - Batch 80, Loss: 8.5496, LR: 0.000000
2025-10-21 16:01:07,148 - INFO - Batch 90, Loss: 8.5734, LR: 0.000000
2025-10-21 16:01:08,557 - INFO - Batch 100, Loss: 8.5599, LR: 0.000000
2025-10-21 16:01:09,966 - INFO - Batch 110, Loss: 8.5412, LR: 0.000000
2025-10-21 16:01:11,387 - INFO - Batch 120, Loss: 8.5475, LR: 0.000000
2025-10-21 16:01:12,816 - INFO - Batch 130, Loss: 8.5697, LR: 0.000000
2025-10-21 16:01:14,234 - INFO - Batch 140, Loss: 8.5613, LR: 0.000000
2025-10-21 16:01:15,650 - INFO - Batch 150, Loss: 8.5590, LR: 0.000000
2025-10-21 16:01:17,063 - INFO - Batch 160, Loss: 8.5402, LR: 0.000000
2025-10-21 16:01:18,483 - INFO - Batch 170, Loss: 8.5522, LR: 0.000000
2025-10-21 16:01:19,910 - INFO - Batch 180, Loss: 8.5425, LR: 0.000000
2025-10-21 16:01:21,331 - INFO - Batch 190, Loss: 8.5609, LR: 0.000000
2025-10-21 16:01:22,745 - INFO - Batch 200, Loss: 8.5410, LR: 0.000000
2025-10-21 16:01:24,154 - INFO - Batch 210, Loss: 8.5431, LR: 0.000000
2025-10-21 16:01:25,570 - INFO - Batch 220, Loss: 8.5584, LR: 0.000000
2025-10-21 16:01:26,978 - INFO - Batch 230, Loss: 8.5104, LR: 0.000000
2025-10-21 16:01:28,400 - INFO - Batch 240, Loss: 8.5453, LR: 0.000000
2025-10-21 16:01:29,820 - INFO - Batch 250, Loss: 8.5379, LR: 0.000000
2025-10-21 16:01:31,236 - INFO - Batch 260, Loss: 8.5080, LR: 0.000000
2025-10-21 16:01:32,659 - INFO - Batch 270, Loss: 8.5229, LR: 0.000000
2025-10-21 16:01:34,086 - INFO - Batch 280, Loss: 8.5202, LR: 0.000000
2025-10-21 16:01:35,499 - INFO - Batch 290, Loss: 8.5185, LR: 0.000000
2025-10-21 16:01:36,905 - INFO - Batch 300, Loss: 8.4848, LR: 0.000000
2025-10-21 16:01:38,312 - INFO - Batch 310, Loss: 8.4824, LR: 0.000000
2025-10-21 16:01:39,720 - INFO - Batch 320, Loss: 8.4727, LR: 0.000000
2025-10-21 16:01:41,137 - INFO - Batch 330, Loss: 8.4820, LR: 0.000000
2025-10-21 16:01:42,549 - INFO - Batch 340, Loss: 8.4707, LR: 0.000000
2025-10-21 16:01:43,967 - INFO - Batch 350, Loss: 8.4901, LR: 0.000000
2025-10-21 16:01:45,385 - INFO - Batch 360, Loss: 8.4400, LR: 0.000000
2025-10-21 16:01:46,789 - INFO - Batch 370, Loss: 8.4666, LR: 0.000000
2025-10-21 16:01:48,207 - INFO - Batch 380, Loss: 8.4519, LR: 0.000000
2025-10-21 16:01:49,625 - INFO - Batch 390, Loss: 8.4799, LR: 0.000000
2025-10-21 16:01:51,039 - INFO - Batch 400, Loss: 8.4457, LR: 0.000000
2025-10-21 16:01:52,458 - INFO - Batch 410, Loss: 8.4442, LR: 0.000000
2025-10-21 16:01:53,865 - INFO - Batch 420, Loss: 8.4208, LR: 0.000000
2025-10-21 16:01:55,282 - INFO - Batch 430, Loss: 8.4269, LR: 0.000000
2025-10-21 16:01:56,700 - INFO - Batch 440, Loss: 8.4237, LR: 0.000000
2025-10-21 16:02:39,172 - INFO - 使用设备: cuda:2
2025-10-21 16:02:39,172 - INFO - 加载数据...
2025-10-21 16:02:39,172 - INFO - 加载训练数据...
2025-10-21 16:02:42,996 - INFO - 成功加载 data/IWSLT2017/iwslt2017_train.pkl
2025-10-21 16:02:42,997 - INFO - 训练数据结构:
2025-10-21 16:02:42,997 - INFO - 数据结构分析:
2025-10-21 16:02:42,997 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 16:02:42,997 - INFO - 
英语相关语言对:
2025-10-21 16:02:42,997 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 16:02:42,997 - INFO -   en->de: 206112 个样本
2025-10-21 16:02:42,997 - INFO - 
德语相关语言对:
2025-10-21 16:02:42,997 - INFO - 德语->其他语言: ['en']
2025-10-21 16:02:42,997 - INFO -   de->en: 206112 个样本
2025-10-21 16:02:42,997 - INFO - 提取 en->de 的翻译对
2025-10-21 16:02:42,997 - INFO - 找到 en->de: 206112 个样本
2025-10-21 16:02:43,032 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 16:02:43,032 - INFO - 加载验证数据...
2025-10-21 16:02:43,063 - INFO - 成功加载 data/IWSLT2017/iwslt2017_validation.pkl
2025-10-21 16:02:43,063 - INFO - 提取 en->de 的翻译对
2025-10-21 16:02:43,063 - INFO - 找到 en->de: 888 个样本
2025-10-21 16:02:43,064 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 16:02:43,064 - INFO - 加载测试数据...
2025-10-21 16:02:43,149 - INFO - 成功加载 data/IWSLT2017/iwslt2017_test.pkl
2025-10-21 16:02:43,149 - INFO - 提取 en->de 的翻译对
2025-10-21 16:02:43,149 - INFO - 找到 en->de: 8079 个样本
2025-10-21 16:02:43,151 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 16:02:43,151 - INFO - 开始构建分词器...
2025-10-21 16:02:43,183 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 16:02:44,709 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 16:02:47,667 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 16:02:47,729 - INFO - 数据加载器创建完成
2025-10-21 16:02:47,729 - INFO - 训练集: 206112 个样本
2025-10-21 16:02:47,729 - INFO - 验证集: 888 个样本
2025-10-21 16:02:47,729 - INFO - 测试集: 8079 个样本
2025-10-21 16:02:48,978 - INFO - 源语言词汇表大小: 5000
2025-10-21 16:02:48,978 - INFO - 目标语言词汇表大小: 5000
2025-10-21 16:02:48,978 - INFO - 创建模型...
2025-10-21 16:02:50,394 - INFO - 总参数数: 15,041,416
2025-10-21 16:02:50,394 - INFO - 可训练参数数: 15,041,416
2025-10-21 16:02:50,394 - INFO - 开始训练...
2025-10-21 16:02:51,446 - INFO - Batch 0, Loss: 8.5653, LR: 0.000000
2025-10-21 16:03:05,469 - INFO - Batch 100, Loss: 8.5599, LR: 0.000000
2025-10-21 16:03:19,531 - INFO - Batch 200, Loss: 8.5410, LR: 0.000000
2025-10-21 16:03:33,547 - INFO - Batch 300, Loss: 8.4848, LR: 0.000000
2025-10-21 16:03:47,889 - INFO - Batch 400, Loss: 8.4457, LR: 0.000000
2025-10-21 16:04:02,275 - INFO - Batch 500, Loss: 8.4150, LR: 0.000000
2025-10-21 16:04:16,706 - INFO - Batch 600, Loss: 8.3398, LR: 0.000000
2025-10-21 16:04:31,124 - INFO - Batch 700, Loss: 8.2754, LR: 0.000000
2025-10-21 16:04:45,553 - INFO - Batch 800, Loss: 8.1447, LR: 0.000000
2025-10-21 16:04:59,887 - INFO - Batch 900, Loss: 8.1485, LR: 0.000000
2025-10-21 16:05:14,085 - INFO - Batch 1000, Loss: 8.1125, LR: 0.000000
2025-10-21 16:05:28,193 - INFO - Batch 1100, Loss: 8.0379, LR: 0.000000
2025-10-21 16:05:42,512 - INFO - Batch 1200, Loss: 7.9482, LR: 0.000000
2025-10-21 16:05:56,677 - INFO - Batch 1300, Loss: 7.9286, LR: 0.000001
2025-10-21 16:06:10,810 - INFO - Batch 1400, Loss: 7.9143, LR: 0.000001
2025-10-21 16:06:24,991 - INFO - Batch 1500, Loss: 7.8296, LR: 0.000001
2025-10-21 16:06:39,295 - INFO - Batch 1600, Loss: 7.8348, LR: 0.000001
2025-10-21 16:06:53,613 - INFO - Batch 1700, Loss: 7.7973, LR: 0.000001
2025-10-21 16:07:07,928 - INFO - Batch 1800, Loss: 7.7022, LR: 0.000001
2025-10-21 16:07:22,401 - INFO - Batch 1900, Loss: 7.7370, LR: 0.000001
2025-10-21 16:07:36,576 - INFO - Batch 2000, Loss: 7.6842, LR: 0.000001
2025-10-21 16:07:50,756 - INFO - Batch 2100, Loss: 7.6726, LR: 0.000001
2025-10-21 16:08:04,910 - INFO - Batch 2200, Loss: 7.6625, LR: 0.000001
2025-10-21 16:08:19,108 - INFO - Batch 2300, Loss: 7.5754, LR: 0.000001
2025-10-21 16:08:33,249 - INFO - Batch 2400, Loss: 7.6186, LR: 0.000001
2025-10-21 16:08:47,374 - INFO - Batch 2500, Loss: 7.5088, LR: 0.000001
2025-10-21 16:09:01,547 - INFO - Batch 2600, Loss: 7.4623, LR: 0.000001
2025-10-21 16:09:15,763 - INFO - Batch 2700, Loss: 7.4077, LR: 0.000001
2025-10-21 16:09:29,919 - INFO - Batch 2800, Loss: 7.3588, LR: 0.000001
2025-10-21 16:09:44,101 - INFO - Batch 2900, Loss: 7.3770, LR: 0.000001
2025-10-21 16:09:58,260 - INFO - Batch 3000, Loss: 7.4309, LR: 0.000001
2025-10-21 16:10:12,598 - INFO - Batch 3100, Loss: 7.3456, LR: 0.000001
2025-10-21 16:10:26,996 - INFO - Batch 3200, Loss: 7.3509, LR: 0.000001
2025-10-21 16:10:41,413 - INFO - Batch 3300, Loss: 7.2978, LR: 0.000001
2025-10-21 16:10:55,843 - INFO - Batch 3400, Loss: 7.2237, LR: 0.000001
2025-10-21 16:11:10,247 - INFO - Batch 3500, Loss: 7.2454, LR: 0.000001
2025-10-21 16:11:24,658 - INFO - Batch 3600, Loss: 7.1670, LR: 0.000001
2025-10-21 16:11:38,837 - INFO - Batch 3700, Loss: 7.1690, LR: 0.000001
2025-10-21 16:11:53,064 - INFO - Batch 3800, Loss: 7.0596, LR: 0.000002
2025-10-21 16:12:07,265 - INFO - Batch 3900, Loss: 7.0600, LR: 0.000002
2025-10-21 16:12:21,547 - INFO - Batch 4000, Loss: 7.0660, LR: 0.000002
2025-10-21 16:12:35,772 - INFO - Batch 4100, Loss: 7.0123, LR: 0.000002
2025-10-21 16:12:49,980 - INFO - Batch 4200, Loss: 6.9384, LR: 0.000002
2025-10-21 16:13:04,203 - INFO - Batch 4300, Loss: 6.9608, LR: 0.000002
2025-10-21 16:13:18,388 - INFO - Batch 4400, Loss: 6.9568, LR: 0.000002
2025-10-21 16:13:32,595 - INFO - Batch 4500, Loss: 6.8820, LR: 0.000001
2025-10-21 16:13:46,820 - INFO - Batch 4600, Loss: 6.9345, LR: 0.000001
2025-10-21 16:14:01,115 - INFO - Batch 4700, Loss: 6.9031, LR: 0.000001
2025-10-21 16:14:15,272 - INFO - Batch 4800, Loss: 6.9266, LR: 0.000001
2025-10-21 16:14:29,437 - INFO - Batch 4900, Loss: 6.8593, LR: 0.000001
2025-10-21 16:14:43,563 - INFO - Batch 5000, Loss: 6.8580, LR: 0.000001
2025-10-21 16:14:57,736 - INFO - Batch 5100, Loss: 6.8272, LR: 0.000001
2025-10-21 16:15:11,911 - INFO - Batch 5200, Loss: 6.8554, LR: 0.000001
2025-10-21 16:15:26,124 - INFO - Batch 5300, Loss: 6.6353, LR: 0.000001
2025-10-21 16:15:40,332 - INFO - Batch 5400, Loss: 6.6798, LR: 0.000001
2025-10-21 16:15:54,506 - INFO - Batch 5500, Loss: 6.6584, LR: 0.000001
2025-10-21 16:16:08,787 - INFO - Batch 5600, Loss: 6.7312, LR: 0.000001
2025-10-21 16:16:23,152 - INFO - Batch 5700, Loss: 6.5569, LR: 0.000001
2025-10-21 16:16:37,613 - INFO - Batch 5800, Loss: 6.6550, LR: 0.000001
2025-10-21 16:16:52,110 - INFO - Batch 5900, Loss: 6.7759, LR: 0.000001
2025-10-21 16:17:06,628 - INFO - Batch 6000, Loss: 6.7583, LR: 0.000001
2025-10-21 16:17:21,067 - INFO - Batch 6100, Loss: 6.6846, LR: 0.000001
2025-10-21 16:17:35,482 - INFO - Batch 6200, Loss: 6.8186, LR: 0.000001
2025-10-21 16:17:49,745 - INFO - Batch 6300, Loss: 6.6060, LR: 0.000001
2025-10-21 16:18:04,209 - INFO - Batch 6400, Loss: 6.6739, LR: 0.000001
2025-10-21 16:18:11,440 - INFO - Epoch 1/50: Train Loss: 7.4002, Val Loss: 6.6818, LR: 0.000001
2025-10-21 16:18:11,655 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 16:18:11,805 - INFO - Batch 0, Loss: 6.6887, LR: 0.000001
2025-10-21 16:18:26,102 - INFO - Batch 100, Loss: 6.5279, LR: 0.000001
2025-10-21 16:18:40,431 - INFO - Batch 200, Loss: 6.6431, LR: 0.000001
2025-10-21 16:19:35,202 - INFO - 使用设备: cuda:2
2025-10-21 16:19:35,202 - INFO - 加载数据...
2025-10-21 16:19:35,202 - INFO - 加载训练数据...
2025-10-21 16:19:38,971 - INFO - 成功加载 data/IWSLT2017/iwslt2017_train.pkl
2025-10-21 16:19:38,972 - INFO - 训练数据结构:
2025-10-21 16:19:38,972 - INFO - 数据结构分析:
2025-10-21 16:19:38,972 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 16:19:38,972 - INFO - 
英语相关语言对:
2025-10-21 16:19:38,972 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 16:19:38,972 - INFO -   en->de: 206112 个样本
2025-10-21 16:19:38,972 - INFO - 
德语相关语言对:
2025-10-21 16:19:38,972 - INFO - 德语->其他语言: ['en']
2025-10-21 16:19:38,972 - INFO -   de->en: 206112 个样本
2025-10-21 16:19:38,972 - INFO - 提取 en->de 的翻译对
2025-10-21 16:19:38,972 - INFO - 找到 en->de: 206112 个样本
2025-10-21 16:19:39,006 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 16:19:39,006 - INFO - 加载验证数据...
2025-10-21 16:19:39,037 - INFO - 成功加载 data/IWSLT2017/iwslt2017_validation.pkl
2025-10-21 16:19:39,037 - INFO - 提取 en->de 的翻译对
2025-10-21 16:19:39,037 - INFO - 找到 en->de: 888 个样本
2025-10-21 16:19:39,037 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 16:19:39,037 - INFO - 加载测试数据...
2025-10-21 16:19:39,120 - INFO - 成功加载 data/IWSLT2017/iwslt2017_test.pkl
2025-10-21 16:19:39,120 - INFO - 提取 en->de 的翻译对
2025-10-21 16:19:39,120 - INFO - 找到 en->de: 8079 个样本
2025-10-21 16:19:39,121 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 16:19:39,122 - INFO - 开始构建分词器...
2025-10-21 16:19:39,152 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 16:19:40,733 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 16:19:43,643 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 16:19:43,703 - INFO - 数据加载器创建完成
2025-10-21 16:19:43,703 - INFO - 训练集: 206112 个样本
2025-10-21 16:19:43,703 - INFO - 验证集: 888 个样本
2025-10-21 16:19:43,703 - INFO - 测试集: 8079 个样本
2025-10-21 16:19:44,948 - INFO - 源语言词汇表大小: 5000
2025-10-21 16:19:44,948 - INFO - 目标语言词汇表大小: 5000
2025-10-21 16:19:44,948 - INFO - 创建模型...
2025-10-21 16:19:46,423 - INFO - 总参数数: 15,041,416
2025-10-21 16:19:46,423 - INFO - 可训练参数数: 15,041,416
2025-10-21 16:19:46,423 - INFO - 开始训练...
2025-10-21 16:19:47,468 - INFO - Batch 0, Loss: 8.5653, LR: 0.000000
2025-10-21 16:20:01,626 - INFO - Batch 100, Loss: 8.5455, LR: 0.000000
2025-10-21 16:20:15,652 - INFO - Batch 200, Loss: 8.4772, LR: 0.000000
2025-10-21 16:20:29,652 - INFO - Batch 300, Loss: 8.3519, LR: 0.000000
2025-10-21 16:20:43,726 - INFO - Batch 400, Loss: 8.2600, LR: 0.000000
2025-10-21 16:20:57,841 - INFO - Batch 500, Loss: 8.2270, LR: 0.000001
2025-10-21 16:21:11,942 - INFO - Batch 600, Loss: 8.1041, LR: 0.000001
2025-10-21 16:21:26,038 - INFO - Batch 700, Loss: 8.0128, LR: 0.000001
2025-10-21 16:21:40,138 - INFO - Batch 800, Loss: 7.8412, LR: 0.000001
2025-10-21 16:21:54,256 - INFO - Batch 900, Loss: 7.8436, LR: 0.000001
2025-10-21 16:22:08,423 - INFO - Batch 1000, Loss: 7.8315, LR: 0.000001
2025-10-21 16:22:22,638 - INFO - Batch 1100, Loss: 7.7403, LR: 0.000001
2025-10-21 16:22:36,899 - INFO - Batch 1200, Loss: 7.6123, LR: 0.000001
2025-10-21 16:22:51,351 - INFO - Batch 1300, Loss: 7.6173, LR: 0.000001
2025-10-21 16:23:05,826 - INFO - Batch 1400, Loss: 7.6083, LR: 0.000002
2025-10-21 16:23:20,297 - INFO - Batch 1500, Loss: 7.4879, LR: 0.000002
2025-10-21 16:23:34,746 - INFO - Batch 1600, Loss: 7.4973, LR: 0.000002
2025-10-21 16:23:49,212 - INFO - Batch 1700, Loss: 7.4184, LR: 0.000002
2025-10-21 16:24:03,673 - INFO - Batch 1800, Loss: 7.2996, LR: 0.000002
2025-10-21 16:24:18,098 - INFO - Batch 1900, Loss: 7.3345, LR: 0.000002
2025-10-21 16:24:32,255 - INFO - Batch 2000, Loss: 7.2510, LR: 0.000002
2025-10-21 16:24:46,397 - INFO - Batch 2100, Loss: 7.2298, LR: 0.000002
2025-10-21 16:25:00,536 - INFO - Batch 2200, Loss: 7.1929, LR: 0.000002
2025-10-21 16:25:14,863 - INFO - Batch 2300, Loss: 7.0742, LR: 0.000002
2025-10-21 16:25:29,351 - INFO - Batch 2400, Loss: 7.1279, LR: 0.000002
2025-10-21 16:25:43,860 - INFO - Batch 2500, Loss: 6.9861, LR: 0.000002
2025-10-21 16:25:58,381 - INFO - Batch 2600, Loss: 6.9533, LR: 0.000002
2025-10-21 16:26:12,868 - INFO - Batch 2700, Loss: 6.8772, LR: 0.000002
2025-10-21 16:26:27,364 - INFO - Batch 2800, Loss: 6.8050, LR: 0.000002
2025-10-21 16:26:41,932 - INFO - Batch 2900, Loss: 6.8707, LR: 0.000002
2025-10-21 16:26:56,355 - INFO - Batch 3000, Loss: 6.9306, LR: 0.000002
2025-10-21 16:27:10,704 - INFO - Batch 3100, Loss: 6.8642, LR: 0.000002
2025-10-21 16:27:25,056 - INFO - Batch 3200, Loss: 6.8504, LR: 0.000002
2025-10-21 16:27:39,314 - INFO - Batch 3300, Loss: 6.8481, LR: 0.000002
2025-10-21 16:27:53,587 - INFO - Batch 3400, Loss: 6.7665, LR: 0.000002
2025-10-21 16:28:07,894 - INFO - Batch 3500, Loss: 6.7917, LR: 0.000002
2025-10-21 16:28:22,241 - INFO - Batch 3600, Loss: 6.7168, LR: 0.000002
2025-10-21 16:28:36,641 - INFO - Batch 3700, Loss: 6.7769, LR: 0.000002
2025-10-21 16:28:51,026 - INFO - Batch 3800, Loss: 6.6804, LR: 0.000002
2025-10-21 16:29:05,496 - INFO - Batch 3900, Loss: 6.6982, LR: 0.000002
2025-10-21 16:29:20,004 - INFO - Batch 4000, Loss: 6.7126, LR: 0.000002
2025-10-21 16:29:34,481 - INFO - Batch 4100, Loss: 6.7085, LR: 0.000002
2025-10-21 16:29:48,898 - INFO - Batch 4200, Loss: 6.6280, LR: 0.000002
2025-10-21 16:30:03,201 - INFO - Batch 4300, Loss: 6.6646, LR: 0.000002
2025-10-21 16:30:17,513 - INFO - Batch 4400, Loss: 6.6888, LR: 0.000002
2025-10-21 16:30:32,004 - INFO - Batch 4500, Loss: 6.6101, LR: 0.000001
2025-10-21 16:30:46,518 - INFO - Batch 4600, Loss: 6.7046, LR: 0.000001
2025-10-21 16:31:01,006 - INFO - Batch 4700, Loss: 6.6668, LR: 0.000001
2025-10-21 16:31:15,377 - INFO - Batch 4800, Loss: 6.7059, LR: 0.000001
2025-10-21 16:31:29,649 - INFO - Batch 4900, Loss: 6.6320, LR: 0.000001
2025-10-21 16:31:43,948 - INFO - Batch 5000, Loss: 6.6502, LR: 0.000001
2025-10-21 16:31:58,223 - INFO - Batch 5100, Loss: 6.6041, LR: 0.000001
2025-10-21 16:32:12,666 - INFO - Batch 5200, Loss: 6.6694, LR: 0.000001
2025-10-21 16:32:27,083 - INFO - Batch 5300, Loss: 6.4293, LR: 0.000001
2025-10-21 16:32:41,358 - INFO - Batch 5400, Loss: 6.4750, LR: 0.000001
2025-10-21 16:32:55,646 - INFO - Batch 5500, Loss: 6.4721, LR: 0.000001
2025-10-21 16:33:09,941 - INFO - Batch 5600, Loss: 6.5564, LR: 0.000001
2025-10-21 16:33:24,189 - INFO - Batch 5700, Loss: 6.3807, LR: 0.000001
2025-10-21 16:33:38,653 - INFO - Batch 5800, Loss: 6.4959, LR: 0.000001
2025-10-21 16:33:53,203 - INFO - Batch 5900, Loss: 6.6450, LR: 0.000001
2025-10-21 16:34:07,789 - INFO - Batch 6000, Loss: 6.6200, LR: 0.000001
2025-10-21 16:34:22,214 - INFO - Batch 6100, Loss: 6.5380, LR: 0.000001
2025-10-21 16:34:36,429 - INFO - Batch 6200, Loss: 6.7182, LR: 0.000001
2025-10-21 16:34:50,678 - INFO - Batch 6300, Loss: 6.4851, LR: 0.000001
2025-10-21 16:35:05,052 - INFO - Batch 6400, Loss: 6.5502, LR: 0.000001
2025-10-21 16:35:12,154 - INFO - Epoch 1/30: Train Loss: 7.0981, Val Loss: 6.5863, LR: 0.000001
2025-10-21 16:35:12,389 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 16:35:12,541 - INFO - Batch 0, Loss: 6.5839, LR: 0.000001
2025-10-21 16:35:26,837 - INFO - Batch 100, Loss: 6.4070, LR: 0.000001
2025-10-21 16:35:41,132 - INFO - Batch 200, Loss: 6.5416, LR: 0.000001
2025-10-21 16:35:55,576 - INFO - Batch 300, Loss: 6.5554, LR: 0.000001
2025-10-21 16:36:10,096 - INFO - Batch 400, Loss: 6.3904, LR: 0.000001
2025-10-21 16:36:24,620 - INFO - Batch 500, Loss: 6.6869, LR: 0.000001
2025-10-21 16:36:39,177 - INFO - Batch 600, Loss: 6.6761, LR: 0.000001
2025-10-21 16:36:53,555 - INFO - Batch 700, Loss: 6.6978, LR: 0.000001
2025-10-21 16:37:07,758 - INFO - Batch 800, Loss: 6.4733, LR: 0.000001
2025-10-21 16:37:22,166 - INFO - Batch 900, Loss: 6.5495, LR: 0.000001
2025-10-21 16:37:36,787 - INFO - Batch 1000, Loss: 6.6203, LR: 0.000001
2025-10-21 16:37:51,395 - INFO - Batch 1100, Loss: 6.5960, LR: 0.000001
2025-10-21 16:38:05,763 - INFO - Batch 1200, Loss: 6.5132, LR: 0.000001
2025-10-21 16:38:20,097 - INFO - Batch 1300, Loss: 6.5874, LR: 0.000001
2025-10-21 16:38:34,608 - INFO - Batch 1400, Loss: 6.6150, LR: 0.000001
2025-10-21 16:38:49,139 - INFO - Batch 1500, Loss: 6.4699, LR: 0.000001
2025-10-21 16:39:03,642 - INFO - Batch 1600, Loss: 6.4474, LR: 0.000001
2025-10-21 16:39:17,899 - INFO - Batch 1700, Loss: 6.3946, LR: 0.000001
2025-10-21 16:39:32,224 - INFO - Batch 1800, Loss: 6.6054, LR: 0.000001
2025-10-21 16:39:46,525 - INFO - Batch 1900, Loss: 6.4271, LR: 0.000001
2025-10-21 16:40:00,802 - INFO - Batch 2000, Loss: 6.5695, LR: 0.000001
2025-10-21 16:40:15,049 - INFO - Batch 2100, Loss: 6.3460, LR: 0.000001
2025-10-21 16:40:29,279 - INFO - Batch 2200, Loss: 6.3146, LR: 0.000001
2025-10-21 16:40:43,494 - INFO - Batch 2300, Loss: 6.4041, LR: 0.000001
2025-10-21 16:40:57,780 - INFO - Batch 2400, Loss: 6.5510, LR: 0.000001
2025-10-21 16:41:12,063 - INFO - Batch 2500, Loss: 6.4519, LR: 0.000001
2025-10-21 16:41:26,298 - INFO - Batch 2600, Loss: 6.4568, LR: 0.000001
2025-10-21 16:41:40,549 - INFO - Batch 2700, Loss: 6.4872, LR: 0.000001
2025-10-21 16:41:54,779 - INFO - Batch 2800, Loss: 6.5377, LR: 0.000001
2025-10-21 16:42:08,997 - INFO - Batch 2900, Loss: 6.3872, LR: 0.000001
2025-10-21 16:42:23,364 - INFO - Batch 3000, Loss: 6.5109, LR: 0.000001
2025-10-21 16:42:37,620 - INFO - Batch 3100, Loss: 6.5086, LR: 0.000001
2025-10-21 16:42:51,916 - INFO - Batch 3200, Loss: 6.4533, LR: 0.000001
2025-10-21 16:43:06,100 - INFO - Batch 3300, Loss: 6.5555, LR: 0.000001
2025-10-21 16:43:20,269 - INFO - Batch 3400, Loss: 6.4297, LR: 0.000001
2025-10-21 16:43:34,462 - INFO - Batch 3500, Loss: 6.4601, LR: 0.000001
2025-10-21 16:43:48,661 - INFO - Batch 3600, Loss: 6.4369, LR: 0.000001
2025-10-21 16:44:02,800 - INFO - Batch 3700, Loss: 6.5598, LR: 0.000001
2025-10-21 16:44:16,974 - INFO - Batch 3800, Loss: 6.3272, LR: 0.000001
2025-10-21 16:44:31,114 - INFO - Batch 3900, Loss: 6.5491, LR: 0.000001
2025-10-21 16:44:45,312 - INFO - Batch 4000, Loss: 6.5153, LR: 0.000001
2025-10-21 16:44:59,467 - INFO - Batch 4100, Loss: 6.4393, LR: 0.000001
2025-10-21 16:45:13,643 - INFO - Batch 4200, Loss: 6.5249, LR: 0.000001
2025-10-21 16:45:27,841 - INFO - Batch 4300, Loss: 6.6072, LR: 0.000001
2025-10-21 16:45:42,097 - INFO - Batch 4400, Loss: 6.2055, LR: 0.000001
2025-10-21 16:45:56,335 - INFO - Batch 4500, Loss: 6.2977, LR: 0.000001
2025-10-21 16:46:10,591 - INFO - Batch 4600, Loss: 6.5033, LR: 0.000001
2025-10-21 16:46:24,732 - INFO - Batch 4700, Loss: 6.4862, LR: 0.000001
2025-10-21 16:46:38,914 - INFO - Batch 4800, Loss: 6.4565, LR: 0.000001
2025-10-21 16:46:53,106 - INFO - Batch 4900, Loss: 6.4129, LR: 0.000001
2025-10-21 16:47:07,254 - INFO - Batch 5000, Loss: 6.3331, LR: 0.000001
2025-10-21 16:47:21,474 - INFO - Batch 5100, Loss: 6.3932, LR: 0.000001
2025-10-21 16:47:35,675 - INFO - Batch 5200, Loss: 6.6383, LR: 0.000001
2025-10-21 16:47:49,825 - INFO - Batch 5300, Loss: 6.3909, LR: 0.000001
2025-10-21 16:48:03,981 - INFO - Batch 5400, Loss: 6.4226, LR: 0.000001
2025-10-21 16:48:18,170 - INFO - Batch 5500, Loss: 6.4387, LR: 0.000001
2025-10-21 16:48:32,386 - INFO - Batch 5600, Loss: 6.3821, LR: 0.000001
2025-10-21 16:48:46,559 - INFO - Batch 5700, Loss: 6.3859, LR: 0.000001
2025-10-21 16:49:00,765 - INFO - Batch 5800, Loss: 6.5582, LR: 0.000001
2025-10-21 16:49:14,937 - INFO - Batch 5900, Loss: 6.2731, LR: 0.000001
2025-10-21 16:49:29,126 - INFO - Batch 6000, Loss: 6.4519, LR: 0.000001
2025-10-21 16:49:43,455 - INFO - Batch 6100, Loss: 6.4913, LR: 0.000001
2025-10-21 16:49:57,746 - INFO - Batch 6200, Loss: 6.3701, LR: 0.000001
2025-10-21 16:50:12,052 - INFO - Batch 6300, Loss: 6.5367, LR: 0.000001
2025-10-21 16:50:26,271 - INFO - Batch 6400, Loss: 6.3452, LR: 0.000001
2025-10-21 16:50:33,370 - INFO - Epoch 2/30: Train Loss: 6.4868, Val Loss: 6.4564, LR: 0.000001
2025-10-21 16:50:33,624 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 16:50:33,777 - INFO - Batch 0, Loss: 6.3175, LR: 0.000001
2025-10-21 16:50:48,049 - INFO - Batch 100, Loss: 6.4579, LR: 0.000001
2025-10-21 16:51:02,284 - INFO - Batch 200, Loss: 6.4697, LR: 0.000001
2025-10-21 16:51:16,579 - INFO - Batch 300, Loss: 6.5109, LR: 0.000001
2025-10-21 16:51:30,781 - INFO - Batch 400, Loss: 6.5351, LR: 0.000001
2025-10-21 16:51:45,156 - INFO - Batch 500, Loss: 6.4228, LR: 0.000001
2025-10-21 16:51:59,583 - INFO - Batch 600, Loss: 6.3347, LR: 0.000001
2025-10-21 16:52:14,043 - INFO - Batch 700, Loss: 6.3559, LR: 0.000001
2025-10-21 16:52:28,463 - INFO - Batch 800, Loss: 6.4543, LR: 0.000001
2025-10-21 16:52:42,949 - INFO - Batch 900, Loss: 6.3803, LR: 0.000001
2025-10-21 16:52:57,416 - INFO - Batch 1000, Loss: 6.3836, LR: 0.000001
2025-10-21 16:53:11,815 - INFO - Batch 1100, Loss: 6.5475, LR: 0.000001
2025-10-21 16:53:26,198 - INFO - Batch 1200, Loss: 6.4480, LR: 0.000001
2025-10-21 16:53:40,404 - INFO - Batch 1300, Loss: 6.3584, LR: 0.000001
2025-10-21 16:53:54,600 - INFO - Batch 1400, Loss: 6.4676, LR: 0.000001
2025-10-21 16:54:08,840 - INFO - Batch 1500, Loss: 6.4728, LR: 0.000001
2025-10-21 16:54:23,080 - INFO - Batch 1600, Loss: 6.5427, LR: 0.000001
2025-10-21 16:54:37,289 - INFO - Batch 1700, Loss: 6.3386, LR: 0.000001
2025-10-21 16:54:51,528 - INFO - Batch 1800, Loss: 6.3085, LR: 0.000001
2025-10-21 16:55:05,763 - INFO - Batch 1900, Loss: 6.2444, LR: 0.000001
2025-10-21 16:55:19,977 - INFO - Batch 2000, Loss: 6.3841, LR: 0.000001
2025-10-21 16:55:34,199 - INFO - Batch 2100, Loss: 6.4608, LR: 0.000001
2025-10-21 16:55:48,408 - INFO - Batch 2200, Loss: 6.4087, LR: 0.000001
2025-10-21 16:56:02,660 - INFO - Batch 2300, Loss: 6.4350, LR: 0.000001
2025-10-21 16:56:16,912 - INFO - Batch 2400, Loss: 6.4079, LR: 0.000001
2025-10-21 16:56:31,112 - INFO - Batch 2500, Loss: 6.4209, LR: 0.000001
2025-10-21 16:56:45,310 - INFO - Batch 2600, Loss: 6.3392, LR: 0.000001
2025-10-21 16:56:59,548 - INFO - Batch 2700, Loss: 6.3628, LR: 0.000001
2025-10-21 16:57:13,792 - INFO - Batch 2800, Loss: 6.3103, LR: 0.000001
2025-10-21 16:57:28,116 - INFO - Batch 2900, Loss: 6.5988, LR: 0.000001
2025-10-21 16:57:42,358 - INFO - Batch 3000, Loss: 6.3103, LR: 0.000001
2025-10-21 16:57:56,627 - INFO - Batch 3100, Loss: 6.4230, LR: 0.000001
2025-10-21 16:58:10,921 - INFO - Batch 3200, Loss: 6.3960, LR: 0.000001
2025-10-21 16:58:25,121 - INFO - Batch 3300, Loss: 6.3632, LR: 0.000001
2025-10-21 16:58:39,329 - INFO - Batch 3400, Loss: 6.3215, LR: 0.000001
2025-10-21 16:58:53,562 - INFO - Batch 3500, Loss: 6.4417, LR: 0.000001
2025-10-21 16:59:07,781 - INFO - Batch 3600, Loss: 6.3381, LR: 0.000001
2025-10-21 16:59:22,006 - INFO - Batch 3700, Loss: 6.2826, LR: 0.000001
2025-10-21 16:59:36,317 - INFO - Batch 3800, Loss: 6.3624, LR: 0.000001
2025-10-21 16:59:50,563 - INFO - Batch 3900, Loss: 6.3841, LR: 0.000001
2025-10-21 17:00:04,836 - INFO - Batch 4000, Loss: 6.3880, LR: 0.000001
2025-10-21 17:00:19,044 - INFO - Batch 4100, Loss: 6.4986, LR: 0.000001
2025-10-21 17:00:33,233 - INFO - Batch 4200, Loss: 6.3356, LR: 0.000001
2025-10-21 17:00:47,446 - INFO - Batch 4300, Loss: 6.4797, LR: 0.000001
2025-10-21 17:01:01,582 - INFO - Batch 4400, Loss: 6.4795, LR: 0.000001
2025-10-21 17:01:15,791 - INFO - Batch 4500, Loss: 6.4838, LR: 0.000001
2025-10-21 17:01:29,948 - INFO - Batch 4600, Loss: 6.3444, LR: 0.000001
2025-10-21 17:01:44,174 - INFO - Batch 4700, Loss: 6.3865, LR: 0.000001
2025-10-21 17:01:58,444 - INFO - Batch 4800, Loss: 6.3742, LR: 0.000001
2025-10-21 17:02:12,708 - INFO - Batch 4900, Loss: 6.3881, LR: 0.000001
2025-10-21 17:02:27,098 - INFO - Batch 5000, Loss: 6.2278, LR: 0.000001
2025-10-21 17:02:41,395 - INFO - Batch 5100, Loss: 6.4776, LR: 0.000001
2025-10-21 17:02:55,669 - INFO - Batch 5200, Loss: 6.2496, LR: 0.000001
2025-10-21 17:03:09,967 - INFO - Batch 5300, Loss: 6.3066, LR: 0.000001
2025-10-21 17:03:24,245 - INFO - Batch 5400, Loss: 6.4378, LR: 0.000001
2025-10-21 17:03:38,541 - INFO - Batch 5500, Loss: 6.4534, LR: 0.000001
2025-10-21 17:03:52,730 - INFO - Batch 5600, Loss: 6.4314, LR: 0.000001
2025-10-21 17:04:06,854 - INFO - Batch 5700, Loss: 6.3628, LR: 0.000001
2025-10-21 17:04:20,976 - INFO - Batch 5800, Loss: 6.3070, LR: 0.000001
2025-10-21 17:04:35,145 - INFO - Batch 5900, Loss: 6.4393, LR: 0.000001
2025-10-21 17:04:49,304 - INFO - Batch 6000, Loss: 6.2454, LR: 0.000001
2025-10-21 17:05:03,461 - INFO - Batch 6100, Loss: 6.4613, LR: 0.000001
2025-10-21 17:05:17,599 - INFO - Batch 6200, Loss: 6.1836, LR: 0.000001
2025-10-21 17:05:31,818 - INFO - Batch 6300, Loss: 6.4297, LR: 0.000001
2025-10-21 17:05:45,976 - INFO - Batch 6400, Loss: 6.2422, LR: 0.000001
2025-10-21 17:05:53,052 - INFO - Epoch 3/30: Train Loss: 6.3831, Val Loss: 6.3625, LR: 0.000001
2025-10-21 17:05:53,289 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 17:05:53,440 - INFO - Batch 0, Loss: 6.2683, LR: 0.000001
2025-10-21 17:06:07,713 - INFO - Batch 100, Loss: 6.1755, LR: 0.000001
2025-10-21 17:06:21,997 - INFO - Batch 200, Loss: 6.4216, LR: 0.000001
2025-10-21 17:06:36,283 - INFO - Batch 300, Loss: 6.3093, LR: 0.000001
2025-10-21 17:06:50,546 - INFO - Batch 400, Loss: 6.4515, LR: 0.000001
2025-10-21 17:07:04,782 - INFO - Batch 500, Loss: 6.3592, LR: 0.000001
2025-10-21 17:07:19,126 - INFO - Batch 600, Loss: 6.3666, LR: 0.000001
2025-10-21 17:07:33,398 - INFO - Batch 700, Loss: 6.2800, LR: 0.000001
2025-10-21 17:07:47,620 - INFO - Batch 800, Loss: 6.3896, LR: 0.000001
2025-10-21 17:08:01,889 - INFO - Batch 900, Loss: 6.4353, LR: 0.000001
2025-10-21 17:08:16,158 - INFO - Batch 1000, Loss: 6.3376, LR: 0.000001
2025-10-21 17:08:30,453 - INFO - Batch 1100, Loss: 6.5066, LR: 0.000001
2025-10-21 17:08:44,712 - INFO - Batch 1200, Loss: 6.2612, LR: 0.000001
2025-10-21 17:08:59,012 - INFO - Batch 1300, Loss: 6.2738, LR: 0.000001
2025-10-21 17:09:13,285 - INFO - Batch 1400, Loss: 6.3879, LR: 0.000001
2025-10-21 17:09:27,553 - INFO - Batch 1500, Loss: 6.2178, LR: 0.000001
2025-10-21 17:09:41,855 - INFO - Batch 1600, Loss: 6.3613, LR: 0.000001
2025-10-21 17:09:56,204 - INFO - Batch 1700, Loss: 6.2314, LR: 0.000001
2025-10-21 17:10:10,457 - INFO - Batch 1800, Loss: 6.3002, LR: 0.000001
2025-10-21 17:10:24,765 - INFO - Batch 1900, Loss: 6.3946, LR: 0.000001
2025-10-21 17:10:39,056 - INFO - Batch 2000, Loss: 6.2827, LR: 0.000001
2025-10-21 17:10:53,377 - INFO - Batch 2100, Loss: 6.3651, LR: 0.000001
2025-10-21 17:11:07,709 - INFO - Batch 2200, Loss: 6.4508, LR: 0.000001
2025-10-21 17:11:22,004 - INFO - Batch 2300, Loss: 6.1260, LR: 0.000001
2025-10-21 17:11:36,295 - INFO - Batch 2400, Loss: 6.2414, LR: 0.000001
2025-10-21 17:11:50,633 - INFO - Batch 2500, Loss: 6.2778, LR: 0.000001
2025-10-21 17:12:04,914 - INFO - Batch 2600, Loss: 6.2200, LR: 0.000001
2025-10-21 17:12:19,266 - INFO - Batch 2700, Loss: 6.3497, LR: 0.000001
2025-10-21 17:12:33,598 - INFO - Batch 2800, Loss: 6.3057, LR: 0.000001
2025-10-21 17:12:47,920 - INFO - Batch 2900, Loss: 6.3263, LR: 0.000001
2025-10-21 17:13:02,252 - INFO - Batch 3000, Loss: 6.1833, LR: 0.000001
2025-10-21 17:13:16,661 - INFO - Batch 3100, Loss: 6.3319, LR: 0.000001
2025-10-21 17:13:31,091 - INFO - Batch 3200, Loss: 6.1631, LR: 0.000001
2025-10-21 17:13:45,472 - INFO - Batch 3300, Loss: 6.3196, LR: 0.000001
2025-10-21 17:13:59,854 - INFO - Batch 3400, Loss: 6.4423, LR: 0.000001
2025-10-21 17:14:14,299 - INFO - Batch 3500, Loss: 6.2477, LR: 0.000001
2025-10-21 17:14:28,857 - INFO - Batch 3600, Loss: 6.1372, LR: 0.000001
2025-10-21 17:14:43,326 - INFO - Batch 3700, Loss: 6.1834, LR: 0.000001
2025-10-21 17:14:57,814 - INFO - Batch 3800, Loss: 6.3925, LR: 0.000001
2025-10-21 17:15:12,189 - INFO - Batch 3900, Loss: 6.3045, LR: 0.000001
2025-10-21 17:15:26,509 - INFO - Batch 4000, Loss: 6.2838, LR: 0.000001
2025-10-21 17:15:40,804 - INFO - Batch 4100, Loss: 6.3708, LR: 0.000001
2025-10-21 17:15:55,088 - INFO - Batch 4200, Loss: 6.1503, LR: 0.000001
2025-10-21 17:16:09,388 - INFO - Batch 4300, Loss: 6.3858, LR: 0.000001
2025-10-21 17:16:23,656 - INFO - Batch 4400, Loss: 6.3419, LR: 0.000001
2025-10-21 17:16:38,241 - INFO - Batch 4500, Loss: 6.2230, LR: 0.000001
2025-10-21 17:16:52,612 - INFO - Batch 4600, Loss: 6.1182, LR: 0.000001
2025-10-21 17:17:07,201 - INFO - Batch 4700, Loss: 6.1875, LR: 0.000001
2025-10-21 17:17:21,667 - INFO - Batch 4800, Loss: 6.2821, LR: 0.000001
2025-10-21 17:17:35,776 - INFO - Batch 4900, Loss: 6.3062, LR: 0.000001
2025-10-21 17:17:49,922 - INFO - Batch 5000, Loss: 6.3040, LR: 0.000001
2025-10-21 17:18:04,048 - INFO - Batch 5100, Loss: 6.2825, LR: 0.000001
2025-10-21 17:18:18,174 - INFO - Batch 5200, Loss: 6.1424, LR: 0.000001
2025-10-21 17:18:32,313 - INFO - Batch 5300, Loss: 6.0400, LR: 0.000001
2025-10-21 17:18:46,428 - INFO - Batch 5400, Loss: 6.2668, LR: 0.000001
2025-10-21 17:19:00,539 - INFO - Batch 5500, Loss: 6.2950, LR: 0.000001
2025-10-21 17:19:14,657 - INFO - Batch 5600, Loss: 6.2513, LR: 0.000001
2025-10-21 17:19:28,791 - INFO - Batch 5700, Loss: 6.2987, LR: 0.000001
2025-10-21 17:19:42,938 - INFO - Batch 5800, Loss: 6.1875, LR: 0.000001
2025-10-21 17:19:57,060 - INFO - Batch 5900, Loss: 6.2403, LR: 0.000001
2025-10-21 17:20:11,195 - INFO - Batch 6000, Loss: 6.2316, LR: 0.000001
2025-10-21 17:20:25,289 - INFO - Batch 6100, Loss: 6.1778, LR: 0.000001
2025-10-21 17:20:39,418 - INFO - Batch 6200, Loss: 6.3553, LR: 0.000001
2025-10-21 17:20:53,562 - INFO - Batch 6300, Loss: 6.3026, LR: 0.000001
2025-10-21 17:21:07,686 - INFO - Batch 6400, Loss: 6.2488, LR: 0.000001
2025-10-21 17:21:14,713 - INFO - Epoch 4/30: Train Loss: 6.3028, Val Loss: 6.2867, LR: 0.000001
2025-10-21 17:21:14,946 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 17:21:15,096 - INFO - Batch 0, Loss: 6.2126, LR: 0.000001
2025-10-21 17:21:29,357 - INFO - Batch 100, Loss: 6.2791, LR: 0.000001
2025-10-21 17:21:43,558 - INFO - Batch 200, Loss: 6.3312, LR: 0.000001
2025-10-21 17:21:57,793 - INFO - Batch 300, Loss: 6.3631, LR: 0.000001
2025-10-21 17:22:12,066 - INFO - Batch 400, Loss: 6.2069, LR: 0.000001
2025-10-21 17:22:26,374 - INFO - Batch 500, Loss: 6.1694, LR: 0.000001
2025-10-21 17:22:40,737 - INFO - Batch 600, Loss: 6.2111, LR: 0.000001
2025-10-21 17:22:50,237 - INFO - 使用设备: cuda:2
2025-10-21 17:22:50,238 - INFO - 加载数据...
2025-10-21 17:22:50,238 - INFO - 加载训练数据...
2025-10-21 17:22:54,037 - INFO - 成功加载 data/IWSLT2017/iwslt2017_train.pkl
2025-10-21 17:22:54,038 - INFO - 训练数据结构:
2025-10-21 17:22:54,038 - INFO - 数据结构分析:
2025-10-21 17:22:54,038 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 17:22:54,038 - INFO - 
英语相关语言对:
2025-10-21 17:22:54,038 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 17:22:54,038 - INFO -   en->de: 206112 个样本
2025-10-21 17:22:54,038 - INFO - 
德语相关语言对:
2025-10-21 17:22:54,038 - INFO - 德语->其他语言: ['en']
2025-10-21 17:22:54,038 - INFO -   de->en: 206112 个样本
2025-10-21 17:22:54,038 - INFO - 提取 en->de 的翻译对
2025-10-21 17:22:54,038 - INFO - 找到 en->de: 206112 个样本
2025-10-21 17:22:54,072 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 17:22:54,072 - INFO - 加载验证数据...
2025-10-21 17:22:54,104 - INFO - 成功加载 data/IWSLT2017/iwslt2017_validation.pkl
2025-10-21 17:22:54,104 - INFO - 提取 en->de 的翻译对
2025-10-21 17:22:54,104 - INFO - 找到 en->de: 888 个样本
2025-10-21 17:22:54,104 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 17:22:54,104 - INFO - 加载测试数据...
2025-10-21 17:22:54,188 - INFO - 成功加载 data/IWSLT2017/iwslt2017_test.pkl
2025-10-21 17:22:54,188 - INFO - 提取 en->de 的翻译对
2025-10-21 17:22:54,188 - INFO - 找到 en->de: 8079 个样本
2025-10-21 17:22:54,190 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 17:22:54,190 - INFO - 开始构建分词器...
2025-10-21 17:22:54,221 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 17:22:55,161 - INFO - Batch 700, Loss: 6.1258, LR: 0.000001
2025-10-21 17:22:55,846 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 17:22:58,918 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 17:22:58,979 - INFO - 数据加载器创建完成
2025-10-21 17:22:58,979 - INFO - 训练集: 206112 个样本
2025-10-21 17:22:58,979 - INFO - 验证集: 888 个样本
2025-10-21 17:22:58,979 - INFO - 测试集: 8079 个样本
2025-10-21 17:23:00,668 - INFO - 源语言词汇表大小: 5000
2025-10-21 17:23:00,668 - INFO - 目标语言词汇表大小: 5000
2025-10-21 17:23:00,669 - INFO - 创建模型...
2025-10-21 17:23:03,205 - INFO - 总参数数: 15,041,416
2025-10-21 17:23:03,205 - INFO - 可训练参数数: 15,041,416
2025-10-21 17:23:03,206 - INFO - 开始训练...
2025-10-21 17:23:05,589 - INFO - Batch 0, Loss: 8.5653, LR: 0.000000
2025-10-21 17:23:14,310 - INFO - Batch 800, Loss: 6.1909, LR: 0.000001
2025-10-21 17:23:24,674 - INFO - Batch 100, Loss: 8.5455, LR: 0.000000
2025-10-21 17:23:39,062 - INFO - Batch 200, Loss: 8.4772, LR: 0.000000
2025-10-21 17:23:53,421 - INFO - Batch 300, Loss: 8.3519, LR: 0.000000
2025-10-21 18:36:13,009 - INFO - 使用设备: cuda:2
2025-10-21 18:36:13,009 - INFO - 加载数据...
2025-10-21 18:36:13,009 - INFO - 加载训练数据...
2025-10-21 18:36:16,815 - INFO - 成功加载 data/IWSLT2017/iwslt2017_train.pkl
2025-10-21 18:36:16,815 - INFO - 训练数据结构:
2025-10-21 18:36:16,815 - INFO - 数据结构分析:
2025-10-21 18:36:16,815 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 18:36:16,815 - INFO - 
英语相关语言对:
2025-10-21 18:36:16,815 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 18:36:16,815 - INFO -   en->de: 206112 个样本
2025-10-21 18:36:16,815 - INFO - 
德语相关语言对:
2025-10-21 18:36:16,815 - INFO - 德语->其他语言: ['en']
2025-10-21 18:36:16,815 - INFO -   de->en: 206112 个样本
2025-10-21 18:36:16,815 - INFO - 提取 en->de 的翻译对
2025-10-21 18:36:16,815 - INFO - 找到 en->de: 206112 个样本
2025-10-21 18:36:16,850 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 18:36:16,850 - INFO - 加载验证数据...
2025-10-21 18:36:16,882 - INFO - 成功加载 data/IWSLT2017/iwslt2017_validation.pkl
2025-10-21 18:36:16,882 - INFO - 提取 en->de 的翻译对
2025-10-21 18:36:16,882 - INFO - 找到 en->de: 888 个样本
2025-10-21 18:36:16,882 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 18:36:16,882 - INFO - 加载测试数据...
2025-10-21 18:36:16,966 - INFO - 成功加载 data/IWSLT2017/iwslt2017_test.pkl
2025-10-21 18:36:16,966 - INFO - 提取 en->de 的翻译对
2025-10-21 18:36:16,966 - INFO - 找到 en->de: 8079 个样本
2025-10-21 18:36:16,967 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 18:36:16,967 - INFO - 开始构建分词器...
2025-10-21 18:36:16,998 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 18:36:18,583 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 18:36:21,544 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 18:36:21,605 - INFO - 数据加载器创建完成
2025-10-21 18:36:21,605 - INFO - 训练集: 206112 个样本
2025-10-21 18:36:21,605 - INFO - 验证集: 888 个样本
2025-10-21 18:36:21,605 - INFO - 测试集: 8079 个样本
2025-10-21 18:36:22,807 - INFO - 源语言词汇表大小: 5000
2025-10-21 18:36:22,808 - INFO - 目标语言词汇表大小: 5000
2025-10-21 18:36:22,808 - INFO - 创建模型...
2025-10-21 18:36:24,223 - INFO - 总参数数: 15,041,416
2025-10-21 18:36:24,223 - INFO - 可训练参数数: 15,041,416
2025-10-21 18:36:24,224 - INFO - 开始训练...
2025-10-21 18:36:25,269 - INFO - Batch 0, Loss: 8.5653, LR: 0.000000
2025-10-21 18:36:39,192 - INFO - Batch 100, Loss: 8.0509, LR: 0.000005
2025-10-21 18:36:53,217 - INFO - Batch 200, Loss: 7.5752, LR: 0.000010
2025-10-21 18:37:07,205 - INFO - Batch 300, Loss: 7.1791, LR: 0.000015
2025-10-21 18:37:21,282 - INFO - Batch 400, Loss: 6.8169, LR: 0.000020
2025-10-21 18:37:35,401 - INFO - Batch 500, Loss: 6.8743, LR: 0.000025
2025-10-21 18:37:49,510 - INFO - Batch 600, Loss: 6.7779, LR: 0.000030
2025-10-21 18:38:03,615 - INFO - Batch 700, Loss: 6.6596, LR: 0.000035
2025-10-21 18:38:17,680 - INFO - Batch 800, Loss: 6.4064, LR: 0.000040
2025-10-21 18:38:31,792 - INFO - Batch 900, Loss: 6.4369, LR: 0.000045
2025-10-21 18:38:45,949 - INFO - Batch 1000, Loss: 6.5548, LR: 0.000050
2025-10-21 18:39:00,078 - INFO - Batch 1100, Loss: 6.3132, LR: 0.000055
2025-10-21 18:39:14,228 - INFO - Batch 1200, Loss: 6.2779, LR: 0.000060
2025-10-21 18:39:28,337 - INFO - Batch 1300, Loss: 6.2803, LR: 0.000065
2025-10-21 18:39:42,468 - INFO - Batch 1400, Loss: 6.2290, LR: 0.000070
2025-10-21 18:39:56,582 - INFO - Batch 1500, Loss: 5.9483, LR: 0.000075
2025-10-21 18:40:10,705 - INFO - Batch 1600, Loss: 6.1133, LR: 0.000080
2025-10-21 18:40:24,884 - INFO - Batch 1700, Loss: 5.9900, LR: 0.000085
2025-10-21 18:40:39,036 - INFO - Batch 1800, Loss: 5.7866, LR: 0.000090
2025-10-21 18:40:53,201 - INFO - Batch 1900, Loss: 5.9871, LR: 0.000095
2025-10-21 18:41:07,381 - INFO - Batch 2000, Loss: 5.7788, LR: 0.000100
2025-10-21 18:41:21,518 - INFO - Batch 2100, Loss: 5.8143, LR: 0.000100
2025-10-21 18:41:35,693 - INFO - Batch 2200, Loss: 5.6029, LR: 0.000100
2025-10-21 18:41:49,905 - INFO - Batch 2300, Loss: 5.5297, LR: 0.000100
2025-10-21 18:42:04,081 - INFO - Batch 2400, Loss: 5.6362, LR: 0.000100
2025-10-21 18:42:18,292 - INFO - Batch 2500, Loss: 5.4903, LR: 0.000100
2025-10-21 18:42:32,495 - INFO - Batch 2600, Loss: 5.5437, LR: 0.000100
2025-10-21 18:42:46,670 - INFO - Batch 2700, Loss: 5.4360, LR: 0.000100
2025-10-21 18:43:00,843 - INFO - Batch 2800, Loss: 5.2621, LR: 0.000100
2025-10-21 18:43:14,999 - INFO - Batch 2900, Loss: 5.3452, LR: 0.000100
2025-10-21 18:43:29,155 - INFO - Batch 3000, Loss: 5.3715, LR: 0.000100
2025-10-21 18:43:43,252 - INFO - Batch 3100, Loss: 5.5146, LR: 0.000100
2025-10-21 18:43:57,407 - INFO - Batch 3200, Loss: 5.4413, LR: 0.000100
2025-10-21 18:44:11,554 - INFO - Batch 3300, Loss: 5.4802, LR: 0.000100
2025-10-21 18:44:25,676 - INFO - Batch 3400, Loss: 5.2238, LR: 0.000100
2025-10-21 18:44:39,805 - INFO - Batch 3500, Loss: 5.2114, LR: 0.000100
2025-10-21 18:44:53,937 - INFO - Batch 3600, Loss: 5.0777, LR: 0.000100
2025-10-21 18:45:08,091 - INFO - Batch 3700, Loss: 5.3464, LR: 0.000100
2025-10-21 18:45:22,259 - INFO - Batch 3800, Loss: 5.0748, LR: 0.000100
2025-10-21 18:45:36,426 - INFO - Batch 3900, Loss: 5.1715, LR: 0.000100
2025-10-21 18:45:50,533 - INFO - Batch 4000, Loss: 5.1947, LR: 0.000100
2025-10-21 18:46:04,672 - INFO - Batch 4100, Loss: 5.0968, LR: 0.000100
2025-10-21 18:46:18,791 - INFO - Batch 4200, Loss: 5.0386, LR: 0.000100
2025-10-21 18:46:32,944 - INFO - Batch 4300, Loss: 5.1186, LR: 0.000100
2025-10-21 18:46:47,078 - INFO - Batch 4400, Loss: 5.1654, LR: 0.000100
2025-10-21 18:47:01,178 - INFO - Batch 4500, Loss: 5.1408, LR: 0.000100
2025-10-21 18:47:15,301 - INFO - Batch 4600, Loss: 5.1630, LR: 0.000100
2025-10-21 18:47:29,441 - INFO - Batch 4700, Loss: 4.9566, LR: 0.000100
2025-10-21 18:47:43,521 - INFO - Batch 4800, Loss: 5.0587, LR: 0.000100
2025-10-21 18:47:57,597 - INFO - Batch 4900, Loss: 5.0318, LR: 0.000100
2025-10-21 18:48:11,670 - INFO - Batch 5000, Loss: 5.1464, LR: 0.000100
2025-10-21 18:48:25,744 - INFO - Batch 5100, Loss: 4.9704, LR: 0.000100
2025-10-21 18:48:39,776 - INFO - Batch 5200, Loss: 5.1814, LR: 0.000100
2025-10-21 18:48:53,792 - INFO - Batch 5300, Loss: 4.7201, LR: 0.000100
2025-10-21 18:49:07,918 - INFO - Batch 5400, Loss: 4.5806, LR: 0.000100
2025-10-21 18:49:22,316 - INFO - Batch 5500, Loss: 4.7734, LR: 0.000100
2025-10-21 18:49:36,605 - INFO - Batch 5600, Loss: 4.9144, LR: 0.000100
2025-10-21 18:49:50,679 - INFO - Batch 5700, Loss: 4.6365, LR: 0.000100
2025-10-21 18:50:04,793 - INFO - Batch 5800, Loss: 4.8793, LR: 0.000100
2025-10-21 18:50:18,845 - INFO - Batch 5900, Loss: 5.0637, LR: 0.000100
2025-10-21 18:50:32,934 - INFO - Batch 6000, Loss: 4.8647, LR: 0.000100
2025-10-21 18:50:47,036 - INFO - Batch 6100, Loss: 4.9888, LR: 0.000100
2025-10-21 18:51:01,095 - INFO - Batch 6200, Loss: 5.0413, LR: 0.000100
2025-10-21 18:51:15,345 - INFO - Batch 6300, Loss: 4.7921, LR: 0.000100
2025-10-21 18:51:29,747 - INFO - Batch 6400, Loss: 4.8954, LR: 0.000100
2025-10-21 18:51:36,967 - INFO - Epoch 1/30: Train Loss: 5.6219, Val Loss: 4.8213, LR: 0.000100
2025-10-21 18:51:37,354 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 18:51:37,509 - INFO - Batch 0, Loss: 4.7617, LR: 0.000100
2025-10-21 18:51:51,683 - INFO - Batch 100, Loss: 4.4717, LR: 0.000100
2025-10-21 18:52:05,926 - INFO - Batch 200, Loss: 4.6500, LR: 0.000100
2025-10-21 18:52:20,119 - INFO - Batch 300, Loss: 4.7964, LR: 0.000100
2025-10-21 18:52:34,369 - INFO - Batch 400, Loss: 4.5035, LR: 0.000100
2025-10-21 18:52:48,575 - INFO - Batch 500, Loss: 4.8258, LR: 0.000100
2025-10-21 18:53:02,793 - INFO - Batch 600, Loss: 4.8963, LR: 0.000100
2025-10-21 18:53:16,934 - INFO - Batch 700, Loss: 4.8773, LR: 0.000100
2025-10-21 18:53:31,044 - INFO - Batch 800, Loss: 4.6107, LR: 0.000100
2025-10-21 18:53:45,281 - INFO - Batch 900, Loss: 4.7964, LR: 0.000100
2025-10-21 18:53:59,582 - INFO - Batch 1000, Loss: 4.8976, LR: 0.000100
2025-10-21 18:54:13,870 - INFO - Batch 1100, Loss: 4.7680, LR: 0.000100
2025-10-21 18:54:28,070 - INFO - Batch 1200, Loss: 4.6583, LR: 0.000100
2025-10-21 18:54:42,428 - INFO - Batch 1300, Loss: 4.7559, LR: 0.000100
2025-10-21 18:54:56,216 - INFO - Batch 1400, Loss: 4.6659, LR: 0.000100
2025-10-21 18:55:10,355 - INFO - Batch 1500, Loss: 4.4233, LR: 0.000100
2025-10-21 18:55:24,492 - INFO - Batch 1600, Loss: 4.6145, LR: 0.000100
2025-10-21 18:55:38,262 - INFO - Batch 1700, Loss: 4.4037, LR: 0.000100
2025-10-21 18:55:52,176 - INFO - Batch 1800, Loss: 4.6557, LR: 0.000100
2025-10-21 18:56:06,046 - INFO - Batch 1900, Loss: 4.5128, LR: 0.000100
2025-10-21 18:56:20,184 - INFO - Batch 2000, Loss: 4.6318, LR: 0.000100
2025-10-21 18:56:34,295 - INFO - Batch 2100, Loss: 4.3457, LR: 0.000100
2025-10-21 18:56:48,508 - INFO - Batch 2200, Loss: 4.3940, LR: 0.000100
2025-10-21 18:57:02,674 - INFO - Batch 2300, Loss: 4.5513, LR: 0.000100
2025-10-21 18:57:16,903 - INFO - Batch 2400, Loss: 4.7467, LR: 0.000100
2025-10-21 18:57:31,210 - INFO - Batch 2500, Loss: 4.3580, LR: 0.000100
2025-10-21 18:57:45,578 - INFO - Batch 2600, Loss: 4.5245, LR: 0.000100
2025-10-21 18:57:59,840 - INFO - Batch 2700, Loss: 4.5995, LR: 0.000100
2025-10-21 18:58:13,962 - INFO - Batch 2800, Loss: 4.4915, LR: 0.000100
2025-10-21 18:58:28,283 - INFO - Batch 2900, Loss: 4.3384, LR: 0.000100
2025-10-21 18:58:42,253 - INFO - Batch 3000, Loss: 4.5112, LR: 0.000100
2025-10-21 18:58:56,362 - INFO - Batch 3100, Loss: 4.5762, LR: 0.000100
2025-10-21 18:59:10,581 - INFO - Batch 3200, Loss: 4.4811, LR: 0.000100
2025-10-21 18:59:24,758 - INFO - Batch 3300, Loss: 4.5587, LR: 0.000100
2025-10-21 18:59:38,933 - INFO - Batch 3400, Loss: 4.4625, LR: 0.000100
2025-10-21 18:59:53,249 - INFO - Batch 3500, Loss: 4.5318, LR: 0.000100
2025-10-21 19:00:07,634 - INFO - Batch 3600, Loss: 4.4896, LR: 0.000100
2025-10-21 19:00:21,857 - INFO - Batch 3700, Loss: 4.4924, LR: 0.000100
2025-10-21 19:00:36,191 - INFO - Batch 3800, Loss: 4.3166, LR: 0.000100
2025-10-21 19:00:50,467 - INFO - Batch 3900, Loss: 4.3165, LR: 0.000100
2025-10-21 19:01:04,720 - INFO - Batch 4000, Loss: 4.5506, LR: 0.000100
2025-10-21 19:01:19,001 - INFO - Batch 4100, Loss: 4.3345, LR: 0.000100
2025-10-21 19:01:33,310 - INFO - Batch 4200, Loss: 4.3632, LR: 0.000099
2025-10-21 19:01:47,443 - INFO - Batch 4300, Loss: 4.5377, LR: 0.000099
2025-10-21 19:02:01,581 - INFO - Batch 4400, Loss: 4.0486, LR: 0.000099
2025-10-21 19:02:15,746 - INFO - Batch 4500, Loss: 4.0846, LR: 0.000099
2025-10-21 19:02:30,210 - INFO - Batch 4600, Loss: 4.4516, LR: 0.000099
2025-10-21 19:02:44,587 - INFO - Batch 4700, Loss: 4.5330, LR: 0.000099
2025-10-21 19:02:58,869 - INFO - Batch 4800, Loss: 4.3386, LR: 0.000099
2025-10-21 19:03:13,105 - INFO - Batch 4900, Loss: 4.4036, LR: 0.000099
2025-10-21 19:03:27,478 - INFO - Batch 5000, Loss: 4.1006, LR: 0.000099
2025-10-21 19:03:41,711 - INFO - Batch 5100, Loss: 4.1783, LR: 0.000099
2025-10-21 19:03:55,921 - INFO - Batch 5200, Loss: 4.5671, LR: 0.000099
2025-10-21 19:04:10,077 - INFO - Batch 5300, Loss: 4.1878, LR: 0.000099
2025-10-21 19:04:24,251 - INFO - Batch 5400, Loss: 4.4491, LR: 0.000099
2025-10-21 19:04:38,717 - INFO - Batch 5500, Loss: 4.3341, LR: 0.000099
2025-10-21 19:04:52,921 - INFO - Batch 5600, Loss: 4.2817, LR: 0.000099
2025-10-21 19:05:07,189 - INFO - Batch 5700, Loss: 4.1884, LR: 0.000099
2025-10-21 19:05:21,384 - INFO - Batch 5800, Loss: 4.4921, LR: 0.000099
2025-10-21 19:05:35,597 - INFO - Batch 5900, Loss: 3.9791, LR: 0.000099
2025-10-21 19:05:49,793 - INFO - Batch 6000, Loss: 4.2923, LR: 0.000099
2025-10-21 19:06:03,986 - INFO - Batch 6100, Loss: 4.3514, LR: 0.000099
2025-10-21 19:06:18,116 - INFO - Batch 6200, Loss: 4.1622, LR: 0.000099
2025-10-21 19:06:32,428 - INFO - Batch 6300, Loss: 4.2309, LR: 0.000099
2025-10-21 19:06:46,825 - INFO - Batch 6400, Loss: 4.0482, LR: 0.000099
2025-10-21 19:06:53,991 - INFO - Epoch 2/30: Train Loss: 4.4856, Val Loss: 4.2386, LR: 0.000099
2025-10-21 19:06:54,224 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 19:06:54,374 - INFO - Batch 0, Loss: 4.1681, LR: 0.000099
2025-10-21 19:07:08,642 - INFO - Batch 100, Loss: 4.2673, LR: 0.000099
2025-10-21 19:07:23,062 - INFO - Batch 200, Loss: 4.2057, LR: 0.000099
2025-10-21 19:07:37,561 - INFO - Batch 300, Loss: 4.2258, LR: 0.000099
2025-10-21 19:07:52,001 - INFO - Batch 400, Loss: 4.3188, LR: 0.000099
2025-10-21 19:08:06,323 - INFO - Batch 500, Loss: 4.1604, LR: 0.000099
2025-10-21 19:08:20,608 - INFO - Batch 600, Loss: 3.9922, LR: 0.000099
2025-10-21 19:08:34,860 - INFO - Batch 700, Loss: 4.2545, LR: 0.000099
2025-10-21 19:08:49,178 - INFO - Batch 800, Loss: 4.3196, LR: 0.000099
2025-10-21 19:09:03,526 - INFO - Batch 900, Loss: 4.1161, LR: 0.000099
2025-10-21 19:09:17,870 - INFO - Batch 1000, Loss: 4.1285, LR: 0.000099
2025-10-21 19:09:32,150 - INFO - Batch 1100, Loss: 4.2057, LR: 0.000099
2025-10-21 19:09:46,302 - INFO - Batch 1200, Loss: 4.1752, LR: 0.000099
2025-10-21 19:10:00,485 - INFO - Batch 1300, Loss: 4.0862, LR: 0.000099
2025-10-21 19:10:14,593 - INFO - Batch 1400, Loss: 4.2029, LR: 0.000099
2025-10-21 19:10:28,809 - INFO - Batch 1500, Loss: 4.2132, LR: 0.000099
2025-10-21 19:10:42,926 - INFO - Batch 1600, Loss: 4.2583, LR: 0.000099
2025-10-21 19:10:57,064 - INFO - Batch 1700, Loss: 4.0669, LR: 0.000099
2025-10-21 19:11:11,213 - INFO - Batch 1800, Loss: 4.0720, LR: 0.000099
2025-10-21 19:11:25,442 - INFO - Batch 1900, Loss: 3.8762, LR: 0.000099
2025-10-21 19:11:39,657 - INFO - Batch 2000, Loss: 4.2261, LR: 0.000099
2025-10-21 19:11:53,773 - INFO - Batch 2100, Loss: 4.1079, LR: 0.000099
2025-10-21 19:12:07,934 - INFO - Batch 2200, Loss: 4.0455, LR: 0.000099
2025-10-21 19:12:22,151 - INFO - Batch 2300, Loss: 4.1468, LR: 0.000099
2025-10-21 19:12:36,296 - INFO - Batch 2400, Loss: 4.1202, LR: 0.000099
2025-10-21 19:12:50,417 - INFO - Batch 2500, Loss: 4.1362, LR: 0.000099
2025-10-21 19:13:04,562 - INFO - Batch 2600, Loss: 4.0099, LR: 0.000099
2025-10-21 19:13:18,704 - INFO - Batch 2700, Loss: 4.1735, LR: 0.000099
2025-10-21 19:13:32,813 - INFO - Batch 2800, Loss: 3.9862, LR: 0.000099
2025-10-21 19:13:46,922 - INFO - Batch 2900, Loss: 4.3983, LR: 0.000099
2025-10-21 19:14:01,115 - INFO - Batch 3000, Loss: 4.0137, LR: 0.000099
2025-10-21 19:14:15,257 - INFO - Batch 3100, Loss: 4.1778, LR: 0.000099
2025-10-21 19:14:29,387 - INFO - Batch 3200, Loss: 4.0389, LR: 0.000099
2025-10-21 19:14:43,516 - INFO - Batch 3300, Loss: 3.9237, LR: 0.000099
2025-10-21 19:14:57,682 - INFO - Batch 3400, Loss: 3.9572, LR: 0.000099
2025-10-21 19:15:11,848 - INFO - Batch 3500, Loss: 4.1787, LR: 0.000099
2025-10-21 19:15:26,000 - INFO - Batch 3600, Loss: 4.0316, LR: 0.000099
2025-10-21 19:15:40,155 - INFO - Batch 3700, Loss: 3.7849, LR: 0.000099
2025-10-21 19:15:54,309 - INFO - Batch 3800, Loss: 4.0106, LR: 0.000099
2025-10-21 19:16:08,481 - INFO - Batch 3900, Loss: 4.0967, LR: 0.000099
2025-10-21 19:16:22,604 - INFO - Batch 4000, Loss: 4.1052, LR: 0.000099
2025-10-21 19:16:36,765 - INFO - Batch 4100, Loss: 4.0030, LR: 0.000098
2025-10-21 19:16:50,904 - INFO - Batch 4200, Loss: 4.0637, LR: 0.000098
2025-10-21 19:17:04,992 - INFO - Batch 4300, Loss: 4.2735, LR: 0.000098
2025-10-21 19:17:19,227 - INFO - Batch 4400, Loss: 4.1178, LR: 0.000098
2025-10-21 19:17:33,398 - INFO - Batch 4500, Loss: 4.0686, LR: 0.000098
2025-10-21 19:17:47,529 - INFO - Batch 4600, Loss: 3.9750, LR: 0.000098
2025-10-21 19:18:01,661 - INFO - Batch 4700, Loss: 4.0441, LR: 0.000098
2025-10-21 19:18:15,807 - INFO - Batch 4800, Loss: 3.8161, LR: 0.000098
2025-10-21 19:18:29,949 - INFO - Batch 4900, Loss: 3.9793, LR: 0.000098
2025-10-21 19:18:44,045 - INFO - Batch 5000, Loss: 3.8175, LR: 0.000098
2025-10-21 19:18:58,133 - INFO - Batch 5100, Loss: 4.1378, LR: 0.000098
2025-10-21 19:19:12,285 - INFO - Batch 5200, Loss: 3.7709, LR: 0.000098
2025-10-21 19:19:26,433 - INFO - Batch 5300, Loss: 3.7396, LR: 0.000098
2025-10-21 19:19:40,552 - INFO - Batch 5400, Loss: 3.9998, LR: 0.000098
2025-10-21 19:19:54,705 - INFO - Batch 5500, Loss: 4.0098, LR: 0.000098
2025-10-21 19:20:08,838 - INFO - Batch 5600, Loss: 4.1138, LR: 0.000098
2025-10-21 19:20:22,997 - INFO - Batch 5700, Loss: 3.7808, LR: 0.000098
2025-10-21 19:20:37,121 - INFO - Batch 5800, Loss: 3.8539, LR: 0.000098
2025-10-21 19:20:51,257 - INFO - Batch 5900, Loss: 3.9711, LR: 0.000098
2025-10-21 19:21:05,435 - INFO - Batch 6000, Loss: 3.5632, LR: 0.000098
2025-10-21 19:21:19,632 - INFO - Batch 6100, Loss: 3.8966, LR: 0.000098
2025-10-21 19:21:33,840 - INFO - Batch 6200, Loss: 3.7466, LR: 0.000098
2025-10-21 19:21:48,142 - INFO - Batch 6300, Loss: 3.8633, LR: 0.000098
2025-10-21 19:22:02,396 - INFO - Batch 6400, Loss: 3.7257, LR: 0.000098
2025-10-21 19:22:09,478 - INFO - Epoch 3/30: Train Loss: 4.0332, Val Loss: 3.7365, LR: 0.000098
2025-10-21 19:22:09,706 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 19:22:09,857 - INFO - Batch 0, Loss: 3.7215, LR: 0.000098
2025-10-21 19:22:24,192 - INFO - Batch 100, Loss: 3.4918, LR: 0.000098
2025-10-21 19:22:38,416 - INFO - Batch 200, Loss: 3.8992, LR: 0.000098
2025-10-21 19:22:52,661 - INFO - Batch 300, Loss: 3.7637, LR: 0.000098
2025-10-21 19:23:06,901 - INFO - Batch 400, Loss: 3.8901, LR: 0.000098
2025-10-21 19:23:21,158 - INFO - Batch 500, Loss: 3.6866, LR: 0.000098
2025-10-21 19:23:35,428 - INFO - Batch 600, Loss: 3.6637, LR: 0.000098
2025-10-21 19:23:49,636 - INFO - Batch 700, Loss: 3.5265, LR: 0.000098
2025-10-21 19:24:03,811 - INFO - Batch 800, Loss: 3.6224, LR: 0.000098
2025-10-21 19:24:18,031 - INFO - Batch 900, Loss: 3.7345, LR: 0.000098
2025-10-21 19:24:32,244 - INFO - Batch 1000, Loss: 3.7166, LR: 0.000098
2025-10-21 19:24:46,445 - INFO - Batch 1100, Loss: 3.8755, LR: 0.000098
2025-10-21 19:25:00,751 - INFO - Batch 1200, Loss: 3.6963, LR: 0.000098
2025-10-21 19:25:15,111 - INFO - Batch 1300, Loss: 3.5130, LR: 0.000098
2025-10-21 19:25:29,510 - INFO - Batch 1400, Loss: 3.6950, LR: 0.000098
2025-10-21 19:25:43,917 - INFO - Batch 1500, Loss: 3.4098, LR: 0.000098
2025-10-21 19:25:58,288 - INFO - Batch 1600, Loss: 3.7580, LR: 0.000098
2025-10-21 19:26:12,671 - INFO - Batch 1700, Loss: 3.6540, LR: 0.000098
2025-10-21 19:26:27,057 - INFO - Batch 1800, Loss: 3.5856, LR: 0.000098
2025-10-21 19:26:41,416 - INFO - Batch 1900, Loss: 3.6562, LR: 0.000098
2025-10-21 19:26:55,783 - INFO - Batch 2000, Loss: 3.5400, LR: 0.000098
2025-10-21 19:27:10,152 - INFO - Batch 2100, Loss: 3.4631, LR: 0.000097
2025-10-21 19:27:24,562 - INFO - Batch 2200, Loss: 3.6985, LR: 0.000097
2025-10-21 19:27:38,851 - INFO - Batch 2300, Loss: 3.1730, LR: 0.000097
2025-10-21 19:27:53,354 - INFO - Batch 2400, Loss: 3.4588, LR: 0.000097
2025-10-21 19:28:07,903 - INFO - Batch 2500, Loss: 3.3419, LR: 0.000097
2025-10-21 19:28:22,438 - INFO - Batch 2600, Loss: 3.3539, LR: 0.000097
2025-10-21 19:28:36,991 - INFO - Batch 2700, Loss: 3.7322, LR: 0.000097
2025-10-21 19:28:51,560 - INFO - Batch 2800, Loss: 3.6470, LR: 0.000097
2025-10-21 19:29:06,004 - INFO - Batch 2900, Loss: 3.4670, LR: 0.000097
2025-10-21 19:29:20,219 - INFO - Batch 3000, Loss: 3.3351, LR: 0.000097
2025-10-21 19:29:34,428 - INFO - Batch 3100, Loss: 3.4465, LR: 0.000097
2025-10-21 19:29:48,660 - INFO - Batch 3200, Loss: 3.5178, LR: 0.000097
2025-10-21 19:30:02,859 - INFO - Batch 3300, Loss: 3.4333, LR: 0.000097
2025-10-21 19:30:17,073 - INFO - Batch 3400, Loss: 3.6974, LR: 0.000097
2025-10-21 19:30:31,296 - INFO - Batch 3500, Loss: 3.5117, LR: 0.000097
2025-10-21 19:30:45,542 - INFO - Batch 3600, Loss: 3.4103, LR: 0.000097
2025-10-21 19:30:59,760 - INFO - Batch 3700, Loss: 3.4060, LR: 0.000097
2025-10-21 19:31:13,994 - INFO - Batch 3800, Loss: 3.3980, LR: 0.000097
2025-10-21 19:31:28,269 - INFO - Batch 3900, Loss: 3.4200, LR: 0.000097
2025-10-21 19:31:42,488 - INFO - Batch 4000, Loss: 3.4795, LR: 0.000097
2025-10-21 19:31:56,709 - INFO - Batch 4100, Loss: 3.6174, LR: 0.000097
2025-10-21 19:32:10,928 - INFO - Batch 4200, Loss: 3.4867, LR: 0.000097
2025-10-21 19:32:25,232 - INFO - Batch 4300, Loss: 3.6590, LR: 0.000097
2025-10-21 19:32:39,457 - INFO - Batch 4400, Loss: 3.3352, LR: 0.000097
2025-10-21 19:32:53,650 - INFO - Batch 4500, Loss: 3.2020, LR: 0.000097
2025-10-21 19:33:07,869 - INFO - Batch 4600, Loss: 3.3917, LR: 0.000097
2025-10-21 19:33:22,123 - INFO - Batch 4700, Loss: 3.5120, LR: 0.000097
2025-10-21 19:33:36,444 - INFO - Batch 4800, Loss: 3.2223, LR: 0.000097
2025-10-21 19:33:50,712 - INFO - Batch 4900, Loss: 3.4076, LR: 0.000097
2025-10-21 19:34:04,987 - INFO - Batch 5000, Loss: 3.3740, LR: 0.000097
2025-10-21 19:34:19,240 - INFO - Batch 5100, Loss: 3.3444, LR: 0.000097
2025-10-21 19:34:33,483 - INFO - Batch 5200, Loss: 3.2038, LR: 0.000097
2025-10-21 19:34:47,731 - INFO - Batch 5300, Loss: 3.2269, LR: 0.000097
2025-10-21 19:35:01,974 - INFO - Batch 5400, Loss: 3.4030, LR: 0.000097
2025-10-21 19:35:16,189 - INFO - Batch 5500, Loss: 3.3494, LR: 0.000097
2025-10-21 19:35:30,419 - INFO - Batch 5600, Loss: 3.2772, LR: 0.000096
2025-10-21 19:35:44,632 - INFO - Batch 5700, Loss: 3.4160, LR: 0.000096
2025-10-21 19:35:58,884 - INFO - Batch 5800, Loss: 3.4184, LR: 0.000096
2025-10-21 19:36:13,161 - INFO - Batch 5900, Loss: 3.4961, LR: 0.000096
2025-10-21 19:36:27,453 - INFO - Batch 6000, Loss: 3.1542, LR: 0.000096
2025-10-21 19:36:41,701 - INFO - Batch 6100, Loss: 3.2350, LR: 0.000096
2025-10-21 19:36:55,909 - INFO - Batch 6200, Loss: 3.4422, LR: 0.000096
2025-10-21 19:37:10,122 - INFO - Batch 6300, Loss: 3.3566, LR: 0.000096
2025-10-21 19:37:24,402 - INFO - Batch 6400, Loss: 3.1759, LR: 0.000096
2025-10-21 19:37:31,501 - INFO - Epoch 4/30: Train Loss: 3.5359, Val Loss: 3.3035, LR: 0.000096
2025-10-21 19:37:31,736 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 19:37:31,888 - INFO - Batch 0, Loss: 3.1603, LR: 0.000096
2025-10-21 19:37:46,137 - INFO - Batch 100, Loss: 3.3051, LR: 0.000096
2025-10-21 19:38:00,242 - INFO - Batch 200, Loss: 3.1824, LR: 0.000096
2025-10-21 19:38:14,191 - INFO - Batch 300, Loss: 3.2544, LR: 0.000096
2025-10-21 19:38:28,140 - INFO - Batch 400, Loss: 3.4330, LR: 0.000096
2025-10-21 19:38:42,094 - INFO - Batch 500, Loss: 3.0944, LR: 0.000096
2025-10-21 19:38:56,059 - INFO - Batch 600, Loss: 3.1687, LR: 0.000096
2025-10-21 19:39:10,015 - INFO - Batch 700, Loss: 3.1674, LR: 0.000096
2025-10-21 19:39:23,978 - INFO - Batch 800, Loss: 3.2712, LR: 0.000096
2025-10-21 19:39:37,956 - INFO - Batch 900, Loss: 3.2742, LR: 0.000096
2025-10-21 19:39:51,902 - INFO - Batch 1000, Loss: 3.1654, LR: 0.000096
2025-10-21 19:40:06,054 - INFO - Batch 1100, Loss: 3.3572, LR: 0.000096
2025-10-21 19:40:20,277 - INFO - Batch 1200, Loss: 3.0771, LR: 0.000096
2025-10-21 19:40:34,530 - INFO - Batch 1300, Loss: 3.0054, LR: 0.000096
2025-10-21 19:40:48,728 - INFO - Batch 1400, Loss: 3.4269, LR: 0.000096
2025-10-21 19:41:02,954 - INFO - Batch 1500, Loss: 3.1896, LR: 0.000096
2025-10-21 19:41:17,156 - INFO - Batch 1600, Loss: 3.3852, LR: 0.000096
2025-10-21 19:41:31,355 - INFO - Batch 1700, Loss: 3.2164, LR: 0.000096
2025-10-21 19:41:45,859 - INFO - Batch 1800, Loss: 3.1604, LR: 0.000096
2025-10-21 19:42:00,389 - INFO - Batch 1900, Loss: 3.3030, LR: 0.000096
2025-10-21 19:42:14,793 - INFO - Batch 2000, Loss: 3.4310, LR: 0.000096
2025-10-21 19:42:29,239 - INFO - Batch 2100, Loss: 3.1105, LR: 0.000096
2025-10-21 19:42:43,637 - INFO - Batch 2200, Loss: 3.2411, LR: 0.000096
2025-10-21 19:42:58,034 - INFO - Batch 2300, Loss: 3.4755, LR: 0.000095
2025-10-21 19:43:12,399 - INFO - Batch 2400, Loss: 3.0756, LR: 0.000095
2025-10-21 19:43:26,850 - INFO - Batch 2500, Loss: 3.4010, LR: 0.000095
2025-10-21 19:43:41,483 - INFO - Batch 2600, Loss: 3.1426, LR: 0.000095
2025-10-21 19:43:56,113 - INFO - Batch 2700, Loss: 3.4239, LR: 0.000095
2025-10-21 19:44:10,570 - INFO - Batch 2800, Loss: 3.1609, LR: 0.000095
2025-10-21 19:44:25,012 - INFO - Batch 2900, Loss: 3.2029, LR: 0.000095
2025-10-21 19:44:39,314 - INFO - Batch 3000, Loss: 3.2346, LR: 0.000095
2025-10-21 19:44:53,664 - INFO - Batch 3100, Loss: 3.3511, LR: 0.000095
2025-10-21 19:45:08,038 - INFO - Batch 3200, Loss: 3.1967, LR: 0.000095
2025-10-21 19:45:22,387 - INFO - Batch 3300, Loss: 3.2555, LR: 0.000095
2025-10-21 19:45:36,773 - INFO - Batch 3400, Loss: 3.3258, LR: 0.000095
2025-10-21 19:45:51,102 - INFO - Batch 3500, Loss: 3.5776, LR: 0.000095
2025-10-21 19:46:05,546 - INFO - Batch 3600, Loss: 3.1209, LR: 0.000095
2025-10-21 19:46:19,892 - INFO - Batch 3700, Loss: 3.2080, LR: 0.000095
2025-10-21 19:46:34,247 - INFO - Batch 3800, Loss: 3.1207, LR: 0.000095
2025-10-21 19:46:48,605 - INFO - Batch 3900, Loss: 2.9626, LR: 0.000095
2025-10-21 19:47:03,140 - INFO - Batch 4000, Loss: 3.2568, LR: 0.000095
2025-10-21 19:47:17,641 - INFO - Batch 4100, Loss: 3.2367, LR: 0.000095
2025-10-21 19:47:32,198 - INFO - Batch 4200, Loss: 3.0790, LR: 0.000095
2025-10-21 19:47:46,692 - INFO - Batch 4300, Loss: 3.1479, LR: 0.000095
2025-10-21 19:48:01,024 - INFO - Batch 4400, Loss: 3.0657, LR: 0.000095
2025-10-21 19:48:15,414 - INFO - Batch 4500, Loss: 2.9536, LR: 0.000095
2025-10-21 19:48:30,036 - INFO - Batch 4600, Loss: 3.0992, LR: 0.000095
2025-10-21 19:48:44,570 - INFO - Batch 4700, Loss: 3.0985, LR: 0.000095
2025-10-21 19:48:58,904 - INFO - Batch 4800, Loss: 3.1715, LR: 0.000095
2025-10-21 19:49:13,194 - INFO - Batch 4900, Loss: 3.2544, LR: 0.000095
2025-10-21 19:49:27,515 - INFO - Batch 5000, Loss: 3.1959, LR: 0.000095
2025-10-21 19:49:42,054 - INFO - Batch 5100, Loss: 2.9820, LR: 0.000094
2025-10-21 19:49:56,346 - INFO - Batch 5200, Loss: 3.0854, LR: 0.000094
2025-10-21 19:50:10,589 - INFO - Batch 5300, Loss: 2.9062, LR: 0.000094
2025-10-21 19:50:24,855 - INFO - Batch 5400, Loss: 3.1238, LR: 0.000094
2025-10-21 19:50:39,096 - INFO - Batch 5500, Loss: 3.1143, LR: 0.000094
2025-10-21 19:50:53,335 - INFO - Batch 5600, Loss: 3.0904, LR: 0.000094
2025-10-21 19:51:07,608 - INFO - Batch 5700, Loss: 3.0460, LR: 0.000094
2025-10-21 19:51:21,859 - INFO - Batch 5800, Loss: 3.0454, LR: 0.000094
2025-10-21 19:51:36,114 - INFO - Batch 5900, Loss: 3.0675, LR: 0.000094
2025-10-21 19:51:50,397 - INFO - Batch 6000, Loss: 3.0467, LR: 0.000094
2025-10-21 19:52:04,675 - INFO - Batch 6100, Loss: 3.1974, LR: 0.000094
2025-10-21 19:52:19,069 - INFO - Batch 6200, Loss: 3.2227, LR: 0.000094
2025-10-21 19:52:33,377 - INFO - Batch 6300, Loss: 2.9490, LR: 0.000094
2025-10-21 19:52:47,667 - INFO - Batch 6400, Loss: 2.8820, LR: 0.000094
2025-10-21 19:52:54,794 - INFO - Epoch 5/30: Train Loss: 3.1888, Val Loss: 2.9690, LR: 0.000094
2025-10-21 19:52:55,047 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 19:52:55,197 - INFO - Batch 0, Loss: 2.8579, LR: 0.000094
2025-10-21 19:53:09,461 - INFO - Batch 100, Loss: 3.0869, LR: 0.000094
2025-10-21 19:53:23,701 - INFO - Batch 200, Loss: 3.1424, LR: 0.000094
2025-10-21 19:53:37,949 - INFO - Batch 300, Loss: 3.0744, LR: 0.000094
2025-10-21 19:53:52,339 - INFO - Batch 400, Loss: 2.9490, LR: 0.000094
2025-10-21 19:54:06,910 - INFO - Batch 500, Loss: 2.9715, LR: 0.000094
2025-10-21 19:54:21,400 - INFO - Batch 600, Loss: 2.7058, LR: 0.000094
2025-10-21 19:54:35,649 - INFO - Batch 700, Loss: 2.8606, LR: 0.000094
2025-10-21 19:54:49,975 - INFO - Batch 800, Loss: 3.0882, LR: 0.000094
2025-10-21 19:55:04,284 - INFO - Batch 900, Loss: 3.0478, LR: 0.000094
2025-10-21 19:55:18,600 - INFO - Batch 1000, Loss: 2.8198, LR: 0.000094
2025-10-21 19:55:32,931 - INFO - Batch 1100, Loss: 3.0073, LR: 0.000094
2025-10-21 19:55:47,221 - INFO - Batch 1200, Loss: 2.9644, LR: 0.000093
2025-10-21 19:56:01,513 - INFO - Batch 1300, Loss: 3.1355, LR: 0.000093
2025-10-21 19:56:15,822 - INFO - Batch 1400, Loss: 2.9376, LR: 0.000093
2025-10-21 19:56:30,175 - INFO - Batch 1500, Loss: 3.0083, LR: 0.000093
2025-10-21 19:56:44,508 - INFO - Batch 1600, Loss: 2.9488, LR: 0.000093
2025-10-21 19:56:58,836 - INFO - Batch 1700, Loss: 2.7307, LR: 0.000093
2025-10-21 19:57:13,141 - INFO - Batch 1800, Loss: 3.0287, LR: 0.000093
2025-10-21 19:57:27,617 - INFO - Batch 1900, Loss: 2.8213, LR: 0.000093
2025-10-21 19:57:42,090 - INFO - Batch 2000, Loss: 3.1415, LR: 0.000093
2025-10-21 19:57:56,593 - INFO - Batch 2100, Loss: 2.8838, LR: 0.000093
2025-10-21 19:58:11,086 - INFO - Batch 2200, Loss: 2.6759, LR: 0.000093
2025-10-21 19:58:25,581 - INFO - Batch 2300, Loss: 2.9732, LR: 0.000093
2025-10-21 19:58:40,100 - INFO - Batch 2400, Loss: 3.0081, LR: 0.000093
2025-10-21 19:58:54,552 - INFO - Batch 2500, Loss: 3.0185, LR: 0.000093
2025-10-21 19:59:09,047 - INFO - Batch 2600, Loss: 2.8753, LR: 0.000093
2025-10-21 19:59:23,538 - INFO - Batch 2700, Loss: 2.9698, LR: 0.000093
2025-10-21 19:59:38,084 - INFO - Batch 2800, Loss: 2.9671, LR: 0.000093
2025-10-21 19:59:52,422 - INFO - Batch 2900, Loss: 2.8594, LR: 0.000093
2025-10-21 20:00:06,860 - INFO - Batch 3000, Loss: 3.1065, LR: 0.000093
2025-10-21 20:00:21,200 - INFO - Batch 3100, Loss: 3.0737, LR: 0.000093
2025-10-21 20:00:35,524 - INFO - Batch 3200, Loss: 3.1389, LR: 0.000093
2025-10-21 20:00:49,890 - INFO - Batch 3300, Loss: 2.5291, LR: 0.000093
2025-10-21 20:01:04,228 - INFO - Batch 3400, Loss: 2.6592, LR: 0.000093
2025-10-21 20:01:18,556 - INFO - Batch 3500, Loss: 2.6804, LR: 0.000093
2025-10-21 20:01:32,874 - INFO - Batch 3600, Loss: 3.1663, LR: 0.000092
2025-10-21 20:01:47,176 - INFO - Batch 3700, Loss: 2.6594, LR: 0.000092
2025-10-21 20:02:01,565 - INFO - Batch 3800, Loss: 2.8611, LR: 0.000092
2025-10-21 20:02:16,057 - INFO - Batch 3900, Loss: 3.2908, LR: 0.000092
2025-10-21 20:02:30,506 - INFO - Batch 4000, Loss: 3.1050, LR: 0.000092
2025-10-21 20:02:44,899 - INFO - Batch 4100, Loss: 3.0294, LR: 0.000092
2025-10-21 20:02:59,356 - INFO - Batch 4200, Loss: 2.8263, LR: 0.000092
2025-10-21 20:03:13,940 - INFO - Batch 4300, Loss: 3.0003, LR: 0.000092
2025-10-21 20:03:28,312 - INFO - Batch 4400, Loss: 2.8211, LR: 0.000092
2025-10-21 20:03:42,875 - INFO - Batch 4500, Loss: 2.9155, LR: 0.000092
2025-10-21 20:03:57,190 - INFO - Batch 4600, Loss: 3.0354, LR: 0.000092
2025-10-21 20:04:11,538 - INFO - Batch 4700, Loss: 2.8689, LR: 0.000092
2025-10-21 20:04:26,135 - INFO - Batch 4800, Loss: 3.0813, LR: 0.000092
2025-10-21 20:04:40,475 - INFO - Batch 4900, Loss: 2.9811, LR: 0.000092
2025-10-21 20:04:54,775 - INFO - Batch 5000, Loss: 2.9402, LR: 0.000092
2025-10-21 20:05:09,076 - INFO - Batch 5100, Loss: 2.8669, LR: 0.000092
2025-10-21 20:05:23,388 - INFO - Batch 5200, Loss: 2.7181, LR: 0.000092
2025-10-21 20:05:37,704 - INFO - Batch 5300, Loss: 2.8100, LR: 0.000092
2025-10-21 20:05:52,006 - INFO - Batch 5400, Loss: 3.0896, LR: 0.000092
2025-10-21 20:06:06,335 - INFO - Batch 5500, Loss: 2.9582, LR: 0.000092
2025-10-21 20:06:20,696 - INFO - Batch 5600, Loss: 2.6160, LR: 0.000092
2025-10-21 20:06:35,195 - INFO - Batch 5700, Loss: 2.8666, LR: 0.000092
2025-10-21 20:06:49,662 - INFO - Batch 5800, Loss: 2.8004, LR: 0.000092
2025-10-21 20:07:03,994 - INFO - Batch 5900, Loss: 2.7429, LR: 0.000091
2025-10-21 20:07:18,301 - INFO - Batch 6000, Loss: 3.0092, LR: 0.000091
2025-10-21 20:07:32,647 - INFO - Batch 6100, Loss: 2.9554, LR: 0.000091
2025-10-21 20:07:46,970 - INFO - Batch 6200, Loss: 2.8024, LR: 0.000091
2025-10-21 20:08:01,310 - INFO - Batch 6300, Loss: 2.6265, LR: 0.000091
2025-10-21 20:08:15,880 - INFO - Batch 6400, Loss: 2.6101, LR: 0.000091
2025-10-21 20:08:23,134 - INFO - Epoch 6/30: Train Loss: 2.9070, Val Loss: 2.7184, LR: 0.000091
2025-10-21 20:08:23,379 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 20:08:23,530 - INFO - Batch 0, Loss: 2.5769, LR: 0.000091
2025-10-21 20:08:38,106 - INFO - Batch 100, Loss: 2.8048, LR: 0.000091
2025-10-21 20:08:52,656 - INFO - Batch 200, Loss: 2.5750, LR: 0.000091
2025-10-21 20:09:07,230 - INFO - Batch 300, Loss: 2.2115, LR: 0.000091
2025-10-21 20:09:21,657 - INFO - Batch 400, Loss: 2.9020, LR: 0.000091
2025-10-21 20:09:36,002 - INFO - Batch 500, Loss: 2.8986, LR: 0.000091
2025-10-21 20:09:50,280 - INFO - Batch 600, Loss: 2.6083, LR: 0.000091
2025-10-21 20:10:04,573 - INFO - Batch 700, Loss: 2.5944, LR: 0.000091
2025-10-21 20:10:18,820 - INFO - Batch 800, Loss: 2.5621, LR: 0.000091
2025-10-21 20:10:33,022 - INFO - Batch 900, Loss: 2.5921, LR: 0.000091
2025-10-21 20:10:47,192 - INFO - Batch 1000, Loss: 2.3839, LR: 0.000091
2025-10-21 20:11:01,340 - INFO - Batch 1100, Loss: 3.0494, LR: 0.000091
2025-10-21 20:11:15,591 - INFO - Batch 1200, Loss: 2.5680, LR: 0.000091
2025-10-21 20:11:29,765 - INFO - Batch 1300, Loss: 2.7091, LR: 0.000091
2025-10-21 20:11:43,943 - INFO - Batch 1400, Loss: 2.6372, LR: 0.000091
2025-10-21 20:11:58,105 - INFO - Batch 1500, Loss: 2.5233, LR: 0.000090
2025-10-21 20:12:12,266 - INFO - Batch 1600, Loss: 2.5872, LR: 0.000090
2025-10-21 20:12:26,420 - INFO - Batch 1700, Loss: 2.7415, LR: 0.000090
2025-10-21 20:12:40,684 - INFO - Batch 1800, Loss: 2.7777, LR: 0.000090
2025-10-21 20:12:54,909 - INFO - Batch 1900, Loss: 2.6076, LR: 0.000090
2025-10-21 20:13:09,133 - INFO - Batch 2000, Loss: 2.8637, LR: 0.000090
2025-10-21 20:13:23,342 - INFO - Batch 2100, Loss: 2.6383, LR: 0.000090
2025-10-21 20:13:37,640 - INFO - Batch 2200, Loss: 2.7465, LR: 0.000090
2025-10-21 20:13:51,850 - INFO - Batch 2300, Loss: 3.0418, LR: 0.000090
2025-10-21 20:14:06,044 - INFO - Batch 2400, Loss: 2.7847, LR: 0.000090
2025-10-21 20:14:20,176 - INFO - Batch 2500, Loss: 3.0988, LR: 0.000090
2025-10-21 20:14:34,280 - INFO - Batch 2600, Loss: 2.5574, LR: 0.000090
2025-10-21 20:14:48,418 - INFO - Batch 2700, Loss: 2.7813, LR: 0.000090
2025-10-21 20:15:02,566 - INFO - Batch 2800, Loss: 2.7809, LR: 0.000090
2025-10-21 20:15:16,722 - INFO - Batch 2900, Loss: 2.6391, LR: 0.000090
2025-10-21 20:15:30,849 - INFO - Batch 3000, Loss: 3.0598, LR: 0.000090
2025-10-21 20:15:45,021 - INFO - Batch 3100, Loss: 2.9739, LR: 0.000090
2025-10-21 20:15:59,194 - INFO - Batch 3200, Loss: 2.6218, LR: 0.000090
2025-10-21 20:16:13,423 - INFO - Batch 3300, Loss: 2.9171, LR: 0.000090
2025-10-21 20:16:27,718 - INFO - Batch 3400, Loss: 2.6109, LR: 0.000090
2025-10-21 20:16:41,957 - INFO - Batch 3500, Loss: 2.5500, LR: 0.000090
2025-10-21 20:16:56,197 - INFO - Batch 3600, Loss: 2.6825, LR: 0.000089
2025-10-21 20:17:10,442 - INFO - Batch 3700, Loss: 2.8087, LR: 0.000089
2025-10-21 20:17:24,810 - INFO - Batch 3800, Loss: 2.8074, LR: 0.000089
2025-10-21 20:17:39,129 - INFO - Batch 3900, Loss: 3.0196, LR: 0.000089
2025-10-21 20:17:53,318 - INFO - Batch 4000, Loss: 2.3811, LR: 0.000089
2025-10-21 20:18:07,523 - INFO - Batch 4100, Loss: 2.4906, LR: 0.000089
2025-10-21 20:18:21,732 - INFO - Batch 4200, Loss: 2.5862, LR: 0.000089
2025-10-21 20:18:36,030 - INFO - Batch 4300, Loss: 3.0205, LR: 0.000089
2025-10-21 20:18:50,295 - INFO - Batch 4400, Loss: 2.7090, LR: 0.000089
2025-10-21 20:19:04,526 - INFO - Batch 4500, Loss: 2.5325, LR: 0.000089
2025-10-21 20:19:18,742 - INFO - Batch 4600, Loss: 2.6954, LR: 0.000089
2025-10-21 20:19:32,920 - INFO - Batch 4700, Loss: 2.4098, LR: 0.000089
2025-10-21 20:19:47,092 - INFO - Batch 4800, Loss: 2.7033, LR: 0.000089
2025-10-21 20:20:01,258 - INFO - Batch 4900, Loss: 2.7096, LR: 0.000089
2025-10-21 20:20:15,402 - INFO - Batch 5000, Loss: 2.3688, LR: 0.000089
2025-10-21 20:20:29,561 - INFO - Batch 5100, Loss: 2.8941, LR: 0.000089
2025-10-21 20:20:43,763 - INFO - Batch 5200, Loss: 2.6551, LR: 0.000089
2025-10-21 20:20:58,019 - INFO - Batch 5300, Loss: 2.9507, LR: 0.000089
2025-10-21 20:21:12,534 - INFO - Batch 5400, Loss: 2.4723, LR: 0.000089
2025-10-21 20:21:26,977 - INFO - Batch 5500, Loss: 2.2986, LR: 0.000088
2025-10-21 20:21:41,155 - INFO - Batch 5600, Loss: 2.8629, LR: 0.000088
2025-10-21 20:21:55,405 - INFO - Batch 5700, Loss: 2.9475, LR: 0.000088
2025-10-21 20:22:09,749 - INFO - Batch 5800, Loss: 2.5585, LR: 0.000088
2025-10-21 20:22:24,064 - INFO - Batch 5900, Loss: 2.5483, LR: 0.000088
2025-10-21 20:22:38,193 - INFO - Batch 6000, Loss: 2.6193, LR: 0.000088
2025-10-21 20:22:52,352 - INFO - Batch 6100, Loss: 2.7143, LR: 0.000088
2025-10-21 20:23:06,508 - INFO - Batch 6200, Loss: 2.6273, LR: 0.000088
2025-10-21 20:23:20,641 - INFO - Batch 6300, Loss: 2.7184, LR: 0.000088
2025-10-21 20:23:34,802 - INFO - Batch 6400, Loss: 2.7788, LR: 0.000088
2025-10-21 20:23:41,876 - INFO - Epoch 7/30: Train Loss: 2.7214, Val Loss: 2.5631, LR: 0.000088
2025-10-21 20:23:42,125 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 20:23:42,273 - INFO - Batch 0, Loss: 2.7289, LR: 0.000088
2025-10-21 20:23:56,867 - INFO - Batch 100, Loss: 2.5138, LR: 0.000088
2025-10-21 20:24:11,415 - INFO - Batch 200, Loss: 2.3577, LR: 0.000088
2025-10-21 20:24:25,943 - INFO - Batch 300, Loss: 2.8508, LR: 0.000088
2025-10-21 20:24:40,161 - INFO - Batch 400, Loss: 2.7534, LR: 0.000088
2025-10-21 20:24:54,342 - INFO - Batch 500, Loss: 2.7030, LR: 0.000088
2025-10-21 20:25:08,544 - INFO - Batch 600, Loss: 2.5033, LR: 0.000088
2025-10-21 20:25:22,757 - INFO - Batch 700, Loss: 2.4760, LR: 0.000088
2025-10-21 20:25:36,952 - INFO - Batch 800, Loss: 2.8280, LR: 0.000088
2025-10-21 20:25:51,152 - INFO - Batch 900, Loss: 2.7437, LR: 0.000088
2025-10-21 20:26:05,368 - INFO - Batch 1000, Loss: 2.6222, LR: 0.000087
2025-10-21 20:26:19,582 - INFO - Batch 1100, Loss: 2.7685, LR: 0.000087
2025-10-21 20:26:33,762 - INFO - Batch 1200, Loss: 2.5603, LR: 0.000087
2025-10-21 20:26:47,948 - INFO - Batch 1300, Loss: 2.8407, LR: 0.000087
2025-10-21 20:27:02,148 - INFO - Batch 1400, Loss: 2.6418, LR: 0.000087
2025-10-21 20:27:16,390 - INFO - Batch 1500, Loss: 2.6879, LR: 0.000087
2025-10-21 20:27:30,630 - INFO - Batch 1600, Loss: 2.5449, LR: 0.000087
2025-10-21 20:27:44,809 - INFO - Batch 1700, Loss: 2.4504, LR: 0.000087
2025-10-21 20:27:59,019 - INFO - Batch 1800, Loss: 2.6176, LR: 0.000087
2025-10-21 20:28:13,277 - INFO - Batch 1900, Loss: 2.3709, LR: 0.000087
2025-10-21 20:28:27,442 - INFO - Batch 2000, Loss: 2.4727, LR: 0.000087
2025-10-21 20:28:41,615 - INFO - Batch 2100, Loss: 2.3705, LR: 0.000087
2025-10-21 20:28:55,770 - INFO - Batch 2200, Loss: 2.4456, LR: 0.000087
2025-10-21 20:29:09,937 - INFO - Batch 2300, Loss: 2.6158, LR: 0.000087
2025-10-21 20:29:24,073 - INFO - Batch 2400, Loss: 2.2854, LR: 0.000087
2025-10-21 20:29:38,216 - INFO - Batch 2500, Loss: 2.5667, LR: 0.000087
2025-10-21 20:29:52,337 - INFO - Batch 2600, Loss: 2.6182, LR: 0.000087
2025-10-21 20:30:06,501 - INFO - Batch 2700, Loss: 2.5093, LR: 0.000087
2025-10-21 20:30:20,678 - INFO - Batch 2800, Loss: 2.3579, LR: 0.000086
2025-10-21 20:30:34,912 - INFO - Batch 2900, Loss: 2.5884, LR: 0.000086
2025-10-21 20:30:49,114 - INFO - Batch 3000, Loss: 2.3088, LR: 0.000086
2025-10-21 20:31:03,374 - INFO - Batch 3100, Loss: 2.7240, LR: 0.000086
2025-10-21 20:31:17,590 - INFO - Batch 3200, Loss: 2.6060, LR: 0.000086
2025-10-21 20:31:31,830 - INFO - Batch 3300, Loss: 2.6643, LR: 0.000086
2025-10-21 20:31:46,142 - INFO - Batch 3400, Loss: 2.8391, LR: 0.000086
2025-10-21 20:32:00,280 - INFO - Batch 3500, Loss: 2.4934, LR: 0.000086
2025-10-21 20:32:14,439 - INFO - Batch 3600, Loss: 2.7595, LR: 0.000086
2025-10-21 20:32:28,589 - INFO - Batch 3700, Loss: 2.7348, LR: 0.000086
2025-10-21 20:32:42,739 - INFO - Batch 3800, Loss: 2.3085, LR: 0.000086
2025-10-21 20:32:56,887 - INFO - Batch 3900, Loss: 2.7857, LR: 0.000086
2025-10-21 20:33:11,031 - INFO - Batch 4000, Loss: 2.5702, LR: 0.000086
2025-10-21 20:33:25,189 - INFO - Batch 4100, Loss: 2.4483, LR: 0.000086
2025-10-21 20:33:39,365 - INFO - Batch 4200, Loss: 2.7705, LR: 0.000086
2025-10-21 20:33:53,508 - INFO - Batch 4300, Loss: 2.5033, LR: 0.000086
2025-10-21 20:34:07,692 - INFO - Batch 4400, Loss: 2.4196, LR: 0.000086
2025-10-21 20:34:21,822 - INFO - Batch 4500, Loss: 2.6471, LR: 0.000085
2025-10-21 20:34:35,950 - INFO - Batch 4600, Loss: 2.4420, LR: 0.000085
2025-10-21 20:34:50,103 - INFO - Batch 4700, Loss: 2.6839, LR: 0.000085
2025-10-21 20:35:04,241 - INFO - Batch 4800, Loss: 2.7691, LR: 0.000085
2025-10-21 20:35:18,396 - INFO - Batch 4900, Loss: 2.4320, LR: 0.000085
2025-10-21 20:35:32,530 - INFO - Batch 5000, Loss: 2.7475, LR: 0.000085
2025-10-21 20:35:46,679 - INFO - Batch 5100, Loss: 2.7580, LR: 0.000085
2025-10-21 20:36:00,838 - INFO - Batch 5200, Loss: 2.6147, LR: 0.000085
2025-10-21 20:36:14,981 - INFO - Batch 5300, Loss: 2.6817, LR: 0.000085
2025-10-21 20:36:29,108 - INFO - Batch 5400, Loss: 2.3483, LR: 0.000085
2025-10-21 20:36:43,298 - INFO - Batch 5500, Loss: 2.4573, LR: 0.000085
2025-10-21 20:36:57,615 - INFO - Batch 5600, Loss: 2.4687, LR: 0.000085
2025-10-21 20:37:11,992 - INFO - Batch 5700, Loss: 2.6057, LR: 0.000085
2025-10-21 20:37:26,267 - INFO - Batch 5800, Loss: 2.8028, LR: 0.000085
2025-10-21 20:37:40,420 - INFO - Batch 5900, Loss: 2.7691, LR: 0.000085
2025-10-21 20:37:54,600 - INFO - Batch 6000, Loss: 2.1372, LR: 0.000085
2025-10-21 20:38:08,737 - INFO - Batch 6100, Loss: 2.7245, LR: 0.000085
2025-10-21 20:38:22,879 - INFO - Batch 6200, Loss: 2.3081, LR: 0.000084
2025-10-21 20:38:37,041 - INFO - Batch 6300, Loss: 2.6591, LR: 0.000084
2025-10-21 20:38:51,229 - INFO - Batch 6400, Loss: 2.5765, LR: 0.000084
2025-10-21 20:38:58,297 - INFO - Epoch 8/30: Train Loss: 2.5995, Val Loss: 2.4813, LR: 0.000084
2025-10-21 20:38:58,543 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 20:38:58,692 - INFO - Batch 0, Loss: 2.4375, LR: 0.000084
2025-10-21 20:39:12,946 - INFO - Batch 100, Loss: 2.6345, LR: 0.000084
2025-10-21 20:39:27,262 - INFO - Batch 200, Loss: 2.9228, LR: 0.000084
2025-10-21 20:39:41,512 - INFO - Batch 300, Loss: 2.5584, LR: 0.000084
2025-10-21 20:39:55,776 - INFO - Batch 400, Loss: 2.3646, LR: 0.000084
2025-10-21 20:40:10,033 - INFO - Batch 500, Loss: 2.3474, LR: 0.000084
2025-10-21 20:40:24,277 - INFO - Batch 600, Loss: 2.6749, LR: 0.000084
2025-10-21 20:40:38,572 - INFO - Batch 700, Loss: 2.5261, LR: 0.000084
2025-10-21 20:40:52,826 - INFO - Batch 800, Loss: 2.4178, LR: 0.000084
2025-10-21 20:41:07,165 - INFO - Batch 900, Loss: 2.3697, LR: 0.000084
2025-10-21 20:41:21,533 - INFO - Batch 1000, Loss: 2.2327, LR: 0.000084
2025-10-21 20:41:35,721 - INFO - Batch 1100, Loss: 2.5181, LR: 0.000084
2025-10-21 20:41:49,861 - INFO - Batch 1200, Loss: 2.6546, LR: 0.000084
2025-10-21 20:42:04,049 - INFO - Batch 1300, Loss: 2.6848, LR: 0.000084
2025-10-21 20:42:18,294 - INFO - Batch 1400, Loss: 2.2345, LR: 0.000083
2025-10-21 20:42:32,521 - INFO - Batch 1500, Loss: 2.6094, LR: 0.000083
2025-10-21 20:42:46,701 - INFO - Batch 1600, Loss: 2.7194, LR: 0.000083
2025-10-21 20:43:00,933 - INFO - Batch 1700, Loss: 2.5642, LR: 0.000083
2025-10-21 20:43:15,161 - INFO - Batch 1800, Loss: 2.3980, LR: 0.000083
2025-10-21 20:43:29,416 - INFO - Batch 1900, Loss: 2.6275, LR: 0.000083
2025-10-21 20:43:43,696 - INFO - Batch 2000, Loss: 2.6864, LR: 0.000083
2025-10-21 20:43:57,924 - INFO - Batch 2100, Loss: 2.4278, LR: 0.000083
2025-10-21 20:44:12,125 - INFO - Batch 2200, Loss: 2.4942, LR: 0.000083
2025-10-21 20:44:26,303 - INFO - Batch 2300, Loss: 2.3887, LR: 0.000083
2025-10-21 20:44:40,556 - INFO - Batch 2400, Loss: 2.6123, LR: 0.000083
2025-10-21 20:44:54,756 - INFO - Batch 2500, Loss: 2.5717, LR: 0.000083
2025-10-21 20:45:08,938 - INFO - Batch 2600, Loss: 2.4879, LR: 0.000083
2025-10-21 20:45:23,135 - INFO - Batch 2700, Loss: 2.5231, LR: 0.000083
2025-10-21 20:45:37,328 - INFO - Batch 2800, Loss: 2.3903, LR: 0.000083
2025-10-21 20:45:51,496 - INFO - Batch 2900, Loss: 2.5132, LR: 0.000083
2025-10-21 20:46:05,696 - INFO - Batch 3000, Loss: 2.4802, LR: 0.000083
2025-10-21 20:46:19,857 - INFO - Batch 3100, Loss: 2.3384, LR: 0.000082
2025-10-21 20:46:34,142 - INFO - Batch 3200, Loss: 2.6879, LR: 0.000082
2025-10-21 20:46:48,761 - INFO - Batch 3300, Loss: 2.5992, LR: 0.000082
2025-10-21 20:47:03,355 - INFO - Batch 3400, Loss: 2.3803, LR: 0.000082
2025-10-21 20:47:17,849 - INFO - Batch 3500, Loss: 2.6139, LR: 0.000082
2025-10-21 20:47:32,046 - INFO - Batch 3600, Loss: 2.4914, LR: 0.000082
2025-10-21 20:47:46,240 - INFO - Batch 3700, Loss: 2.6627, LR: 0.000082
2025-10-21 20:48:00,412 - INFO - Batch 3800, Loss: 2.6754, LR: 0.000082
2025-10-21 20:48:14,590 - INFO - Batch 3900, Loss: 2.3612, LR: 0.000082
2025-10-21 20:48:28,923 - INFO - Batch 4000, Loss: 2.6921, LR: 0.000082
2025-10-21 20:48:43,245 - INFO - Batch 4100, Loss: 2.5200, LR: 0.000082
2025-10-21 20:48:57,420 - INFO - Batch 4200, Loss: 2.4002, LR: 0.000082
2025-10-21 20:49:11,729 - INFO - Batch 4300, Loss: 2.2826, LR: 0.000082
2025-10-21 20:49:25,990 - INFO - Batch 4400, Loss: 2.5958, LR: 0.000082
2025-10-21 20:49:40,263 - INFO - Batch 4500, Loss: 2.5101, LR: 0.000082
2025-10-21 20:49:54,503 - INFO - Batch 4600, Loss: 2.8052, LR: 0.000081
2025-10-21 20:50:08,694 - INFO - Batch 4700, Loss: 2.8723, LR: 0.000081
2025-10-21 20:50:22,883 - INFO - Batch 4800, Loss: 2.5453, LR: 0.000081
2025-10-21 20:50:37,062 - INFO - Batch 4900, Loss: 2.2977, LR: 0.000081
2025-10-21 20:50:51,228 - INFO - Batch 5000, Loss: 2.7753, LR: 0.000081
2025-10-21 20:51:05,377 - INFO - Batch 5100, Loss: 2.2659, LR: 0.000081
2025-10-21 20:51:19,578 - INFO - Batch 5200, Loss: 2.1813, LR: 0.000081
2025-10-21 20:51:33,949 - INFO - Batch 5300, Loss: 2.3646, LR: 0.000081
2025-10-21 20:51:48,162 - INFO - Batch 5400, Loss: 2.5458, LR: 0.000081
2025-10-21 20:52:02,323 - INFO - Batch 5500, Loss: 2.6620, LR: 0.000081
2025-10-21 20:52:16,549 - INFO - Batch 5600, Loss: 2.6611, LR: 0.000081
2025-10-21 20:52:30,776 - INFO - Batch 5700, Loss: 2.6493, LR: 0.000081
2025-10-21 20:52:44,978 - INFO - Batch 5800, Loss: 2.6606, LR: 0.000081
2025-10-21 20:52:59,389 - INFO - Batch 5900, Loss: 2.6608, LR: 0.000081
2025-10-21 20:53:13,840 - INFO - Batch 6000, Loss: 2.3415, LR: 0.000081
2025-10-21 20:53:28,281 - INFO - Batch 6100, Loss: 2.2873, LR: 0.000081
2025-10-21 20:53:42,709 - INFO - Batch 6200, Loss: 2.5943, LR: 0.000080
2025-10-21 20:53:57,133 - INFO - Batch 6300, Loss: 2.4825, LR: 0.000080
2025-10-21 20:54:11,317 - INFO - Batch 6400, Loss: 2.4834, LR: 0.000080
2025-10-21 20:54:18,395 - INFO - Epoch 9/30: Train Loss: 2.5144, Val Loss: 2.3951, LR: 0.000080
2025-10-21 20:54:18,645 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 20:54:18,793 - INFO - Batch 0, Loss: 2.4908, LR: 0.000080
2025-10-21 20:54:32,888 - INFO - Batch 100, Loss: 2.4661, LR: 0.000080
2025-10-21 20:54:46,986 - INFO - Batch 200, Loss: 2.4781, LR: 0.000080
2025-10-21 20:55:01,099 - INFO - Batch 300, Loss: 2.6419, LR: 0.000080
2025-10-21 20:55:15,231 - INFO - Batch 400, Loss: 2.6087, LR: 0.000080
2025-10-21 20:55:29,349 - INFO - Batch 500, Loss: 2.5576, LR: 0.000080
2025-10-21 20:55:43,485 - INFO - Batch 600, Loss: 2.6991, LR: 0.000080
2025-10-21 20:55:57,627 - INFO - Batch 700, Loss: 2.1679, LR: 0.000080
2025-10-21 20:56:11,760 - INFO - Batch 800, Loss: 2.3862, LR: 0.000080
2025-10-21 20:56:26,207 - INFO - Batch 900, Loss: 2.6311, LR: 0.000080
2025-10-21 20:56:40,640 - INFO - Batch 1000, Loss: 2.4032, LR: 0.000080
2025-10-21 20:56:54,984 - INFO - Batch 1100, Loss: 2.2836, LR: 0.000080
2025-10-21 20:57:09,349 - INFO - Batch 1200, Loss: 2.5336, LR: 0.000080
2025-10-21 20:57:23,774 - INFO - Batch 1300, Loss: 2.6002, LR: 0.000079
2025-10-21 20:57:37,838 - INFO - Batch 1400, Loss: 2.5237, LR: 0.000079
2025-10-21 20:57:51,917 - INFO - Batch 1500, Loss: 2.4263, LR: 0.000079
2025-10-21 20:58:05,981 - INFO - Batch 1600, Loss: 2.3444, LR: 0.000079
2025-10-21 20:58:20,041 - INFO - Batch 1700, Loss: 2.5700, LR: 0.000079
2025-10-21 20:58:34,112 - INFO - Batch 1800, Loss: 2.4516, LR: 0.000079
2025-10-21 20:58:48,201 - INFO - Batch 1900, Loss: 2.5708, LR: 0.000079
2025-10-21 20:59:02,275 - INFO - Batch 2000, Loss: 2.4653, LR: 0.000079
2025-10-21 20:59:16,363 - INFO - Batch 2100, Loss: 2.4352, LR: 0.000079
2025-10-21 20:59:30,516 - INFO - Batch 2200, Loss: 2.2789, LR: 0.000079
2025-10-21 20:59:44,594 - INFO - Batch 2300, Loss: 2.3339, LR: 0.000079
2025-10-21 20:59:58,791 - INFO - Batch 2400, Loss: 2.4027, LR: 0.000079
2025-10-21 21:00:13,047 - INFO - Batch 2500, Loss: 2.5004, LR: 0.000079
2025-10-21 21:00:27,330 - INFO - Batch 2600, Loss: 2.4087, LR: 0.000079
2025-10-21 21:00:41,591 - INFO - Batch 2700, Loss: 2.4524, LR: 0.000079
2025-10-21 21:00:55,807 - INFO - Batch 2800, Loss: 2.4539, LR: 0.000078
2025-10-21 21:01:10,010 - INFO - Batch 2900, Loss: 2.3368, LR: 0.000078
2025-10-21 21:01:24,241 - INFO - Batch 3000, Loss: 2.4538, LR: 0.000078
2025-10-21 21:01:38,535 - INFO - Batch 3100, Loss: 2.6145, LR: 0.000078
2025-10-21 21:01:52,943 - INFO - Batch 3200, Loss: 2.3269, LR: 0.000078
2025-10-21 21:02:07,351 - INFO - Batch 3300, Loss: 2.5706, LR: 0.000078
2025-10-21 21:02:21,696 - INFO - Batch 3400, Loss: 2.6230, LR: 0.000078
2025-10-21 21:02:36,065 - INFO - Batch 3500, Loss: 2.4453, LR: 0.000078
2025-10-21 21:02:50,437 - INFO - Batch 3600, Loss: 2.4701, LR: 0.000078
2025-10-21 21:03:04,838 - INFO - Batch 3700, Loss: 2.2487, LR: 0.000078
2025-10-21 21:03:19,066 - INFO - Batch 3800, Loss: 2.3772, LR: 0.000078
2025-10-21 21:03:33,291 - INFO - Batch 3900, Loss: 2.4347, LR: 0.000078
2025-10-21 21:03:47,767 - INFO - Batch 4000, Loss: 2.4516, LR: 0.000078
2025-10-21 21:04:01,996 - INFO - Batch 4100, Loss: 2.3225, LR: 0.000078
2025-10-21 21:04:16,300 - INFO - Batch 4200, Loss: 2.6210, LR: 0.000077
2025-10-21 21:04:30,626 - INFO - Batch 4300, Loss: 2.3354, LR: 0.000077
2025-10-21 21:04:44,866 - INFO - Batch 4400, Loss: 2.2201, LR: 0.000077
2025-10-21 21:04:59,112 - INFO - Batch 4500, Loss: 2.4718, LR: 0.000077
2025-10-21 21:05:13,318 - INFO - Batch 4600, Loss: 2.2568, LR: 0.000077
2025-10-21 21:05:27,536 - INFO - Batch 4700, Loss: 2.4483, LR: 0.000077
2025-10-21 21:05:41,791 - INFO - Batch 4800, Loss: 2.5195, LR: 0.000077
2025-10-21 21:05:56,077 - INFO - Batch 4900, Loss: 2.4541, LR: 0.000077
2025-10-21 21:06:10,330 - INFO - Batch 5000, Loss: 2.3637, LR: 0.000077
2025-10-21 21:06:24,618 - INFO - Batch 5100, Loss: 2.6333, LR: 0.000077
2025-10-21 21:06:38,817 - INFO - Batch 5200, Loss: 2.4905, LR: 0.000077
2025-10-21 21:06:53,211 - INFO - Batch 5300, Loss: 2.3309, LR: 0.000077
2025-10-21 21:07:07,442 - INFO - Batch 5400, Loss: 2.5485, LR: 0.000077
2025-10-21 21:07:21,796 - INFO - Batch 5500, Loss: 2.4596, LR: 0.000077
2025-10-21 21:07:36,056 - INFO - Batch 5600, Loss: 2.5831, LR: 0.000077
2025-10-21 21:07:50,331 - INFO - Batch 5700, Loss: 2.2664, LR: 0.000076
2025-10-21 21:08:04,576 - INFO - Batch 5800, Loss: 2.5733, LR: 0.000076
2025-10-21 21:08:18,835 - INFO - Batch 5900, Loss: 2.4080, LR: 0.000076
2025-10-21 21:08:33,033 - INFO - Batch 6000, Loss: 2.2116, LR: 0.000076
2025-10-21 21:08:47,208 - INFO - Batch 6100, Loss: 2.3465, LR: 0.000076
2025-10-21 21:09:01,366 - INFO - Batch 6200, Loss: 2.4642, LR: 0.000076
2025-10-21 21:09:15,532 - INFO - Batch 6300, Loss: 2.3052, LR: 0.000076
2025-10-21 21:09:29,688 - INFO - Batch 6400, Loss: 2.5565, LR: 0.000076
2025-10-21 21:09:36,777 - INFO - Epoch 10/30: Train Loss: 2.4510, Val Loss: 2.3354, LR: 0.000076
2025-10-21 21:09:37,223 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 21:09:37,418 - INFO - 模型已保存到: ./checkpoints/checkpoint_epoch_10.pth
2025-10-21 21:09:37,567 - INFO - Batch 0, Loss: 2.2556, LR: 0.000076
2025-10-21 21:09:52,231 - INFO - Batch 100, Loss: 2.4092, LR: 0.000076
2025-10-21 21:10:06,726 - INFO - Batch 200, Loss: 2.5517, LR: 0.000076
2025-10-21 21:10:21,202 - INFO - Batch 300, Loss: 2.2173, LR: 0.000076
2025-10-21 21:10:35,481 - INFO - Batch 400, Loss: 2.4957, LR: 0.000076
2025-10-21 21:10:49,769 - INFO - Batch 500, Loss: 2.3661, LR: 0.000076
2025-10-21 21:11:04,211 - INFO - Batch 600, Loss: 1.9316, LR: 0.000076
2025-10-21 21:11:18,766 - INFO - Batch 700, Loss: 2.4919, LR: 0.000075
2025-10-21 21:11:33,197 - INFO - Batch 800, Loss: 2.5819, LR: 0.000075
2025-10-21 21:11:47,450 - INFO - Batch 900, Loss: 2.1138, LR: 0.000075
2025-10-21 21:12:01,684 - INFO - Batch 1000, Loss: 2.2265, LR: 0.000075
2025-10-21 21:12:15,910 - INFO - Batch 1100, Loss: 2.3297, LR: 0.000075
2025-10-21 21:12:30,204 - INFO - Batch 1200, Loss: 2.4231, LR: 0.000075
2025-10-21 21:12:44,359 - INFO - Batch 1300, Loss: 2.4550, LR: 0.000075
2025-10-21 21:12:58,593 - INFO - Batch 1400, Loss: 2.4439, LR: 0.000075
2025-10-21 21:13:12,819 - INFO - Batch 1500, Loss: 2.2074, LR: 0.000075
2025-10-21 21:13:27,066 - INFO - Batch 1600, Loss: 2.4755, LR: 0.000075
2025-10-21 21:13:41,427 - INFO - Batch 1700, Loss: 2.3421, LR: 0.000075
2025-10-21 21:13:55,830 - INFO - Batch 1800, Loss: 2.1251, LR: 0.000075
2025-10-21 21:14:10,267 - INFO - Batch 1900, Loss: 2.5608, LR: 0.000075
2025-10-21 21:14:24,668 - INFO - Batch 2000, Loss: 2.5490, LR: 0.000075
2025-10-21 21:14:39,061 - INFO - Batch 2100, Loss: 2.3983, LR: 0.000074
2025-10-21 21:14:53,656 - INFO - Batch 2200, Loss: 2.4178, LR: 0.000074
2025-10-21 21:15:08,275 - INFO - Batch 2300, Loss: 2.7410, LR: 0.000074
2025-10-21 21:15:22,846 - INFO - Batch 2400, Loss: 2.2612, LR: 0.000074
2025-10-21 21:15:37,262 - INFO - Batch 2500, Loss: 2.2585, LR: 0.000074
2025-10-21 21:15:51,759 - INFO - Batch 2600, Loss: 2.4587, LR: 0.000074
2025-10-21 21:16:06,189 - INFO - Batch 2700, Loss: 2.1986, LR: 0.000074
2025-10-21 21:16:20,630 - INFO - Batch 2800, Loss: 2.3765, LR: 0.000074
2025-10-21 21:16:34,900 - INFO - Batch 2900, Loss: 2.5842, LR: 0.000074
2025-10-21 21:16:49,128 - INFO - Batch 3000, Loss: 2.6746, LR: 0.000074
2025-10-21 21:17:03,325 - INFO - Batch 3100, Loss: 2.1293, LR: 0.000074
2025-10-21 21:17:17,639 - INFO - Batch 3200, Loss: 2.3454, LR: 0.000074
2025-10-21 21:17:31,857 - INFO - Batch 3300, Loss: 2.3879, LR: 0.000074
2025-10-21 21:17:46,138 - INFO - Batch 3400, Loss: 2.4438, LR: 0.000074
2025-10-21 21:18:00,409 - INFO - Batch 3500, Loss: 2.3120, LR: 0.000073
2025-10-21 21:18:14,678 - INFO - Batch 3600, Loss: 2.9815, LR: 0.000073
2025-10-21 21:18:28,938 - INFO - Batch 3700, Loss: 2.6318, LR: 0.000073
2025-10-21 21:18:43,454 - INFO - Batch 3800, Loss: 2.5986, LR: 0.000073
2025-10-21 21:18:57,963 - INFO - Batch 3900, Loss: 2.3487, LR: 0.000073
2025-10-21 21:19:12,182 - INFO - Batch 4000, Loss: 2.5523, LR: 0.000073
2025-10-21 21:19:26,386 - INFO - Batch 4100, Loss: 2.3654, LR: 0.000073
2025-10-21 21:19:40,636 - INFO - Batch 4200, Loss: 2.4212, LR: 0.000073
2025-10-21 21:19:54,830 - INFO - Batch 4300, Loss: 2.1096, LR: 0.000073
2025-10-21 21:20:09,075 - INFO - Batch 4400, Loss: 2.1387, LR: 0.000073
2025-10-21 21:20:23,244 - INFO - Batch 4500, Loss: 2.3007, LR: 0.000073
2025-10-21 21:20:37,463 - INFO - Batch 4600, Loss: 2.1447, LR: 0.000073
2025-10-21 21:20:51,953 - INFO - Batch 4700, Loss: 2.4968, LR: 0.000073
2025-10-21 21:21:06,218 - INFO - Batch 4800, Loss: 2.6531, LR: 0.000072
2025-10-21 21:21:20,392 - INFO - Batch 4900, Loss: 2.4066, LR: 0.000072
2025-10-21 21:21:34,525 - INFO - Batch 5000, Loss: 2.3566, LR: 0.000072
2025-10-21 21:21:48,678 - INFO - Batch 5100, Loss: 2.1057, LR: 0.000072
2025-10-21 21:22:02,859 - INFO - Batch 5200, Loss: 2.4297, LR: 0.000072
2025-10-21 21:22:17,047 - INFO - Batch 5300, Loss: 2.4269, LR: 0.000072
2025-10-21 21:22:31,316 - INFO - Batch 5400, Loss: 2.5839, LR: 0.000072
2025-10-21 21:22:45,494 - INFO - Batch 5500, Loss: 2.6724, LR: 0.000072
2025-10-21 21:22:59,687 - INFO - Batch 5600, Loss: 2.2894, LR: 0.000072
2025-10-21 21:23:13,866 - INFO - Batch 5700, Loss: 2.4758, LR: 0.000072
2025-10-21 21:23:28,060 - INFO - Batch 5800, Loss: 2.6462, LR: 0.000072
2025-10-21 21:23:42,297 - INFO - Batch 5900, Loss: 2.4524, LR: 0.000072
2025-10-21 21:23:56,537 - INFO - Batch 6000, Loss: 2.8630, LR: 0.000072
2025-10-21 21:24:10,754 - INFO - Batch 6100, Loss: 2.3374, LR: 0.000072
2025-10-21 21:24:25,032 - INFO - Batch 6200, Loss: 2.1572, LR: 0.000071
2025-10-21 21:24:39,548 - INFO - Batch 6300, Loss: 2.4121, LR: 0.000071
2025-10-21 21:24:53,875 - INFO - Batch 6400, Loss: 2.4708, LR: 0.000071
2025-10-21 21:25:00,990 - INFO - Epoch 11/30: Train Loss: 2.4002, Val Loss: 2.2908, LR: 0.000071
2025-10-21 21:25:01,245 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 21:25:01,395 - INFO - Batch 0, Loss: 1.9247, LR: 0.000071
2025-10-21 21:25:15,855 - INFO - Batch 100, Loss: 2.3696, LR: 0.000071
2025-10-21 21:25:30,254 - INFO - Batch 200, Loss: 2.4028, LR: 0.000071
2025-10-21 21:25:44,584 - INFO - Batch 300, Loss: 2.0725, LR: 0.000071
2025-10-21 21:25:58,884 - INFO - Batch 400, Loss: 2.2833, LR: 0.000071
2025-10-21 21:26:13,175 - INFO - Batch 500, Loss: 2.4089, LR: 0.000071
2025-10-21 21:26:27,481 - INFO - Batch 600, Loss: 2.2000, LR: 0.000071
2025-10-21 21:26:41,794 - INFO - Batch 700, Loss: 2.6615, LR: 0.000071
2025-10-21 21:26:56,335 - INFO - Batch 800, Loss: 2.3080, LR: 0.000071
2025-10-21 21:27:10,714 - INFO - Batch 900, Loss: 2.2445, LR: 0.000071
2025-10-21 21:27:25,069 - INFO - Batch 1000, Loss: 2.3960, LR: 0.000071
2025-10-21 21:27:39,369 - INFO - Batch 1100, Loss: 2.4547, LR: 0.000070
2025-10-21 21:27:53,657 - INFO - Batch 1200, Loss: 2.1163, LR: 0.000070
2025-10-21 21:28:08,185 - INFO - Batch 1300, Loss: 2.6476, LR: 0.000070
2025-10-21 21:28:22,709 - INFO - Batch 1400, Loss: 2.4701, LR: 0.000070
2025-10-21 21:28:37,059 - INFO - Batch 1500, Loss: 2.3079, LR: 0.000070
2025-10-21 21:28:51,431 - INFO - Batch 1600, Loss: 2.2996, LR: 0.000070
2025-10-21 21:29:05,721 - INFO - Batch 1700, Loss: 2.7352, LR: 0.000070
2025-10-21 21:29:20,009 - INFO - Batch 1800, Loss: 2.1433, LR: 0.000070
2025-10-21 21:29:34,311 - INFO - Batch 1900, Loss: 2.0554, LR: 0.000070
2025-10-21 21:29:48,608 - INFO - Batch 2000, Loss: 2.2515, LR: 0.000070
2025-10-21 21:30:02,941 - INFO - Batch 2100, Loss: 2.4667, LR: 0.000070
2025-10-21 21:30:17,262 - INFO - Batch 2200, Loss: 2.4721, LR: 0.000070
2025-10-21 21:30:31,580 - INFO - Batch 2300, Loss: 2.2531, LR: 0.000070
2025-10-21 21:30:45,915 - INFO - Batch 2400, Loss: 2.5931, LR: 0.000069
2025-10-21 21:31:00,280 - INFO - Batch 2500, Loss: 2.4238, LR: 0.000069
2025-10-21 21:31:14,794 - INFO - Batch 2600, Loss: 2.6372, LR: 0.000069
2025-10-21 21:31:29,302 - INFO - Batch 2700, Loss: 2.3928, LR: 0.000069
2025-10-21 21:31:43,620 - INFO - Batch 2800, Loss: 2.3658, LR: 0.000069
2025-10-21 21:31:58,110 - INFO - Batch 2900, Loss: 2.3991, LR: 0.000069
2025-10-21 21:32:12,537 - INFO - Batch 3000, Loss: 2.5206, LR: 0.000069
2025-10-21 21:32:27,038 - INFO - Batch 3100, Loss: 2.5700, LR: 0.000069
2025-10-21 21:32:41,459 - INFO - Batch 3200, Loss: 2.7848, LR: 0.000069
2025-10-21 21:32:55,662 - INFO - Batch 3300, Loss: 2.4451, LR: 0.000069
2025-10-21 21:33:09,827 - INFO - Batch 3400, Loss: 2.4037, LR: 0.000069
2025-10-21 21:33:23,917 - INFO - Batch 3500, Loss: 2.4842, LR: 0.000069
2025-10-21 21:33:38,046 - INFO - Batch 3600, Loss: 2.2862, LR: 0.000069
2025-10-21 21:33:52,178 - INFO - Batch 3700, Loss: 2.2122, LR: 0.000068
2025-10-21 21:34:06,354 - INFO - Batch 3800, Loss: 2.5041, LR: 0.000068
2025-10-21 21:34:20,502 - INFO - Batch 3900, Loss: 2.3207, LR: 0.000068
2025-10-21 21:34:34,604 - INFO - Batch 4000, Loss: 2.3518, LR: 0.000068
2025-10-21 21:34:48,713 - INFO - Batch 4100, Loss: 2.3920, LR: 0.000068
2025-10-21 21:35:02,815 - INFO - Batch 4200, Loss: 2.1804, LR: 0.000068
2025-10-21 21:35:16,967 - INFO - Batch 4300, Loss: 2.2315, LR: 0.000068
2025-10-21 21:35:31,090 - INFO - Batch 4400, Loss: 2.4611, LR: 0.000068
2025-10-21 21:35:45,260 - INFO - Batch 4500, Loss: 2.4041, LR: 0.000068
2025-10-21 21:35:59,369 - INFO - Batch 4600, Loss: 2.4672, LR: 0.000068
2025-10-21 21:36:13,521 - INFO - Batch 4700, Loss: 2.4492, LR: 0.000068
2025-10-21 21:36:27,710 - INFO - Batch 4800, Loss: 2.6507, LR: 0.000068
2025-10-21 21:36:41,840 - INFO - Batch 4900, Loss: 2.4035, LR: 0.000068
2025-10-21 21:36:55,959 - INFO - Batch 5000, Loss: 2.2637, LR: 0.000067
2025-10-21 21:37:10,074 - INFO - Batch 5100, Loss: 2.5193, LR: 0.000067
2025-10-21 21:37:24,348 - INFO - Batch 5200, Loss: 2.1058, LR: 0.000067
2025-10-21 21:37:38,713 - INFO - Batch 5300, Loss: 2.1877, LR: 0.000067
2025-10-21 21:37:53,092 - INFO - Batch 5400, Loss: 2.4374, LR: 0.000067
2025-10-21 21:38:07,466 - INFO - Batch 5500, Loss: 2.4927, LR: 0.000067
2025-10-21 21:38:21,833 - INFO - Batch 5600, Loss: 2.1974, LR: 0.000067
2025-10-21 21:38:36,098 - INFO - Batch 5700, Loss: 2.1397, LR: 0.000067
2025-10-21 21:38:50,200 - INFO - Batch 5800, Loss: 2.4550, LR: 0.000067
2025-10-21 21:39:04,261 - INFO - Batch 5900, Loss: 2.1682, LR: 0.000067
2025-10-21 21:39:18,326 - INFO - Batch 6000, Loss: 2.2106, LR: 0.000067
2025-10-21 21:39:32,519 - INFO - Batch 6100, Loss: 2.5777, LR: 0.000067
2025-10-21 21:39:46,893 - INFO - Batch 6200, Loss: 2.3462, LR: 0.000067
2025-10-21 21:40:01,278 - INFO - Batch 6300, Loss: 2.2831, LR: 0.000066
2025-10-21 21:40:15,575 - INFO - Batch 6400, Loss: 2.4988, LR: 0.000066
2025-10-21 21:40:22,637 - INFO - Epoch 12/30: Train Loss: 2.3604, Val Loss: 2.2573, LR: 0.000066
2025-10-21 21:40:22,889 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 21:40:23,037 - INFO - Batch 0, Loss: 2.3694, LR: 0.000066
2025-10-21 21:40:37,274 - INFO - Batch 100, Loss: 1.9303, LR: 0.000066
2025-10-21 21:40:51,457 - INFO - Batch 200, Loss: 2.3118, LR: 0.000066
2025-10-21 21:41:05,659 - INFO - Batch 300, Loss: 2.2637, LR: 0.000066
2025-10-21 21:41:19,862 - INFO - Batch 400, Loss: 2.2427, LR: 0.000066
2025-10-21 21:41:34,110 - INFO - Batch 500, Loss: 2.3122, LR: 0.000066
2025-10-21 21:41:48,356 - INFO - Batch 600, Loss: 2.4349, LR: 0.000066
2025-10-21 21:42:02,622 - INFO - Batch 700, Loss: 2.2759, LR: 0.000066
2025-10-21 21:42:16,899 - INFO - Batch 800, Loss: 1.9853, LR: 0.000066
2025-10-21 21:42:31,354 - INFO - Batch 900, Loss: 2.2855, LR: 0.000066
2025-10-21 21:42:45,854 - INFO - Batch 1000, Loss: 2.1904, LR: 0.000066
2025-10-21 21:43:00,337 - INFO - Batch 1100, Loss: 2.2007, LR: 0.000066
2025-10-21 21:43:14,905 - INFO - Batch 1200, Loss: 2.4688, LR: 0.000065
2025-10-21 21:43:29,451 - INFO - Batch 1300, Loss: 2.5636, LR: 0.000065
2025-10-21 21:43:44,011 - INFO - Batch 1400, Loss: 2.4580, LR: 0.000065
2025-10-21 21:43:58,328 - INFO - Batch 1500, Loss: 2.0942, LR: 0.000065
2025-10-21 21:44:12,873 - INFO - Batch 1600, Loss: 2.2745, LR: 0.000065
2025-10-21 21:44:27,431 - INFO - Batch 1700, Loss: 2.5568, LR: 0.000065
2025-10-21 21:44:41,866 - INFO - Batch 1800, Loss: 2.0958, LR: 0.000065
2025-10-21 21:44:56,162 - INFO - Batch 1900, Loss: 2.3106, LR: 0.000065
2025-10-21 21:45:10,687 - INFO - Batch 2000, Loss: 2.0598, LR: 0.000065
2025-10-21 21:45:25,144 - INFO - Batch 2100, Loss: 2.4705, LR: 0.000065
2025-10-21 21:45:39,432 - INFO - Batch 2200, Loss: 2.5310, LR: 0.000065
2025-10-21 21:45:53,745 - INFO - Batch 2300, Loss: 2.2997, LR: 0.000065
2025-10-21 21:46:08,034 - INFO - Batch 2400, Loss: 2.2300, LR: 0.000065
2025-10-21 21:46:22,285 - INFO - Batch 2500, Loss: 2.2683, LR: 0.000064
2025-10-21 21:46:36,561 - INFO - Batch 2600, Loss: 2.1982, LR: 0.000064
2025-10-21 21:46:50,811 - INFO - Batch 2700, Loss: 2.1526, LR: 0.000064
2025-10-21 21:47:05,121 - INFO - Batch 2800, Loss: 2.1597, LR: 0.000064
2025-10-21 21:47:19,450 - INFO - Batch 2900, Loss: 2.1934, LR: 0.000064
2025-10-21 21:47:33,798 - INFO - Batch 3000, Loss: 2.5864, LR: 0.000064
2025-10-21 21:47:48,213 - INFO - Batch 3100, Loss: 2.5670, LR: 0.000064
2025-10-21 21:48:02,424 - INFO - Batch 3200, Loss: 2.0345, LR: 0.000064
2025-10-21 21:48:16,722 - INFO - Batch 3300, Loss: 2.0428, LR: 0.000064
2025-10-21 21:48:31,073 - INFO - Batch 3400, Loss: 2.2285, LR: 0.000064
2025-10-21 21:48:45,353 - INFO - Batch 3500, Loss: 2.2738, LR: 0.000064
2025-10-21 21:48:59,562 - INFO - Batch 3600, Loss: 2.3569, LR: 0.000064
2025-10-21 21:49:13,827 - INFO - Batch 3700, Loss: 2.1387, LR: 0.000063
2025-10-21 21:49:28,049 - INFO - Batch 3800, Loss: 2.2892, LR: 0.000063
2025-10-21 21:49:42,296 - INFO - Batch 3900, Loss: 2.1727, LR: 0.000063
2025-10-21 21:49:56,535 - INFO - Batch 4000, Loss: 2.2430, LR: 0.000063
2025-10-21 21:50:10,881 - INFO - Batch 4100, Loss: 2.3117, LR: 0.000063
2025-10-21 21:50:25,124 - INFO - Batch 4200, Loss: 2.2727, LR: 0.000063
2025-10-21 21:50:39,476 - INFO - Batch 4300, Loss: 2.4978, LR: 0.000063
2025-10-21 21:50:53,936 - INFO - Batch 4400, Loss: 2.4003, LR: 0.000063
2025-10-21 21:51:08,407 - INFO - Batch 4500, Loss: 2.2194, LR: 0.000063
2025-10-21 21:51:22,803 - INFO - Batch 4600, Loss: 2.1278, LR: 0.000063
2025-10-21 21:51:37,159 - INFO - Batch 4700, Loss: 2.2225, LR: 0.000063
2025-10-21 21:51:51,635 - INFO - Batch 4800, Loss: 2.3069, LR: 0.000063
2025-10-21 21:52:06,070 - INFO - Batch 4900, Loss: 2.2446, LR: 0.000063
2025-10-21 21:52:20,536 - INFO - Batch 5000, Loss: 2.1760, LR: 0.000062
2025-10-21 21:52:35,126 - INFO - Batch 5100, Loss: 2.0797, LR: 0.000062
2025-10-21 21:52:49,699 - INFO - Batch 5200, Loss: 2.4500, LR: 0.000062
2025-10-21 21:53:04,149 - INFO - Batch 5300, Loss: 2.1949, LR: 0.000062
2025-10-21 21:53:18,411 - INFO - Batch 5400, Loss: 2.1381, LR: 0.000062
2025-10-21 21:53:32,717 - INFO - Batch 5500, Loss: 2.1485, LR: 0.000062
2025-10-21 21:53:47,163 - INFO - Batch 5600, Loss: 2.1753, LR: 0.000062
2025-10-21 21:54:01,740 - INFO - Batch 5700, Loss: 2.4276, LR: 0.000062
2025-10-21 21:54:16,308 - INFO - Batch 5800, Loss: 2.2859, LR: 0.000062
2025-10-21 21:54:30,835 - INFO - Batch 5900, Loss: 1.9688, LR: 0.000062
2025-10-21 21:54:45,454 - INFO - Batch 6000, Loss: 2.3774, LR: 0.000062
2025-10-21 21:55:00,021 - INFO - Batch 6100, Loss: 2.6748, LR: 0.000062
2025-10-21 21:55:14,535 - INFO - Batch 6200, Loss: 2.2378, LR: 0.000061
2025-10-21 21:55:29,091 - INFO - Batch 6300, Loss: 2.4292, LR: 0.000061
2025-10-21 21:55:43,624 - INFO - Batch 6400, Loss: 2.2599, LR: 0.000061
2025-10-21 21:55:50,810 - INFO - Epoch 13/30: Train Loss: 2.3261, Val Loss: 2.2298, LR: 0.000061
2025-10-21 21:55:51,076 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 21:55:51,226 - INFO - Batch 0, Loss: 2.3464, LR: 0.000061
2025-10-21 21:56:05,694 - INFO - Batch 100, Loss: 2.1836, LR: 0.000061
2025-10-21 21:56:20,345 - INFO - Batch 200, Loss: 2.1015, LR: 0.000061
2025-10-21 21:56:34,934 - INFO - Batch 300, Loss: 2.4576, LR: 0.000061
2025-10-21 21:56:49,434 - INFO - Batch 400, Loss: 2.2989, LR: 0.000061
2025-10-21 21:57:03,892 - INFO - Batch 500, Loss: 2.3023, LR: 0.000061
2025-10-21 21:57:18,462 - INFO - Batch 600, Loss: 2.3379, LR: 0.000061
2025-10-21 21:57:32,651 - INFO - Batch 700, Loss: 2.1289, LR: 0.000061
2025-10-21 21:57:46,841 - INFO - Batch 800, Loss: 2.1322, LR: 0.000061
2025-10-21 21:58:01,001 - INFO - Batch 900, Loss: 2.2825, LR: 0.000061
2025-10-21 21:58:15,181 - INFO - Batch 1000, Loss: 2.3758, LR: 0.000061
2025-10-21 21:58:29,334 - INFO - Batch 1100, Loss: 2.1893, LR: 0.000060
2025-10-21 21:58:43,505 - INFO - Batch 1200, Loss: 1.9758, LR: 0.000060
2025-10-21 21:58:57,639 - INFO - Batch 1300, Loss: 2.0010, LR: 0.000060
2025-10-21 21:59:11,913 - INFO - Batch 1400, Loss: 2.4692, LR: 0.000060
2025-10-21 21:59:26,162 - INFO - Batch 1500, Loss: 2.4429, LR: 0.000060
2025-10-21 21:59:40,481 - INFO - Batch 1600, Loss: 2.5182, LR: 0.000060
2025-10-21 21:59:55,099 - INFO - Batch 1700, Loss: 2.5416, LR: 0.000060
2025-10-21 22:00:09,784 - INFO - Batch 1800, Loss: 2.1852, LR: 0.000060
2025-10-21 22:00:24,229 - INFO - Batch 1900, Loss: 2.1808, LR: 0.000060
2025-10-21 22:00:38,530 - INFO - Batch 2000, Loss: 2.2868, LR: 0.000060
2025-10-21 22:00:52,905 - INFO - Batch 2100, Loss: 2.1952, LR: 0.000060
2025-10-21 22:01:07,286 - INFO - Batch 2200, Loss: 2.2419, LR: 0.000060
2025-10-21 22:01:21,737 - INFO - Batch 2300, Loss: 2.2690, LR: 0.000059
2025-10-21 22:01:36,122 - INFO - Batch 2400, Loss: 2.3066, LR: 0.000059
2025-10-21 22:01:50,544 - INFO - Batch 2500, Loss: 2.1569, LR: 0.000059
2025-10-21 22:02:04,945 - INFO - Batch 2600, Loss: 2.4620, LR: 0.000059
2025-10-21 22:02:19,404 - INFO - Batch 2700, Loss: 2.2584, LR: 0.000059
2025-10-21 22:02:33,839 - INFO - Batch 2800, Loss: 2.1720, LR: 0.000059
2025-10-21 22:02:48,250 - INFO - Batch 2900, Loss: 2.2111, LR: 0.000059
2025-10-21 22:03:02,710 - INFO - Batch 3000, Loss: 2.3432, LR: 0.000059
2025-10-21 22:03:17,151 - INFO - Batch 3100, Loss: 2.2107, LR: 0.000059
2025-10-21 22:03:31,593 - INFO - Batch 3200, Loss: 2.0485, LR: 0.000059
2025-10-21 22:03:46,044 - INFO - Batch 3300, Loss: 2.2005, LR: 0.000059
2025-10-21 22:04:00,476 - INFO - Batch 3400, Loss: 2.3177, LR: 0.000059
2025-10-21 22:04:14,845 - INFO - Batch 3500, Loss: 2.2375, LR: 0.000058
2025-10-21 22:04:29,273 - INFO - Batch 3600, Loss: 2.3649, LR: 0.000058
2025-10-21 22:04:43,510 - INFO - Batch 3700, Loss: 2.3275, LR: 0.000058
2025-10-21 22:04:57,689 - INFO - Batch 3800, Loss: 2.2695, LR: 0.000058
2025-10-21 22:05:12,027 - INFO - Batch 3900, Loss: 2.1533, LR: 0.000058
2025-10-21 22:05:26,408 - INFO - Batch 4000, Loss: 2.4030, LR: 0.000058
2025-10-21 22:05:40,814 - INFO - Batch 4100, Loss: 1.9800, LR: 0.000058
2025-10-21 22:05:55,296 - INFO - Batch 4200, Loss: 2.3606, LR: 0.000058
2025-10-21 22:06:09,657 - INFO - Batch 4300, Loss: 2.3112, LR: 0.000058
2025-10-21 22:06:23,851 - INFO - Batch 4400, Loss: 2.3538, LR: 0.000058
2025-10-21 22:06:38,226 - INFO - Batch 4500, Loss: 2.2534, LR: 0.000058
2025-10-21 22:06:52,638 - INFO - Batch 4600, Loss: 2.3278, LR: 0.000058
2025-10-21 22:07:07,083 - INFO - Batch 4700, Loss: 2.1158, LR: 0.000058
2025-10-21 22:07:21,632 - INFO - Batch 4800, Loss: 2.3433, LR: 0.000057
2025-10-21 22:07:35,886 - INFO - Batch 4900, Loss: 2.1574, LR: 0.000057
2025-10-21 22:07:50,190 - INFO - Batch 5000, Loss: 2.4642, LR: 0.000057
2025-10-21 22:08:04,685 - INFO - Batch 5100, Loss: 2.4711, LR: 0.000057
2025-10-21 22:08:18,976 - INFO - Batch 5200, Loss: 2.4649, LR: 0.000057
2025-10-21 22:08:33,234 - INFO - Batch 5300, Loss: 2.3193, LR: 0.000057
2025-10-21 22:08:47,491 - INFO - Batch 5400, Loss: 2.3359, LR: 0.000057
2025-10-21 22:09:01,654 - INFO - Batch 5500, Loss: 2.4237, LR: 0.000057
2025-10-21 22:09:16,025 - INFO - Batch 5600, Loss: 2.3220, LR: 0.000057
2025-10-21 22:09:30,046 - INFO - Batch 5700, Loss: 2.5150, LR: 0.000057
2025-10-21 22:09:44,266 - INFO - Batch 5800, Loss: 2.3018, LR: 0.000057
2025-10-21 22:09:58,552 - INFO - Batch 5900, Loss: 2.2804, LR: 0.000057
2025-10-21 22:10:12,671 - INFO - Batch 6000, Loss: 2.1253, LR: 0.000056
2025-10-21 22:10:26,898 - INFO - Batch 6100, Loss: 2.1801, LR: 0.000056
2025-10-21 22:10:41,271 - INFO - Batch 6200, Loss: 2.2238, LR: 0.000056
2025-10-21 22:10:55,395 - INFO - Batch 6300, Loss: 2.2279, LR: 0.000056
2025-10-21 22:11:09,509 - INFO - Batch 6400, Loss: 2.2050, LR: 0.000056
2025-10-21 22:11:16,570 - INFO - Epoch 14/30: Train Loss: 2.2966, Val Loss: 2.2080, LR: 0.000056
2025-10-21 22:11:16,841 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 22:11:16,994 - INFO - Batch 0, Loss: 2.3535, LR: 0.000056
2025-10-21 22:11:31,306 - INFO - Batch 100, Loss: 2.0763, LR: 0.000056
2025-10-21 22:11:45,812 - INFO - Batch 200, Loss: 2.2855, LR: 0.000056
2025-10-21 22:12:00,173 - INFO - Batch 300, Loss: 2.1850, LR: 0.000056
2025-10-21 22:12:14,513 - INFO - Batch 400, Loss: 2.1228, LR: 0.000056
2025-10-21 22:12:28,818 - INFO - Batch 500, Loss: 2.4277, LR: 0.000056
2025-10-21 22:12:43,268 - INFO - Batch 600, Loss: 2.3910, LR: 0.000056
2025-10-21 22:12:57,680 - INFO - Batch 700, Loss: 2.5677, LR: 0.000056
2025-10-21 22:13:12,276 - INFO - Batch 800, Loss: 2.3056, LR: 0.000055
2025-10-21 22:13:26,616 - INFO - Batch 900, Loss: 2.1599, LR: 0.000055
2025-10-21 22:13:40,938 - INFO - Batch 1000, Loss: 2.4798, LR: 0.000055
2025-10-21 22:13:55,218 - INFO - Batch 1100, Loss: 2.3763, LR: 0.000055
2025-10-21 22:14:09,513 - INFO - Batch 1200, Loss: 2.2493, LR: 0.000055
2025-10-21 22:14:23,880 - INFO - Batch 1300, Loss: 2.1012, LR: 0.000055
2025-10-21 22:14:38,288 - INFO - Batch 1400, Loss: 2.4172, LR: 0.000055
2025-10-21 22:14:52,675 - INFO - Batch 1500, Loss: 2.2777, LR: 0.000055
2025-10-21 22:15:07,139 - INFO - Batch 1600, Loss: 2.3846, LR: 0.000055
2025-10-21 22:15:21,675 - INFO - Batch 1700, Loss: 2.2076, LR: 0.000055
2025-10-21 22:15:36,233 - INFO - Batch 1800, Loss: 1.8952, LR: 0.000055
2025-10-21 22:15:50,855 - INFO - Batch 1900, Loss: 2.5091, LR: 0.000055
2025-10-21 22:16:05,467 - INFO - Batch 2000, Loss: 2.2997, LR: 0.000054
2025-10-21 22:16:19,955 - INFO - Batch 2100, Loss: 2.1544, LR: 0.000054
2025-10-21 22:16:34,563 - INFO - Batch 2200, Loss: 2.3777, LR: 0.000054
2025-10-21 22:16:49,106 - INFO - Batch 2300, Loss: 2.4686, LR: 0.000054
2025-10-21 22:17:03,710 - INFO - Batch 2400, Loss: 2.2617, LR: 0.000054
2025-10-21 22:17:18,231 - INFO - Batch 2500, Loss: 2.3388, LR: 0.000054
2025-10-21 22:17:32,818 - INFO - Batch 2600, Loss: 2.2374, LR: 0.000054
2025-10-21 22:17:47,153 - INFO - Batch 2700, Loss: 2.2695, LR: 0.000054
2025-10-21 22:18:01,622 - INFO - Batch 2800, Loss: 1.9098, LR: 0.000054
2025-10-21 22:18:16,147 - INFO - Batch 2900, Loss: 2.1243, LR: 0.000054
2025-10-21 22:18:30,700 - INFO - Batch 3000, Loss: 2.2385, LR: 0.000054
2025-10-21 22:18:45,162 - INFO - Batch 3100, Loss: 2.3069, LR: 0.000054
2025-10-21 22:18:59,587 - INFO - Batch 3200, Loss: 2.2557, LR: 0.000053
2025-10-21 22:19:13,961 - INFO - Batch 3300, Loss: 2.3824, LR: 0.000053
2025-10-21 22:19:28,403 - INFO - Batch 3400, Loss: 2.3657, LR: 0.000053
2025-10-21 22:19:42,828 - INFO - Batch 3500, Loss: 2.2000, LR: 0.000053
2025-10-21 22:19:57,245 - INFO - Batch 3600, Loss: 2.1552, LR: 0.000053
2025-10-21 22:20:11,490 - INFO - Batch 3700, Loss: 2.2546, LR: 0.000053
2025-10-21 22:20:25,919 - INFO - Batch 3800, Loss: 2.3220, LR: 0.000053
2025-10-21 22:20:40,450 - INFO - Batch 3900, Loss: 2.4052, LR: 0.000053
2025-10-21 22:20:54,770 - INFO - Batch 4000, Loss: 2.3459, LR: 0.000053
2025-10-21 22:21:09,208 - INFO - Batch 4100, Loss: 2.1362, LR: 0.000053
2025-10-21 22:21:23,618 - INFO - Batch 4200, Loss: 2.3658, LR: 0.000053
2025-10-21 22:21:38,003 - INFO - Batch 4300, Loss: 2.1666, LR: 0.000053
2025-10-21 22:21:52,413 - INFO - Batch 4400, Loss: 2.2917, LR: 0.000052
2025-10-21 22:22:06,943 - INFO - Batch 4500, Loss: 2.3203, LR: 0.000052
2025-10-21 22:22:21,390 - INFO - Batch 4600, Loss: 2.2125, LR: 0.000052
2025-10-21 22:22:35,852 - INFO - Batch 4700, Loss: 2.2491, LR: 0.000052
2025-10-21 22:22:50,349 - INFO - Batch 4800, Loss: 2.5483, LR: 0.000052
2025-10-21 22:23:04,730 - INFO - Batch 4900, Loss: 2.4578, LR: 0.000052
2025-10-21 22:23:19,182 - INFO - Batch 5000, Loss: 2.4075, LR: 0.000052
2025-10-21 22:23:33,643 - INFO - Batch 5100, Loss: 2.0881, LR: 0.000052
2025-10-21 22:23:47,965 - INFO - Batch 5200, Loss: 2.2177, LR: 0.000052
2025-10-21 22:24:02,235 - INFO - Batch 5300, Loss: 2.3948, LR: 0.000052
2025-10-21 22:24:16,530 - INFO - Batch 5400, Loss: 2.1234, LR: 0.000052
2025-10-21 22:24:30,954 - INFO - Batch 5500, Loss: 2.2609, LR: 0.000052
2025-10-21 22:24:45,281 - INFO - Batch 5600, Loss: 2.2591, LR: 0.000052
2025-10-21 22:24:59,616 - INFO - Batch 5700, Loss: 2.1473, LR: 0.000051
2025-10-21 22:25:13,948 - INFO - Batch 5800, Loss: 2.3681, LR: 0.000051
2025-10-21 22:25:28,269 - INFO - Batch 5900, Loss: 2.2250, LR: 0.000051
2025-10-21 22:25:42,630 - INFO - Batch 6000, Loss: 2.3346, LR: 0.000051
2025-10-21 22:25:57,049 - INFO - Batch 6100, Loss: 2.4332, LR: 0.000051
2025-10-21 22:26:11,345 - INFO - Batch 6200, Loss: 2.3355, LR: 0.000051
2025-10-21 22:26:25,691 - INFO - Batch 6300, Loss: 2.2708, LR: 0.000051
2025-10-21 22:26:39,998 - INFO - Batch 6400, Loss: 2.0068, LR: 0.000051
2025-10-21 22:26:47,195 - INFO - Epoch 15/30: Train Loss: 2.2697, Val Loss: 2.1815, LR: 0.000051
2025-10-21 22:26:47,471 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 22:26:47,622 - INFO - Batch 0, Loss: 1.9454, LR: 0.000051
2025-10-21 22:27:01,993 - INFO - Batch 100, Loss: 2.0609, LR: 0.000051
2025-10-21 22:27:16,361 - INFO - Batch 200, Loss: 2.2746, LR: 0.000051
2025-10-21 22:27:30,728 - INFO - Batch 300, Loss: 2.2070, LR: 0.000051
2025-10-21 22:27:45,128 - INFO - Batch 400, Loss: 2.4437, LR: 0.000050
2025-10-21 22:27:59,529 - INFO - Batch 500, Loss: 2.1328, LR: 0.000050
2025-10-21 22:28:13,924 - INFO - Batch 600, Loss: 2.4613, LR: 0.000050
2025-10-21 22:28:28,400 - INFO - Batch 700, Loss: 2.2395, LR: 0.000050
2025-10-21 22:28:42,844 - INFO - Batch 800, Loss: 1.9931, LR: 0.000050
2025-10-21 22:28:57,232 - INFO - Batch 900, Loss: 2.1802, LR: 0.000050
2025-10-21 22:29:12,395 - INFO - Batch 1000, Loss: 2.4613, LR: 0.000050
2025-10-21 22:29:28,397 - INFO - Batch 1100, Loss: 2.1793, LR: 0.000050
2025-10-21 22:29:44,384 - INFO - Batch 1200, Loss: 2.1042, LR: 0.000050
2025-10-21 22:30:00,428 - INFO - Batch 1300, Loss: 2.2284, LR: 0.000050
2025-10-21 22:30:16,490 - INFO - Batch 1400, Loss: 2.0194, LR: 0.000050
2025-10-21 22:30:32,521 - INFO - Batch 1500, Loss: 2.1388, LR: 0.000050
2025-10-21 22:30:48,552 - INFO - Batch 1600, Loss: 2.2428, LR: 0.000050
2025-10-21 22:31:04,606 - INFO - Batch 1700, Loss: 2.2117, LR: 0.000049
2025-10-21 22:31:20,535 - INFO - Batch 1800, Loss: 2.2725, LR: 0.000049
2025-10-21 22:31:36,421 - INFO - Batch 1900, Loss: 2.2801, LR: 0.000049
2025-10-21 22:31:52,305 - INFO - Batch 2000, Loss: 1.9338, LR: 0.000049
2025-10-21 22:32:08,113 - INFO - Batch 2100, Loss: 2.1968, LR: 0.000049
2025-10-21 22:32:24,120 - INFO - Batch 2200, Loss: 2.2370, LR: 0.000049
2025-10-21 22:32:40,294 - INFO - Batch 2300, Loss: 2.1463, LR: 0.000049
2025-10-21 22:32:56,345 - INFO - Batch 2400, Loss: 2.3125, LR: 0.000049
2025-10-21 22:33:12,367 - INFO - Batch 2500, Loss: 2.1415, LR: 0.000049
2025-10-21 22:33:28,456 - INFO - Batch 2600, Loss: 2.3394, LR: 0.000049
2025-10-21 22:33:44,453 - INFO - Batch 2700, Loss: 2.2712, LR: 0.000049
2025-10-21 22:34:00,451 - INFO - Batch 2800, Loss: 2.3902, LR: 0.000049
2025-10-21 22:34:16,567 - INFO - Batch 2900, Loss: 2.4731, LR: 0.000048
2025-10-21 22:34:32,412 - INFO - Batch 3000, Loss: 2.1871, LR: 0.000048
2025-10-21 22:34:48,218 - INFO - Batch 3100, Loss: 2.1152, LR: 0.000048
2025-10-21 22:35:04,234 - INFO - Batch 3200, Loss: 2.3584, LR: 0.000048
2025-10-21 22:35:20,302 - INFO - Batch 3300, Loss: 2.1506, LR: 0.000048
2025-10-21 22:35:36,355 - INFO - Batch 3400, Loss: 2.3410, LR: 0.000048
2025-10-21 22:35:52,476 - INFO - Batch 3500, Loss: 2.3064, LR: 0.000048
2025-10-21 22:36:08,558 - INFO - Batch 3600, Loss: 2.4084, LR: 0.000048
2025-10-21 22:36:24,561 - INFO - Batch 3700, Loss: 2.3597, LR: 0.000048
2025-10-21 22:36:40,582 - INFO - Batch 3800, Loss: 2.2438, LR: 0.000048
2025-10-21 22:36:56,591 - INFO - Batch 3900, Loss: 2.0663, LR: 0.000048
2025-10-21 22:37:12,607 - INFO - Batch 4000, Loss: 2.2242, LR: 0.000048
2025-10-21 22:37:28,428 - INFO - Batch 4100, Loss: 2.4855, LR: 0.000047
2025-10-21 22:37:44,307 - INFO - Batch 4200, Loss: 2.3918, LR: 0.000047
2025-10-21 22:38:00,119 - INFO - Batch 4300, Loss: 2.3946, LR: 0.000047
2025-10-21 22:38:16,050 - INFO - Batch 4400, Loss: 2.3311, LR: 0.000047
2025-10-21 22:38:32,119 - INFO - Batch 4500, Loss: 2.5207, LR: 0.000047
2025-10-21 22:38:48,139 - INFO - Batch 4600, Loss: 2.4696, LR: 0.000047
2025-10-21 22:39:04,115 - INFO - Batch 4700, Loss: 2.2418, LR: 0.000047
2025-10-21 22:39:20,143 - INFO - Batch 4800, Loss: 2.7474, LR: 0.000047
2025-10-21 22:39:36,164 - INFO - Batch 4900, Loss: 2.4390, LR: 0.000047
2025-10-21 22:39:52,185 - INFO - Batch 5000, Loss: 2.1069, LR: 0.000047
2025-10-21 22:40:08,204 - INFO - Batch 5100, Loss: 2.0514, LR: 0.000047
2025-10-21 22:40:24,023 - INFO - Batch 5200, Loss: 2.4861, LR: 0.000047
2025-10-21 22:40:39,806 - INFO - Batch 5300, Loss: 1.9435, LR: 0.000046
2025-10-21 22:40:55,604 - INFO - Batch 5400, Loss: 2.6409, LR: 0.000046
2025-10-21 22:41:11,526 - INFO - Batch 5500, Loss: 2.0682, LR: 0.000046
2025-10-21 22:41:27,529 - INFO - Batch 5600, Loss: 2.1565, LR: 0.000046
2025-10-21 22:41:43,598 - INFO - Batch 5700, Loss: 2.1607, LR: 0.000046
2025-10-21 22:41:59,641 - INFO - Batch 5800, Loss: 2.0802, LR: 0.000046
2025-10-21 22:42:15,674 - INFO - Batch 5900, Loss: 2.3988, LR: 0.000046
2025-10-21 22:42:31,727 - INFO - Batch 6000, Loss: 2.3049, LR: 0.000046
2025-10-21 22:42:47,821 - INFO - Batch 6100, Loss: 2.1856, LR: 0.000046
2025-10-21 22:43:03,852 - INFO - Batch 6200, Loss: 2.0070, LR: 0.000046
2025-10-21 22:43:19,701 - INFO - Batch 6300, Loss: 2.2356, LR: 0.000046
2025-10-21 22:43:35,537 - INFO - Batch 6400, Loss: 2.5294, LR: 0.000046
2025-10-21 22:43:43,491 - INFO - Epoch 16/30: Train Loss: 2.2466, Val Loss: 2.1676, LR: 0.000046
2025-10-21 22:43:43,780 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 22:43:43,952 - INFO - Batch 0, Loss: 2.1521, LR: 0.000046
2025-10-21 22:43:59,918 - INFO - Batch 100, Loss: 2.1489, LR: 0.000045
2025-10-21 22:44:16,136 - INFO - Batch 200, Loss: 2.1972, LR: 0.000045
2025-10-21 22:44:32,280 - INFO - Batch 300, Loss: 2.1938, LR: 0.000045
2025-10-21 22:44:48,430 - INFO - Batch 400, Loss: 2.1238, LR: 0.000045
2025-10-21 22:45:04,581 - INFO - Batch 500, Loss: 2.2986, LR: 0.000045
2025-10-21 22:45:20,727 - INFO - Batch 600, Loss: 2.2653, LR: 0.000045
2025-10-21 22:45:36,877 - INFO - Batch 700, Loss: 2.0392, LR: 0.000045
2025-10-21 22:45:52,996 - INFO - Batch 800, Loss: 2.2848, LR: 0.000045
2025-10-21 22:46:09,037 - INFO - Batch 900, Loss: 2.2778, LR: 0.000045
2025-10-21 22:46:25,051 - INFO - Batch 1000, Loss: 2.5135, LR: 0.000045
2025-10-21 22:46:41,007 - INFO - Batch 1100, Loss: 2.2775, LR: 0.000045
2025-10-21 22:46:57,057 - INFO - Batch 1200, Loss: 2.2870, LR: 0.000045
2025-10-21 22:47:13,191 - INFO - Batch 1300, Loss: 2.0999, LR: 0.000044
2025-10-21 22:47:29,294 - INFO - Batch 1400, Loss: 2.3435, LR: 0.000044
2025-10-21 22:47:45,489 - INFO - Batch 1500, Loss: 2.1816, LR: 0.000044
2025-10-21 22:48:01,624 - INFO - Batch 1600, Loss: 2.2884, LR: 0.000044
2025-10-21 22:48:17,740 - INFO - Batch 1700, Loss: 2.0038, LR: 0.000044
2025-10-21 22:48:33,893 - INFO - Batch 1800, Loss: 2.5028, LR: 0.000044
2025-10-21 22:48:49,983 - INFO - Batch 1900, Loss: 1.9903, LR: 0.000044
2025-10-21 22:49:05,892 - INFO - Batch 2000, Loss: 2.5179, LR: 0.000044
2025-10-21 22:49:21,827 - INFO - Batch 2100, Loss: 2.0989, LR: 0.000044
2025-10-21 22:49:37,800 - INFO - Batch 2200, Loss: 2.3924, LR: 0.000044
2025-10-21 22:49:53,765 - INFO - Batch 2300, Loss: 2.2521, LR: 0.000044
2025-10-21 22:50:09,922 - INFO - Batch 2400, Loss: 2.4236, LR: 0.000044
2025-10-21 22:50:26,040 - INFO - Batch 2500, Loss: 2.5408, LR: 0.000043
2025-10-21 22:50:42,175 - INFO - Batch 2600, Loss: 2.3381, LR: 0.000043
2025-10-21 22:50:58,262 - INFO - Batch 2700, Loss: 2.0905, LR: 0.000043
2025-10-21 22:51:14,377 - INFO - Batch 2800, Loss: 2.2474, LR: 0.000043
2025-10-21 22:51:30,484 - INFO - Batch 2900, Loss: 2.3767, LR: 0.000043
2025-10-21 22:51:46,563 - INFO - Batch 3000, Loss: 2.3146, LR: 0.000043
2025-10-21 22:52:02,497 - INFO - Batch 3100, Loss: 2.2111, LR: 0.000043
2025-10-21 22:52:18,366 - INFO - Batch 3200, Loss: 2.1070, LR: 0.000043
2025-10-21 22:52:34,190 - INFO - Batch 3300, Loss: 2.3184, LR: 0.000043
2025-10-21 22:52:50,188 - INFO - Batch 3400, Loss: 2.0391, LR: 0.000043
2025-10-21 22:53:06,287 - INFO - Batch 3500, Loss: 2.4632, LR: 0.000043
2025-10-21 22:53:22,380 - INFO - Batch 3600, Loss: 2.2765, LR: 0.000043
2025-10-21 22:53:38,415 - INFO - Batch 3700, Loss: 2.2672, LR: 0.000043
2025-10-21 22:53:54,559 - INFO - Batch 3800, Loss: 2.4667, LR: 0.000042
2025-10-21 22:54:10,639 - INFO - Batch 3900, Loss: 2.4043, LR: 0.000042
2025-10-21 22:54:26,710 - INFO - Batch 4000, Loss: 2.2188, LR: 0.000042
2025-10-21 22:54:42,750 - INFO - Batch 4100, Loss: 2.2594, LR: 0.000042
2025-10-21 22:54:58,675 - INFO - Batch 4200, Loss: 1.9683, LR: 0.000042
2025-10-21 22:55:14,590 - INFO - Batch 4300, Loss: 2.2104, LR: 0.000042
2025-10-21 22:55:30,449 - INFO - Batch 4400, Loss: 2.4222, LR: 0.000042
2025-10-21 22:55:46,384 - INFO - Batch 4500, Loss: 2.3066, LR: 0.000042
2025-10-21 22:56:02,426 - INFO - Batch 4600, Loss: 2.5525, LR: 0.000042
2025-10-21 22:56:18,451 - INFO - Batch 4700, Loss: 2.3681, LR: 0.000042
2025-10-21 22:56:34,508 - INFO - Batch 4800, Loss: 2.0409, LR: 0.000042
2025-10-21 22:56:50,533 - INFO - Batch 4900, Loss: 2.1638, LR: 0.000042
2025-10-21 22:57:06,594 - INFO - Batch 5000, Loss: 2.1578, LR: 0.000041
2025-10-21 22:57:22,638 - INFO - Batch 5100, Loss: 2.2039, LR: 0.000041
2025-10-21 22:57:38,679 - INFO - Batch 5200, Loss: 2.1667, LR: 0.000041
2025-10-21 22:57:54,569 - INFO - Batch 5300, Loss: 2.1626, LR: 0.000041
2025-10-21 22:58:10,375 - INFO - Batch 5400, Loss: 2.3665, LR: 0.000041
2025-10-21 22:58:26,165 - INFO - Batch 5500, Loss: 2.0162, LR: 0.000041
2025-10-21 22:58:42,012 - INFO - Batch 5600, Loss: 2.2573, LR: 0.000041
2025-10-21 22:58:58,037 - INFO - Batch 5700, Loss: 2.0665, LR: 0.000041
2025-10-21 22:59:14,065 - INFO - Batch 5800, Loss: 2.4800, LR: 0.000041
2025-10-21 22:59:30,098 - INFO - Batch 5900, Loss: 2.2579, LR: 0.000041
2025-10-21 22:59:46,151 - INFO - Batch 6000, Loss: 2.2388, LR: 0.000041
2025-10-21 23:00:02,274 - INFO - Batch 6100, Loss: 2.0883, LR: 0.000041
2025-10-21 23:00:18,323 - INFO - Batch 6200, Loss: 2.4926, LR: 0.000040
2025-10-21 23:00:34,307 - INFO - Batch 6300, Loss: 2.2115, LR: 0.000040
2025-10-21 23:00:50,226 - INFO - Batch 6400, Loss: 2.2233, LR: 0.000040
2025-10-21 23:00:58,189 - INFO - Epoch 17/30: Train Loss: 2.2260, Val Loss: 2.1497, LR: 0.000040
2025-10-21 23:00:58,757 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 23:00:58,925 - INFO - Batch 0, Loss: 2.3203, LR: 0.000040
2025-10-21 23:01:14,625 - INFO - Batch 100, Loss: 2.4186, LR: 0.000040
2025-10-21 23:01:30,255 - INFO - Batch 200, Loss: 2.4386, LR: 0.000040
2025-10-21 23:01:46,190 - INFO - Batch 300, Loss: 2.1242, LR: 0.000040
2025-10-21 23:02:02,160 - INFO - Batch 400, Loss: 2.1616, LR: 0.000040
2025-10-21 23:02:18,078 - INFO - Batch 500, Loss: 2.2139, LR: 0.000040
2025-10-21 23:02:34,014 - INFO - Batch 600, Loss: 2.4851, LR: 0.000040
2025-10-21 23:02:49,915 - INFO - Batch 700, Loss: 2.4221, LR: 0.000040
2025-10-21 23:03:05,843 - INFO - Batch 800, Loss: 2.0277, LR: 0.000040
2025-10-21 23:03:21,803 - INFO - Batch 900, Loss: 2.0444, LR: 0.000040
2025-10-21 23:03:37,671 - INFO - Batch 1000, Loss: 2.3977, LR: 0.000039
2025-10-21 23:03:53,442 - INFO - Batch 1100, Loss: 2.3918, LR: 0.000039
2025-10-21 23:04:09,237 - INFO - Batch 1200, Loss: 2.1455, LR: 0.000039
2025-10-21 23:04:24,950 - INFO - Batch 1300, Loss: 2.2088, LR: 0.000039
2025-10-21 23:04:40,923 - INFO - Batch 1400, Loss: 2.3344, LR: 0.000039
2025-10-21 23:04:56,974 - INFO - Batch 1500, Loss: 2.2888, LR: 0.000039
2025-10-21 23:05:12,961 - INFO - Batch 1600, Loss: 2.1282, LR: 0.000039
2025-10-21 23:05:28,980 - INFO - Batch 1700, Loss: 2.1107, LR: 0.000039
2025-10-21 23:05:45,012 - INFO - Batch 1800, Loss: 2.4027, LR: 0.000039
2025-10-21 23:06:01,033 - INFO - Batch 1900, Loss: 2.2265, LR: 0.000039
2025-10-21 23:06:17,135 - INFO - Batch 2000, Loss: 2.4056, LR: 0.000039
2025-10-21 23:06:33,027 - INFO - Batch 2100, Loss: 2.1231, LR: 0.000039
2025-10-21 23:06:48,832 - INFO - Batch 2200, Loss: 2.2255, LR: 0.000039
2025-10-21 23:07:04,646 - INFO - Batch 2300, Loss: 2.0486, LR: 0.000038
2025-10-21 23:07:20,454 - INFO - Batch 2400, Loss: 2.1660, LR: 0.000038
2025-10-21 23:07:36,433 - INFO - Batch 2500, Loss: 2.2669, LR: 0.000038
2025-10-21 23:07:52,568 - INFO - Batch 2600, Loss: 2.2834, LR: 0.000038
2025-10-21 23:08:08,609 - INFO - Batch 2700, Loss: 2.1723, LR: 0.000038
2025-10-21 23:08:24,625 - INFO - Batch 2800, Loss: 2.0959, LR: 0.000038
2025-10-21 23:08:40,699 - INFO - Batch 2900, Loss: 2.4679, LR: 0.000038
2025-10-21 23:08:56,699 - INFO - Batch 3000, Loss: 2.2628, LR: 0.000038
2025-10-21 23:09:12,698 - INFO - Batch 3100, Loss: 2.2899, LR: 0.000038
2025-10-21 23:09:28,597 - INFO - Batch 3200, Loss: 1.9381, LR: 0.000038
2025-10-21 23:09:44,386 - INFO - Batch 3300, Loss: 2.5769, LR: 0.000038
2025-10-21 23:10:00,211 - INFO - Batch 3400, Loss: 2.2287, LR: 0.000038
2025-10-21 23:10:16,045 - INFO - Batch 3500, Loss: 2.2399, LR: 0.000037
2025-10-21 23:10:32,120 - INFO - Batch 3600, Loss: 2.4131, LR: 0.000037
2025-10-21 23:10:48,197 - INFO - Batch 3700, Loss: 2.2922, LR: 0.000037
2025-10-21 23:11:04,168 - INFO - Batch 3800, Loss: 2.0950, LR: 0.000037
2025-10-21 23:11:20,262 - INFO - Batch 3900, Loss: 2.2002, LR: 0.000037
2025-10-21 23:11:36,276 - INFO - Batch 4000, Loss: 2.0455, LR: 0.000037
2025-10-21 23:11:52,334 - INFO - Batch 4100, Loss: 2.1195, LR: 0.000037
2025-10-21 23:12:08,450 - INFO - Batch 4200, Loss: 2.1603, LR: 0.000037
2025-10-21 23:12:24,357 - INFO - Batch 4300, Loss: 2.1099, LR: 0.000037
2025-10-21 23:12:40,163 - INFO - Batch 4400, Loss: 2.2464, LR: 0.000037
2025-10-21 23:12:56,104 - INFO - Batch 4500, Loss: 2.2071, LR: 0.000037
2025-10-21 23:13:11,897 - INFO - Batch 4600, Loss: 2.3092, LR: 0.000037
2025-10-21 23:13:27,855 - INFO - Batch 4700, Loss: 2.1419, LR: 0.000037
2025-10-21 23:13:43,884 - INFO - Batch 4800, Loss: 2.3405, LR: 0.000036
2025-10-21 23:13:59,983 - INFO - Batch 4900, Loss: 2.4174, LR: 0.000036
2025-10-21 23:14:16,031 - INFO - Batch 5000, Loss: 2.2444, LR: 0.000036
2025-10-21 23:14:32,073 - INFO - Batch 5100, Loss: 2.1561, LR: 0.000036
2025-10-21 23:14:48,089 - INFO - Batch 5200, Loss: 2.2455, LR: 0.000036
2025-10-21 23:15:04,095 - INFO - Batch 5300, Loss: 2.1805, LR: 0.000036
2025-10-21 23:15:19,939 - INFO - Batch 5400, Loss: 2.1257, LR: 0.000036
2025-10-21 23:15:35,754 - INFO - Batch 5500, Loss: 2.0366, LR: 0.000036
2025-10-21 23:15:51,531 - INFO - Batch 5600, Loss: 2.2112, LR: 0.000036
2025-10-21 23:16:07,377 - INFO - Batch 5700, Loss: 2.2669, LR: 0.000036
2025-10-21 23:16:23,392 - INFO - Batch 5800, Loss: 2.1786, LR: 0.000036
2025-10-21 23:16:39,450 - INFO - Batch 5900, Loss: 1.8611, LR: 0.000036
2025-10-21 23:16:55,466 - INFO - Batch 6000, Loss: 2.3201, LR: 0.000036
2025-10-21 23:17:11,444 - INFO - Batch 6100, Loss: 2.3301, LR: 0.000035
2025-10-21 23:17:27,475 - INFO - Batch 6200, Loss: 2.3208, LR: 0.000035
2025-10-21 23:17:43,471 - INFO - Batch 6300, Loss: 2.2224, LR: 0.000035
2025-10-21 23:17:59,547 - INFO - Batch 6400, Loss: 2.3571, LR: 0.000035
2025-10-21 23:18:07,519 - INFO - Epoch 18/30: Train Loss: 2.2068, Val Loss: 2.1406, LR: 0.000035
2025-10-21 23:18:08,081 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 23:18:08,246 - INFO - Batch 0, Loss: 2.1187, LR: 0.000035
2025-10-21 23:18:24,175 - INFO - Batch 100, Loss: 2.3256, LR: 0.000035
2025-10-21 23:18:40,112 - INFO - Batch 200, Loss: 2.2707, LR: 0.000035
2025-10-21 23:18:56,049 - INFO - Batch 300, Loss: 2.1552, LR: 0.000035
2025-10-21 23:19:12,155 - INFO - Batch 400, Loss: 2.1638, LR: 0.000035
2025-10-21 23:19:28,248 - INFO - Batch 500, Loss: 2.2281, LR: 0.000035
2025-10-21 23:19:44,366 - INFO - Batch 600, Loss: 2.1794, LR: 0.000035
2025-10-21 23:20:00,512 - INFO - Batch 700, Loss: 1.9443, LR: 0.000035
2025-10-21 23:20:16,601 - INFO - Batch 800, Loss: 2.1773, LR: 0.000035
2025-10-21 23:20:32,693 - INFO - Batch 900, Loss: 2.3190, LR: 0.000034
2025-10-21 23:20:48,818 - INFO - Batch 1000, Loss: 1.9708, LR: 0.000034
2025-10-21 23:21:04,811 - INFO - Batch 1100, Loss: 2.2644, LR: 0.000034
2025-10-21 23:21:20,710 - INFO - Batch 1200, Loss: 1.8743, LR: 0.000034
2025-10-21 23:21:36,603 - INFO - Batch 1300, Loss: 2.0098, LR: 0.000034
2025-10-21 23:21:52,629 - INFO - Batch 1400, Loss: 2.0232, LR: 0.000034
2025-10-21 23:22:08,752 - INFO - Batch 1500, Loss: 2.3014, LR: 0.000034
2025-10-21 23:22:24,859 - INFO - Batch 1600, Loss: 2.4121, LR: 0.000034
2025-10-21 23:22:40,970 - INFO - Batch 1700, Loss: 2.0694, LR: 0.000034
2025-10-21 23:22:57,107 - INFO - Batch 1800, Loss: 1.9947, LR: 0.000034
2025-10-21 23:23:13,194 - INFO - Batch 1900, Loss: 1.9979, LR: 0.000034
2025-10-21 23:23:29,333 - INFO - Batch 2000, Loss: 2.0220, LR: 0.000034
2025-10-21 23:23:45,421 - INFO - Batch 2100, Loss: 2.2093, LR: 0.000034
2025-10-21 23:24:01,366 - INFO - Batch 2200, Loss: 2.0332, LR: 0.000033
2025-10-21 23:24:17,291 - INFO - Batch 2300, Loss: 2.2264, LR: 0.000033
2025-10-21 23:24:33,198 - INFO - Batch 2400, Loss: 2.1290, LR: 0.000033
2025-10-21 23:24:49,184 - INFO - Batch 2500, Loss: 2.1271, LR: 0.000033
2025-10-21 23:25:05,308 - INFO - Batch 2600, Loss: 2.1522, LR: 0.000033
2025-10-21 23:25:21,405 - INFO - Batch 2700, Loss: 2.3266, LR: 0.000033
2025-10-21 23:25:37,521 - INFO - Batch 2800, Loss: 1.8752, LR: 0.000033
2025-10-21 23:25:53,703 - INFO - Batch 2900, Loss: 2.1506, LR: 0.000033
2025-10-21 23:26:09,805 - INFO - Batch 3000, Loss: 2.0737, LR: 0.000033
2025-10-21 23:26:25,951 - INFO - Batch 3100, Loss: 2.4050, LR: 0.000033
2025-10-21 23:26:42,053 - INFO - Batch 3200, Loss: 2.4002, LR: 0.000033
2025-10-21 23:26:58,003 - INFO - Batch 3300, Loss: 2.2434, LR: 0.000033
2025-10-21 23:27:13,900 - INFO - Batch 3400, Loss: 2.1957, LR: 0.000033
2025-10-21 23:27:29,824 - INFO - Batch 3500, Loss: 2.0879, LR: 0.000032
2025-10-21 23:27:45,901 - INFO - Batch 3600, Loss: 2.1759, LR: 0.000032
2025-10-21 23:28:02,066 - INFO - Batch 3700, Loss: 2.1342, LR: 0.000032
2025-10-21 23:28:18,187 - INFO - Batch 3800, Loss: 2.0711, LR: 0.000032
2025-10-21 23:28:34,283 - INFO - Batch 3900, Loss: 2.1144, LR: 0.000032
2025-10-21 23:28:50,459 - INFO - Batch 4000, Loss: 2.0787, LR: 0.000032
2025-10-21 23:29:06,558 - INFO - Batch 4100, Loss: 2.1712, LR: 0.000032
2025-10-21 23:29:22,685 - INFO - Batch 4200, Loss: 2.2296, LR: 0.000032
2025-10-21 23:29:38,744 - INFO - Batch 4300, Loss: 2.4884, LR: 0.000032
2025-10-21 23:29:54,616 - INFO - Batch 4400, Loss: 2.2377, LR: 0.000032
2025-10-21 23:30:10,490 - INFO - Batch 4500, Loss: 2.2485, LR: 0.000032
2025-10-21 23:30:26,386 - INFO - Batch 4600, Loss: 2.1569, LR: 0.000032
2025-10-21 23:30:42,439 - INFO - Batch 4700, Loss: 2.0393, LR: 0.000032
2025-10-21 23:30:58,557 - INFO - Batch 4800, Loss: 2.2288, LR: 0.000031
2025-10-21 23:31:14,729 - INFO - Batch 4900, Loss: 2.1225, LR: 0.000031
2025-10-21 23:31:30,857 - INFO - Batch 5000, Loss: 2.1516, LR: 0.000031
2025-10-21 23:31:46,990 - INFO - Batch 5100, Loss: 2.0433, LR: 0.000031
2025-10-21 23:32:03,132 - INFO - Batch 5200, Loss: 2.2304, LR: 0.000031
2025-10-21 23:32:19,244 - INFO - Batch 5300, Loss: 2.1714, LR: 0.000031
2025-10-21 23:32:35,297 - INFO - Batch 5400, Loss: 2.4056, LR: 0.000031
2025-10-21 23:32:51,274 - INFO - Batch 5500, Loss: 1.7752, LR: 0.000031
2025-10-21 23:33:07,184 - INFO - Batch 5600, Loss: 2.1147, LR: 0.000031
2025-10-21 23:33:23,051 - INFO - Batch 5700, Loss: 2.1872, LR: 0.000031
2025-10-21 23:33:39,172 - INFO - Batch 5800, Loss: 2.1233, LR: 0.000031
2025-10-21 23:33:55,372 - INFO - Batch 5900, Loss: 2.0416, LR: 0.000031
2025-10-21 23:34:11,465 - INFO - Batch 6000, Loss: 2.0335, LR: 0.000031
2025-10-21 23:34:27,549 - INFO - Batch 6100, Loss: 1.8819, LR: 0.000030
2025-10-21 23:34:43,776 - INFO - Batch 6200, Loss: 2.5071, LR: 0.000030
2025-10-21 23:34:59,848 - INFO - Batch 6300, Loss: 2.2303, LR: 0.000030
2025-10-21 23:35:15,923 - INFO - Batch 6400, Loss: 2.2165, LR: 0.000030
2025-10-21 23:35:23,946 - INFO - Epoch 19/30: Train Loss: 2.1897, Val Loss: 2.1202, LR: 0.000030
2025-10-21 23:35:24,205 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 23:35:24,374 - INFO - Batch 0, Loss: 2.2394, LR: 0.000030
2025-10-21 23:35:40,227 - INFO - Batch 100, Loss: 2.0708, LR: 0.000030
2025-10-21 23:35:56,075 - INFO - Batch 200, Loss: 2.1170, LR: 0.000030
2025-10-21 23:36:11,959 - INFO - Batch 300, Loss: 2.1834, LR: 0.000030
2025-10-21 23:36:28,111 - INFO - Batch 400, Loss: 2.2400, LR: 0.000030
2025-10-21 23:36:44,232 - INFO - Batch 500, Loss: 1.9663, LR: 0.000030
2025-10-21 23:37:00,384 - INFO - Batch 600, Loss: 2.1407, LR: 0.000030
2025-10-21 23:37:16,470 - INFO - Batch 700, Loss: 2.1120, LR: 0.000030
2025-10-21 23:37:32,539 - INFO - Batch 800, Loss: 2.3047, LR: 0.000030
2025-10-21 23:37:48,675 - INFO - Batch 900, Loss: 2.2137, LR: 0.000030
2025-10-21 23:38:04,911 - INFO - Batch 1000, Loss: 1.9939, LR: 0.000029
2025-10-21 23:38:20,851 - INFO - Batch 1100, Loss: 2.4554, LR: 0.000029
2025-10-21 23:38:36,655 - INFO - Batch 1200, Loss: 2.1036, LR: 0.000029
2025-10-21 23:38:52,549 - INFO - Batch 1300, Loss: 2.1571, LR: 0.000029
2025-10-21 23:39:08,447 - INFO - Batch 1400, Loss: 2.0905, LR: 0.000029
2025-10-21 23:39:24,561 - INFO - Batch 1500, Loss: 2.0050, LR: 0.000029
2025-10-21 23:39:40,697 - INFO - Batch 1600, Loss: 2.0101, LR: 0.000029
2025-10-21 23:39:56,914 - INFO - Batch 1700, Loss: 2.2478, LR: 0.000029
2025-10-21 23:40:12,985 - INFO - Batch 1800, Loss: 2.0110, LR: 0.000029
2025-10-21 23:40:29,097 - INFO - Batch 1900, Loss: 2.1596, LR: 0.000029
2025-10-21 23:40:45,419 - INFO - Batch 2000, Loss: 1.9736, LR: 0.000029
2025-10-21 23:41:01,611 - INFO - Batch 2100, Loss: 2.1859, LR: 0.000029
2025-10-21 23:41:17,603 - INFO - Batch 2200, Loss: 2.3064, LR: 0.000029
2025-10-21 23:41:33,597 - INFO - Batch 2300, Loss: 2.1260, LR: 0.000028
2025-10-21 23:41:49,468 - INFO - Batch 2400, Loss: 2.1327, LR: 0.000028
2025-10-21 23:42:05,436 - INFO - Batch 2500, Loss: 2.2198, LR: 0.000028
2025-10-21 23:42:21,621 - INFO - Batch 2600, Loss: 2.3557, LR: 0.000028
2025-10-21 23:42:37,798 - INFO - Batch 2700, Loss: 2.5918, LR: 0.000028
2025-10-21 23:42:53,956 - INFO - Batch 2800, Loss: 2.3156, LR: 0.000028
2025-10-21 23:43:10,056 - INFO - Batch 2900, Loss: 1.9982, LR: 0.000028
2025-10-21 23:43:26,157 - INFO - Batch 3000, Loss: 2.2856, LR: 0.000028
2025-10-21 23:43:42,365 - INFO - Batch 3100, Loss: 2.2083, LR: 0.000028
2025-10-21 23:43:58,582 - INFO - Batch 3200, Loss: 2.1827, LR: 0.000028
2025-10-21 23:44:14,555 - INFO - Batch 3300, Loss: 2.3841, LR: 0.000028
2025-10-21 23:44:30,510 - INFO - Batch 3400, Loss: 2.1576, LR: 0.000028
2025-10-21 23:44:46,654 - INFO - Batch 3500, Loss: 2.4174, LR: 0.000028
2025-10-21 23:45:02,792 - INFO - Batch 3600, Loss: 2.4433, LR: 0.000028
2025-10-21 23:45:18,951 - INFO - Batch 3700, Loss: 1.9157, LR: 0.000027
2025-10-21 23:45:35,113 - INFO - Batch 3800, Loss: 2.0740, LR: 0.000027
2025-10-21 23:45:51,338 - INFO - Batch 3900, Loss: 2.1694, LR: 0.000027
2025-10-21 23:46:07,411 - INFO - Batch 4000, Loss: 2.1799, LR: 0.000027
2025-10-21 23:46:23,531 - INFO - Batch 4100, Loss: 2.1659, LR: 0.000027
2025-10-21 23:46:39,815 - INFO - Batch 4200, Loss: 2.2144, LR: 0.000027
2025-10-21 23:46:55,889 - INFO - Batch 4300, Loss: 2.3753, LR: 0.000027
2025-10-21 23:47:11,792 - INFO - Batch 4400, Loss: 2.4591, LR: 0.000027
2025-10-21 23:47:27,838 - INFO - Batch 4500, Loss: 2.0311, LR: 0.000027
2025-10-21 23:47:43,934 - INFO - Batch 4600, Loss: 2.4691, LR: 0.000027
2025-10-21 23:48:00,077 - INFO - Batch 4700, Loss: 2.3683, LR: 0.000027
2025-10-21 23:48:16,291 - INFO - Batch 4800, Loss: 2.0193, LR: 0.000027
2025-10-21 23:48:32,472 - INFO - Batch 4900, Loss: 2.1788, LR: 0.000027
2025-10-21 23:48:48,639 - INFO - Batch 5000, Loss: 2.2072, LR: 0.000027
2025-10-21 23:49:04,810 - INFO - Batch 5100, Loss: 2.1272, LR: 0.000026
2025-10-21 23:49:20,909 - INFO - Batch 5200, Loss: 2.2422, LR: 0.000026
2025-10-21 23:49:37,120 - INFO - Batch 5300, Loss: 2.0365, LR: 0.000026
2025-10-21 23:49:53,213 - INFO - Batch 5400, Loss: 2.1355, LR: 0.000026
2025-10-21 23:50:09,146 - INFO - Batch 5500, Loss: 2.3022, LR: 0.000026
2025-10-21 23:50:25,089 - INFO - Batch 5600, Loss: 2.1097, LR: 0.000026
2025-10-21 23:50:41,251 - INFO - Batch 5700, Loss: 2.0659, LR: 0.000026
2025-10-21 23:50:57,376 - INFO - Batch 5800, Loss: 2.1292, LR: 0.000026
2025-10-21 23:51:13,505 - INFO - Batch 5900, Loss: 2.2567, LR: 0.000026
2025-10-21 23:51:29,678 - INFO - Batch 6000, Loss: 2.2159, LR: 0.000026
2025-10-21 23:51:46,008 - INFO - Batch 6100, Loss: 1.9605, LR: 0.000026
2025-10-21 23:52:02,250 - INFO - Batch 6200, Loss: 2.3025, LR: 0.000026
2025-10-21 23:52:18,456 - INFO - Batch 6300, Loss: 2.2546, LR: 0.000026
2025-10-21 23:52:34,773 - INFO - Batch 6400, Loss: 2.1946, LR: 0.000026
2025-10-21 23:52:42,829 - INFO - Epoch 20/30: Train Loss: 2.1746, Val Loss: 2.1079, LR: 0.000025
2025-10-21 23:52:43,097 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 23:52:43,311 - INFO - 模型已保存到: ./checkpoints/checkpoint_epoch_20.pth
2025-10-21 23:52:43,474 - INFO - Batch 0, Loss: 2.0328, LR: 0.000025
2025-10-21 23:52:59,547 - INFO - Batch 100, Loss: 2.0562, LR: 0.000025
2025-10-21 23:53:15,666 - INFO - Batch 200, Loss: 2.2489, LR: 0.000025
2025-10-21 23:53:31,903 - INFO - Batch 300, Loss: 1.9901, LR: 0.000025
2025-10-21 23:53:48,115 - INFO - Batch 400, Loss: 2.2762, LR: 0.000025
2025-10-21 23:54:04,320 - INFO - Batch 500, Loss: 2.2997, LR: 0.000025
2025-10-21 23:54:20,516 - INFO - Batch 600, Loss: 2.1194, LR: 0.000025
2025-10-21 23:54:36,805 - INFO - Batch 700, Loss: 2.2370, LR: 0.000025
2025-10-21 23:54:53,029 - INFO - Batch 800, Loss: 2.1472, LR: 0.000025
2025-10-21 23:55:09,242 - INFO - Batch 900, Loss: 2.2520, LR: 0.000025
2025-10-21 23:55:25,324 - INFO - Batch 1000, Loss: 2.1244, LR: 0.000025
2025-10-21 23:55:41,316 - INFO - Batch 1100, Loss: 2.0231, LR: 0.000025
2025-10-21 23:55:57,350 - INFO - Batch 1200, Loss: 2.1639, LR: 0.000025
2025-10-21 23:56:13,370 - INFO - Batch 1300, Loss: 2.1268, LR: 0.000025
2025-10-21 23:56:29,703 - INFO - Batch 1400, Loss: 1.9037, LR: 0.000024
2025-10-21 23:56:45,922 - INFO - Batch 1500, Loss: 2.0724, LR: 0.000024
2025-10-21 23:57:02,082 - INFO - Batch 1600, Loss: 1.9699, LR: 0.000024
2025-10-21 23:57:18,357 - INFO - Batch 1700, Loss: 1.9541, LR: 0.000024
2025-10-21 23:57:34,582 - INFO - Batch 1800, Loss: 2.3390, LR: 0.000024
2025-10-21 23:57:50,779 - INFO - Batch 1900, Loss: 2.0217, LR: 0.000024
2025-10-21 23:58:06,905 - INFO - Batch 2000, Loss: 2.0000, LR: 0.000024
2025-10-21 23:58:22,837 - INFO - Batch 2100, Loss: 2.2709, LR: 0.000024
2025-10-21 23:58:38,908 - INFO - Batch 2200, Loss: 2.0489, LR: 0.000024
2025-10-21 23:58:55,022 - INFO - Batch 2300, Loss: 2.1617, LR: 0.000024
2025-10-21 23:59:11,241 - INFO - Batch 2400, Loss: 1.9440, LR: 0.000024
2025-10-21 23:59:27,510 - INFO - Batch 2500, Loss: 2.0751, LR: 0.000024
2025-10-21 23:59:43,703 - INFO - Batch 2600, Loss: 2.1606, LR: 0.000024
2025-10-21 23:59:59,905 - INFO - Batch 2700, Loss: 1.9147, LR: 0.000024
2025-10-22 00:00:16,169 - INFO - Batch 2800, Loss: 2.2134, LR: 0.000023
2025-10-22 00:00:32,428 - INFO - Batch 2900, Loss: 2.1797, LR: 0.000023
2025-10-22 00:00:48,635 - INFO - Batch 3000, Loss: 2.0230, LR: 0.000023
2025-10-22 00:01:04,793 - INFO - Batch 3100, Loss: 2.2085, LR: 0.000023
2025-10-22 00:01:20,895 - INFO - Batch 3200, Loss: 2.0380, LR: 0.000023
2025-10-22 00:01:36,962 - INFO - Batch 3300, Loss: 2.3404, LR: 0.000023
2025-10-22 00:01:53,146 - INFO - Batch 3400, Loss: 2.2364, LR: 0.000023
2025-10-22 00:02:09,406 - INFO - Batch 3500, Loss: 2.2265, LR: 0.000023
2025-10-22 00:02:25,657 - INFO - Batch 3600, Loss: 2.2381, LR: 0.000023
2025-10-22 00:02:41,850 - INFO - Batch 3700, Loss: 2.0290, LR: 0.000023
2025-10-22 00:02:58,003 - INFO - Batch 3800, Loss: 2.3591, LR: 0.000023
2025-10-22 00:03:14,267 - INFO - Batch 3900, Loss: 2.0847, LR: 0.000023
2025-10-22 00:03:30,471 - INFO - Batch 4000, Loss: 2.0617, LR: 0.000023
2025-10-22 00:03:46,612 - INFO - Batch 4100, Loss: 2.2043, LR: 0.000023
2025-10-22 00:04:02,721 - INFO - Batch 4200, Loss: 1.9879, LR: 0.000023
2025-10-22 00:04:18,836 - INFO - Batch 4300, Loss: 2.1175, LR: 0.000022
2025-10-22 00:04:34,914 - INFO - Batch 4400, Loss: 1.9707, LR: 0.000022
2025-10-22 00:04:51,098 - INFO - Batch 4500, Loss: 2.0370, LR: 0.000022
2025-10-22 00:05:07,470 - INFO - Batch 4600, Loss: 2.0575, LR: 0.000022
2025-10-22 00:05:23,665 - INFO - Batch 4700, Loss: 2.2282, LR: 0.000022
2025-10-22 00:05:39,864 - INFO - Batch 4800, Loss: 1.9016, LR: 0.000022
2025-10-22 00:05:56,082 - INFO - Batch 4900, Loss: 1.8881, LR: 0.000022
2025-10-22 00:06:12,347 - INFO - Batch 5000, Loss: 2.0702, LR: 0.000022
2025-10-22 00:06:28,477 - INFO - Batch 5100, Loss: 2.2472, LR: 0.000022
2025-10-22 00:06:44,546 - INFO - Batch 5200, Loss: 2.1371, LR: 0.000022
2025-10-22 00:07:00,707 - INFO - Batch 5300, Loss: 2.2996, LR: 0.000022
2025-10-22 00:07:16,819 - INFO - Batch 5400, Loss: 2.2533, LR: 0.000022
2025-10-22 00:07:32,891 - INFO - Batch 5500, Loss: 2.2243, LR: 0.000022
2025-10-22 00:07:49,127 - INFO - Batch 5600, Loss: 2.0006, LR: 0.000022
2025-10-22 00:08:05,450 - INFO - Batch 5700, Loss: 2.1785, LR: 0.000022
2025-10-22 00:08:21,598 - INFO - Batch 5800, Loss: 2.1589, LR: 0.000021
2025-10-22 00:08:37,778 - INFO - Batch 5900, Loss: 2.2322, LR: 0.000021
2025-10-22 00:08:54,073 - INFO - Batch 6000, Loss: 2.2570, LR: 0.000021
2025-10-22 00:09:10,286 - INFO - Batch 6100, Loss: 2.1474, LR: 0.000021
2025-10-22 00:09:26,382 - INFO - Batch 6200, Loss: 2.0596, LR: 0.000021
2025-10-22 00:09:42,483 - INFO - Batch 6300, Loss: 2.5126, LR: 0.000021
2025-10-22 00:09:58,618 - INFO - Batch 6400, Loss: 2.3412, LR: 0.000021
2025-10-22 00:10:06,653 - INFO - Epoch 21/30: Train Loss: 2.1613, Val Loss: 2.1014, LR: 0.000021
2025-10-22 00:10:06,936 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-22 00:10:07,107 - INFO - Batch 0, Loss: 2.2709, LR: 0.000021
2025-10-22 00:10:23,278 - INFO - Batch 100, Loss: 2.2831, LR: 0.000021
2025-10-22 00:10:39,434 - INFO - Batch 200, Loss: 2.1002, LR: 0.000021
2025-10-22 00:10:55,742 - INFO - Batch 300, Loss: 1.9817, LR: 0.000021
2025-10-22 00:11:11,916 - INFO - Batch 400, Loss: 2.1212, LR: 0.000021
2025-10-22 00:11:28,116 - INFO - Batch 500, Loss: 2.3084, LR: 0.000021
2025-10-22 00:11:44,347 - INFO - Batch 600, Loss: 2.0128, LR: 0.000021
2025-10-22 00:12:00,574 - INFO - Batch 700, Loss: 2.0956, LR: 0.000021
2025-10-22 00:12:16,561 - INFO - Batch 800, Loss: 2.0833, LR: 0.000020
2025-10-22 00:12:32,640 - INFO - Batch 900, Loss: 2.1034, LR: 0.000020
2025-10-22 00:12:48,594 - INFO - Batch 1000, Loss: 2.1683, LR: 0.000020
2025-10-22 00:13:04,695 - INFO - Batch 1100, Loss: 2.1469, LR: 0.000020
2025-10-22 00:13:20,810 - INFO - Batch 1200, Loss: 2.1583, LR: 0.000020
2025-10-22 00:13:36,980 - INFO - Batch 1300, Loss: 2.1004, LR: 0.000020
2025-10-22 00:13:53,326 - INFO - Batch 1400, Loss: 2.1338, LR: 0.000020
2025-10-22 00:14:09,628 - INFO - Batch 1500, Loss: 2.2690, LR: 0.000020
2025-10-22 00:14:25,948 - INFO - Batch 1600, Loss: 1.9305, LR: 0.000020
2025-10-22 00:14:42,281 - INFO - Batch 1700, Loss: 2.1849, LR: 0.000020
2025-10-22 00:14:58,333 - INFO - Batch 1800, Loss: 2.2321, LR: 0.000020
2025-10-22 00:15:14,139 - INFO - Batch 1900, Loss: 1.9011, LR: 0.000020
2025-10-22 00:15:29,939 - INFO - Batch 2000, Loss: 2.1910, LR: 0.000020
2025-10-22 00:15:45,868 - INFO - Batch 2100, Loss: 2.1524, LR: 0.000020
2025-10-22 00:16:02,190 - INFO - Batch 2200, Loss: 1.8070, LR: 0.000020
2025-10-22 00:16:18,523 - INFO - Batch 2300, Loss: 2.2273, LR: 0.000019
2025-10-22 00:16:34,844 - INFO - Batch 2400, Loss: 1.9137, LR: 0.000019
2025-10-22 00:16:51,184 - INFO - Batch 2500, Loss: 2.3245, LR: 0.000019
2025-10-22 00:17:07,515 - INFO - Batch 2600, Loss: 2.0106, LR: 0.000019
2025-10-22 00:17:23,806 - INFO - Batch 2700, Loss: 2.2035, LR: 0.000019
2025-10-22 00:17:40,036 - INFO - Batch 2800, Loss: 2.2290, LR: 0.000019
2025-10-22 00:17:56,035 - INFO - Batch 2900, Loss: 2.1814, LR: 0.000019
2025-10-22 00:18:12,057 - INFO - Batch 3000, Loss: 2.3765, LR: 0.000019
2025-10-22 00:18:28,111 - INFO - Batch 3100, Loss: 2.4465, LR: 0.000019
2025-10-22 00:18:44,341 - INFO - Batch 3200, Loss: 2.1170, LR: 0.000019
2025-10-22 00:19:00,585 - INFO - Batch 3300, Loss: 2.4224, LR: 0.000019
2025-10-22 00:19:16,787 - INFO - Batch 3400, Loss: 2.1707, LR: 0.000019
2025-10-22 00:19:33,038 - INFO - Batch 3500, Loss: 2.1594, LR: 0.000019
2025-10-22 00:19:49,232 - INFO - Batch 3600, Loss: 2.0582, LR: 0.000019
2025-10-22 00:20:05,485 - INFO - Batch 3700, Loss: 1.9758, LR: 0.000019
2025-10-22 00:20:21,618 - INFO - Batch 3800, Loss: 2.3299, LR: 0.000019
2025-10-22 00:20:37,668 - INFO - Batch 3900, Loss: 2.1291, LR: 0.000018
2025-10-22 00:20:53,603 - INFO - Batch 4000, Loss: 2.2549, LR: 0.000018
2025-10-22 00:21:09,716 - INFO - Batch 4100, Loss: 2.1261, LR: 0.000018
2025-10-22 00:21:25,921 - INFO - Batch 4200, Loss: 2.2536, LR: 0.000018
2025-10-22 00:21:42,170 - INFO - Batch 4300, Loss: 2.3187, LR: 0.000018
2025-10-22 00:21:58,377 - INFO - Batch 4400, Loss: 1.9258, LR: 0.000018
2025-10-22 00:22:14,607 - INFO - Batch 4500, Loss: 2.1332, LR: 0.000018
2025-10-22 00:22:30,856 - INFO - Batch 4600, Loss: 2.2052, LR: 0.000018
2025-10-22 00:22:47,099 - INFO - Batch 4700, Loss: 2.1386, LR: 0.000018
2025-10-22 00:23:03,224 - INFO - Batch 4800, Loss: 2.1454, LR: 0.000018
2025-10-22 00:23:19,220 - INFO - Batch 4900, Loss: 2.1038, LR: 0.000018
2025-10-22 00:23:35,251 - INFO - Batch 5000, Loss: 2.2809, LR: 0.000018
2025-10-22 00:23:51,389 - INFO - Batch 5100, Loss: 2.3738, LR: 0.000018
2025-10-22 00:24:07,654 - INFO - Batch 5200, Loss: 2.1567, LR: 0.000018
2025-10-22 00:24:23,886 - INFO - Batch 5300, Loss: 1.9467, LR: 0.000018
2025-10-22 00:24:40,109 - INFO - Batch 5400, Loss: 2.0432, LR: 0.000018
2025-10-22 00:24:56,329 - INFO - Batch 5500, Loss: 2.3805, LR: 0.000017
2025-10-22 00:25:12,565 - INFO - Batch 5600, Loss: 1.9137, LR: 0.000017
2025-10-22 00:25:28,809 - INFO - Batch 5700, Loss: 2.0548, LR: 0.000017
2025-10-22 00:25:44,909 - INFO - Batch 5800, Loss: 2.0924, LR: 0.000017
2025-10-22 00:26:00,916 - INFO - Batch 5900, Loss: 1.9540, LR: 0.000017
2025-10-22 00:26:16,922 - INFO - Batch 6000, Loss: 2.1137, LR: 0.000017
2025-10-22 00:26:33,032 - INFO - Batch 6100, Loss: 2.2498, LR: 0.000017
2025-10-22 00:26:49,273 - INFO - Batch 6200, Loss: 2.2789, LR: 0.000017
2025-10-22 00:27:05,481 - INFO - Batch 6300, Loss: 2.2799, LR: 0.000017
2025-10-22 00:27:21,701 - INFO - Batch 6400, Loss: 1.9588, LR: 0.000017
2025-10-22 00:27:29,802 - INFO - Epoch 22/30: Train Loss: 2.1472, Val Loss: 2.1004, LR: 0.000017
2025-10-22 00:27:30,075 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-22 00:27:30,252 - INFO - Batch 0, Loss: 2.0537, LR: 0.000017
2025-10-22 00:27:46,135 - INFO - Batch 100, Loss: 2.2789, LR: 0.000017
2025-10-22 00:28:02,467 - INFO - Batch 200, Loss: 2.1120, LR: 0.000017
2025-10-22 00:28:18,716 - INFO - Batch 300, Loss: 2.1006, LR: 0.000017
2025-10-22 00:28:34,722 - INFO - Batch 400, Loss: 2.3963, LR: 0.000017
2025-10-22 00:28:50,665 - INFO - Batch 500, Loss: 1.8243, LR: 0.000017
2025-10-22 00:29:06,636 - INFO - Batch 600, Loss: 2.2394, LR: 0.000017
2025-10-22 00:29:22,974 - INFO - Batch 700, Loss: 2.0660, LR: 0.000016
2025-10-22 00:29:39,329 - INFO - Batch 800, Loss: 2.0084, LR: 0.000016
2025-10-22 00:29:55,389 - INFO - Batch 900, Loss: 2.2199, LR: 0.000016
2025-10-22 00:30:11,723 - INFO - Batch 1000, Loss: 1.9508, LR: 0.000016
2025-10-22 00:30:28,024 - INFO - Batch 1100, Loss: 1.9966, LR: 0.000016
2025-10-22 00:30:44,352 - INFO - Batch 1200, Loss: 2.1039, LR: 0.000016
2025-10-22 00:31:00,598 - INFO - Batch 1300, Loss: 1.8040, LR: 0.000016
2025-10-22 00:31:16,470 - INFO - Batch 1400, Loss: 2.3294, LR: 0.000016
2025-10-22 00:31:32,309 - INFO - Batch 1500, Loss: 2.1819, LR: 0.000016
2025-10-22 00:31:48,167 - INFO - Batch 1600, Loss: 2.0886, LR: 0.000016
2025-10-22 00:32:04,485 - INFO - Batch 1700, Loss: 2.0984, LR: 0.000016
2025-10-22 00:32:20,819 - INFO - Batch 1800, Loss: 2.2302, LR: 0.000016
2025-10-22 00:32:37,135 - INFO - Batch 1900, Loss: 2.1250, LR: 0.000016
2025-10-22 00:32:53,434 - INFO - Batch 2000, Loss: 2.1277, LR: 0.000016
2025-10-22 00:33:09,775 - INFO - Batch 2100, Loss: 2.2300, LR: 0.000016
2025-10-22 00:33:26,129 - INFO - Batch 2200, Loss: 1.8276, LR: 0.000016
2025-10-22 00:33:42,385 - INFO - Batch 2300, Loss: 1.8497, LR: 0.000015
2025-10-22 00:33:58,497 - INFO - Batch 2400, Loss: 2.1629, LR: 0.000015
2025-10-22 00:34:14,434 - INFO - Batch 2500, Loss: 2.2772, LR: 0.000015
2025-10-22 00:34:30,249 - INFO - Batch 2600, Loss: 1.7291, LR: 0.000015
2025-10-22 00:34:46,574 - INFO - Batch 2700, Loss: 1.9822, LR: 0.000015
2025-10-22 00:35:02,917 - INFO - Batch 2800, Loss: 2.4436, LR: 0.000015
2025-10-22 00:35:19,209 - INFO - Batch 2900, Loss: 2.0068, LR: 0.000015
2025-10-22 00:35:35,496 - INFO - Batch 3000, Loss: 2.1300, LR: 0.000015
2025-10-22 00:35:51,811 - INFO - Batch 3100, Loss: 1.9589, LR: 0.000015
2025-10-22 00:36:08,143 - INFO - Batch 3200, Loss: 2.2634, LR: 0.000015
2025-10-22 00:36:24,422 - INFO - Batch 3300, Loss: 2.1314, LR: 0.000015
2025-10-22 00:36:40,565 - INFO - Batch 3400, Loss: 2.2474, LR: 0.000015
2025-10-22 00:36:56,653 - INFO - Batch 3500, Loss: 2.1177, LR: 0.000015
2025-10-22 00:37:12,815 - INFO - Batch 3600, Loss: 2.0937, LR: 0.000015
2025-10-22 00:37:29,065 - INFO - Batch 3700, Loss: 2.0838, LR: 0.000015
2025-10-22 00:37:45,355 - INFO - Batch 3800, Loss: 2.0695, LR: 0.000015
2025-10-22 00:38:01,660 - INFO - Batch 3900, Loss: 2.1724, LR: 0.000015
2025-10-22 00:38:17,971 - INFO - Batch 4000, Loss: 2.2150, LR: 0.000014
2025-10-22 00:38:34,278 - INFO - Batch 4100, Loss: 2.5187, LR: 0.000014
2025-10-22 00:38:50,612 - INFO - Batch 4200, Loss: 2.1102, LR: 0.000014
2025-10-22 00:39:06,829 - INFO - Batch 4300, Loss: 2.1758, LR: 0.000014
2025-10-22 00:39:22,842 - INFO - Batch 4400, Loss: 2.1056, LR: 0.000014
2025-10-22 00:39:38,963 - INFO - Batch 4500, Loss: 2.1635, LR: 0.000014
2025-10-22 00:39:55,152 - INFO - Batch 4600, Loss: 2.2326, LR: 0.000014
2025-10-22 00:40:11,455 - INFO - Batch 4700, Loss: 2.1221, LR: 0.000014
2025-10-22 00:40:27,771 - INFO - Batch 4800, Loss: 1.9590, LR: 0.000014
2025-10-22 00:40:43,942 - INFO - Batch 4900, Loss: 2.1384, LR: 0.000014
2025-10-22 00:41:00,193 - INFO - Batch 5000, Loss: 2.0911, LR: 0.000014
2025-10-22 00:41:16,432 - INFO - Batch 5100, Loss: 2.0966, LR: 0.000014
2025-10-22 00:41:32,559 - INFO - Batch 5200, Loss: 2.2384, LR: 0.000014
2025-10-22 00:41:48,772 - INFO - Batch 5300, Loss: 2.0828, LR: 0.000014
2025-10-22 00:42:04,709 - INFO - Batch 5400, Loss: 2.1223, LR: 0.000014
2025-10-22 00:42:20,784 - INFO - Batch 5500, Loss: 2.1213, LR: 0.000014
2025-10-22 00:42:36,824 - INFO - Batch 5600, Loss: 1.9108, LR: 0.000014
2025-10-22 00:42:53,026 - INFO - Batch 5700, Loss: 2.0387, LR: 0.000014
2025-10-22 00:43:09,275 - INFO - Batch 5800, Loss: 2.1278, LR: 0.000013
2025-10-22 00:43:25,526 - INFO - Batch 5900, Loss: 2.1030, LR: 0.000013
2025-10-22 00:43:41,796 - INFO - Batch 6000, Loss: 2.0652, LR: 0.000013
2025-10-22 00:43:57,992 - INFO - Batch 6100, Loss: 2.2147, LR: 0.000013
2025-10-22 00:44:14,277 - INFO - Batch 6200, Loss: 2.2513, LR: 0.000013
2025-10-22 00:44:30,340 - INFO - Batch 6300, Loss: 2.0654, LR: 0.000013
2025-10-22 00:44:46,159 - INFO - Batch 6400, Loss: 2.0857, LR: 0.000013
2025-10-22 00:44:54,098 - INFO - Epoch 23/30: Train Loss: 2.1359, Val Loss: 2.0890, LR: 0.000013
2025-10-22 00:44:54,371 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-22 00:44:54,539 - INFO - Batch 0, Loss: 2.1808, LR: 0.000013
2025-10-22 00:45:10,484 - INFO - Batch 100, Loss: 1.9224, LR: 0.000013
2025-10-22 00:45:26,640 - INFO - Batch 200, Loss: 1.9990, LR: 0.000013
2025-10-22 00:45:42,974 - INFO - Batch 300, Loss: 1.9726, LR: 0.000013
2025-10-22 00:45:59,275 - INFO - Batch 400, Loss: 2.1704, LR: 0.000013
2025-10-22 00:46:15,578 - INFO - Batch 500, Loss: 1.9098, LR: 0.000013
2025-10-22 00:46:31,819 - INFO - Batch 600, Loss: 2.1245, LR: 0.000013
2025-10-22 00:46:47,788 - INFO - Batch 700, Loss: 1.9335, LR: 0.000013
2025-10-22 00:47:04,100 - INFO - Batch 800, Loss: 2.1797, LR: 0.000013
2025-10-22 00:47:20,210 - INFO - Batch 900, Loss: 2.0589, LR: 0.000013
2025-10-22 00:47:36,016 - INFO - Batch 1000, Loss: 1.8989, LR: 0.000013
2025-10-22 00:47:51,754 - INFO - Batch 1100, Loss: 2.2952, LR: 0.000012
2025-10-22 00:48:07,720 - INFO - Batch 1200, Loss: 2.1916, LR: 0.000012
2025-10-22 00:48:23,968 - INFO - Batch 1300, Loss: 2.1405, LR: 0.000012
2025-10-22 00:48:40,253 - INFO - Batch 1400, Loss: 2.0916, LR: 0.000012
2025-10-22 00:48:56,544 - INFO - Batch 1500, Loss: 2.0370, LR: 0.000012
2025-10-22 00:49:12,770 - INFO - Batch 1600, Loss: 1.9740, LR: 0.000012
2025-10-22 00:49:28,977 - INFO - Batch 1700, Loss: 1.9983, LR: 0.000012
2025-10-22 00:49:45,202 - INFO - Batch 1800, Loss: 2.0642, LR: 0.000012
2025-10-22 00:50:01,411 - INFO - Batch 1900, Loss: 2.0521, LR: 0.000012
2025-10-22 00:50:17,506 - INFO - Batch 2000, Loss: 2.0344, LR: 0.000012
2025-10-22 00:50:33,571 - INFO - Batch 2100, Loss: 2.2534, LR: 0.000012
2025-10-22 00:50:49,748 - INFO - Batch 2200, Loss: 2.2611, LR: 0.000012
2025-10-22 00:51:05,914 - INFO - Batch 2300, Loss: 2.2639, LR: 0.000012
2025-10-22 00:51:22,093 - INFO - Batch 2400, Loss: 2.0073, LR: 0.000012
2025-10-22 00:51:38,301 - INFO - Batch 2500, Loss: 1.9228, LR: 0.000012
2025-10-22 00:51:54,581 - INFO - Batch 2600, Loss: 2.1441, LR: 0.000012
2025-10-22 00:52:10,778 - INFO - Batch 2700, Loss: 2.0826, LR: 0.000012
2025-10-22 00:52:27,004 - INFO - Batch 2800, Loss: 2.0005, LR: 0.000012
2025-10-22 00:52:43,207 - INFO - Batch 2900, Loss: 1.9255, LR: 0.000012
2025-10-22 00:52:59,195 - INFO - Batch 3000, Loss: 2.2258, LR: 0.000011
2025-10-22 00:53:15,255 - INFO - Batch 3100, Loss: 2.2064, LR: 0.000011
2025-10-22 00:53:31,321 - INFO - Batch 3200, Loss: 2.4033, LR: 0.000011
2025-10-22 00:53:47,592 - INFO - Batch 3300, Loss: 2.1395, LR: 0.000011
2025-10-22 00:54:03,854 - INFO - Batch 3400, Loss: 1.8905, LR: 0.000011
2025-10-22 00:54:20,156 - INFO - Batch 3500, Loss: 2.3219, LR: 0.000011
2025-10-22 00:54:36,338 - INFO - Batch 3600, Loss: 1.6881, LR: 0.000011
2025-10-22 00:54:52,565 - INFO - Batch 3700, Loss: 2.1393, LR: 0.000011
2025-10-22 00:55:08,750 - INFO - Batch 3800, Loss: 2.2786, LR: 0.000011
2025-10-22 00:55:24,701 - INFO - Batch 3900, Loss: 2.1092, LR: 0.000011
2025-10-22 00:55:40,478 - INFO - Batch 4000, Loss: 1.9386, LR: 0.000011
2025-10-22 00:55:56,253 - INFO - Batch 4100, Loss: 2.2182, LR: 0.000011
2025-10-22 00:56:12,083 - INFO - Batch 4200, Loss: 2.0694, LR: 0.000011
2025-10-22 00:56:28,229 - INFO - Batch 4300, Loss: 2.2235, LR: 0.000011
2025-10-22 00:56:44,431 - INFO - Batch 4400, Loss: 1.9907, LR: 0.000011
2025-10-22 00:57:00,619 - INFO - Batch 4500, Loss: 2.2264, LR: 0.000011
2025-10-22 00:57:16,890 - INFO - Batch 4600, Loss: 2.1159, LR: 0.000011
2025-10-22 00:57:33,139 - INFO - Batch 4700, Loss: 1.9475, LR: 0.000011
2025-10-22 00:57:49,384 - INFO - Batch 4800, Loss: 2.0100, LR: 0.000011
2025-10-22 00:58:05,548 - INFO - Batch 4900, Loss: 2.0553, LR: 0.000011
2025-10-22 00:58:21,308 - INFO - Batch 5000, Loss: 1.9677, LR: 0.000010
2025-10-22 00:58:37,069 - INFO - Batch 5100, Loss: 2.0847, LR: 0.000010
2025-10-22 00:58:52,817 - INFO - Batch 5200, Loss: 2.0496, LR: 0.000010
2025-10-22 00:59:08,770 - INFO - Batch 5300, Loss: 2.1627, LR: 0.000010
2025-10-22 00:59:24,975 - INFO - Batch 5400, Loss: 2.0144, LR: 0.000010
2025-10-22 00:59:41,284 - INFO - Batch 5500, Loss: 2.0423, LR: 0.000010
2025-10-22 00:59:57,535 - INFO - Batch 5600, Loss: 2.0251, LR: 0.000010
2025-10-22 01:00:13,797 - INFO - Batch 5700, Loss: 2.2277, LR: 0.000010
2025-10-22 01:00:30,000 - INFO - Batch 5800, Loss: 2.2530, LR: 0.000010
2025-10-22 01:00:46,229 - INFO - Batch 5900, Loss: 2.3655, LR: 0.000010
2025-10-22 01:01:02,255 - INFO - Batch 6000, Loss: 2.0086, LR: 0.000010
2025-10-22 01:01:18,300 - INFO - Batch 6100, Loss: 2.4643, LR: 0.000010
2025-10-22 01:01:34,376 - INFO - Batch 6200, Loss: 2.1778, LR: 0.000010
2025-10-22 01:01:50,528 - INFO - Batch 6300, Loss: 2.0586, LR: 0.000010
2025-10-22 01:02:06,756 - INFO - Batch 6400, Loss: 2.0414, LR: 0.000010
2025-10-22 01:02:14,837 - INFO - Epoch 24/30: Train Loss: 2.1251, Val Loss: 2.0887, LR: 0.000010
2025-10-22 01:02:15,103 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-22 01:02:15,270 - INFO - Batch 0, Loss: 2.1644, LR: 0.000010
2025-10-22 01:02:31,397 - INFO - Batch 100, Loss: 2.0121, LR: 0.000010
2025-10-22 01:02:47,542 - INFO - Batch 200, Loss: 2.1090, LR: 0.000010
2025-10-22 01:03:03,745 - INFO - Batch 300, Loss: 2.2068, LR: 0.000010
2025-10-22 01:03:19,948 - INFO - Batch 400, Loss: 2.0891, LR: 0.000010
2025-10-22 01:03:35,969 - INFO - Batch 500, Loss: 2.2009, LR: 0.000010
2025-10-22 01:03:51,894 - INFO - Batch 600, Loss: 2.1313, LR: 0.000009
2025-10-22 01:04:07,561 - INFO - Batch 700, Loss: 2.2137, LR: 0.000009
2025-10-22 01:04:23,213 - INFO - Batch 800, Loss: 2.1744, LR: 0.000009
2025-10-22 01:04:39,287 - INFO - Batch 900, Loss: 2.0292, LR: 0.000009
2025-10-22 01:04:55,386 - INFO - Batch 1000, Loss: 2.1357, LR: 0.000009
2025-10-22 01:05:11,509 - INFO - Batch 1100, Loss: 2.0759, LR: 0.000009
2025-10-22 01:05:27,667 - INFO - Batch 1200, Loss: 2.2485, LR: 0.000009
2025-10-22 01:05:43,822 - INFO - Batch 1300, Loss: 2.1837, LR: 0.000009
2025-10-22 01:05:59,949 - INFO - Batch 1400, Loss: 1.9336, LR: 0.000009
2025-10-22 01:06:16,015 - INFO - Batch 1500, Loss: 2.1169, LR: 0.000009
2025-10-22 01:06:31,873 - INFO - Batch 1600, Loss: 2.2122, LR: 0.000009
2025-10-22 01:06:47,597 - INFO - Batch 1700, Loss: 2.1536, LR: 0.000009
2025-10-22 01:07:03,252 - INFO - Batch 1800, Loss: 2.3133, LR: 0.000009
2025-10-22 01:07:19,087 - INFO - Batch 1900, Loss: 2.3193, LR: 0.000009
2025-10-22 01:07:35,193 - INFO - Batch 2000, Loss: 2.2530, LR: 0.000009
2025-10-22 01:07:51,247 - INFO - Batch 2100, Loss: 1.9149, LR: 0.000009
2025-10-22 01:08:07,415 - INFO - Batch 2200, Loss: 2.1890, LR: 0.000009
2025-10-22 01:08:23,577 - INFO - Batch 2300, Loss: 2.4618, LR: 0.000009
2025-10-22 01:08:39,726 - INFO - Batch 2400, Loss: 2.1512, LR: 0.000009
2025-10-22 01:08:55,885 - INFO - Batch 2500, Loss: 2.2784, LR: 0.000009
2025-10-22 01:09:12,005 - INFO - Batch 2600, Loss: 1.8882, LR: 0.000009
2025-10-22 01:09:28,019 - INFO - Batch 2700, Loss: 2.2655, LR: 0.000008
2025-10-22 01:09:44,013 - INFO - Batch 2800, Loss: 2.3073, LR: 0.000008
2025-10-22 01:10:00,083 - INFO - Batch 2900, Loss: 2.0160, LR: 0.000008
2025-10-22 01:10:16,258 - INFO - Batch 3000, Loss: 2.1306, LR: 0.000008
2025-10-22 01:10:32,472 - INFO - Batch 3100, Loss: 1.9639, LR: 0.000008
2025-10-22 01:10:48,587 - INFO - Batch 3200, Loss: 2.2611, LR: 0.000008
2025-10-22 01:11:04,749 - INFO - Batch 3300, Loss: 1.9105, LR: 0.000008
2025-10-22 01:11:20,813 - INFO - Batch 3400, Loss: 2.1816, LR: 0.000008
2025-10-22 01:11:36,871 - INFO - Batch 3500, Loss: 2.2996, LR: 0.000008
2025-10-22 01:11:52,799 - INFO - Batch 3600, Loss: 2.1823, LR: 0.000008
2025-10-22 01:12:08,617 - INFO - Batch 3700, Loss: 2.1710, LR: 0.000008
2025-10-22 01:12:24,405 - INFO - Batch 3800, Loss: 1.9555, LR: 0.000008
2025-10-22 01:12:40,286 - INFO - Batch 3900, Loss: 2.3462, LR: 0.000008
2025-10-22 01:12:56,306 - INFO - Batch 4000, Loss: 2.0608, LR: 0.000008
2025-10-22 01:13:12,440 - INFO - Batch 4100, Loss: 2.1225, LR: 0.000008
2025-10-22 01:13:28,309 - INFO - Batch 4200, Loss: 2.1553, LR: 0.000008
2025-10-22 01:13:44,491 - INFO - Batch 4300, Loss: 2.1904, LR: 0.000008
2025-10-22 01:14:00,746 - INFO - Batch 4400, Loss: 1.9744, LR: 0.000008
2025-10-22 01:14:16,946 - INFO - Batch 4500, Loss: 2.1195, LR: 0.000008
2025-10-22 01:14:32,950 - INFO - Batch 4600, Loss: 2.0719, LR: 0.000008
2025-10-22 01:14:48,823 - INFO - Batch 4700, Loss: 2.5066, LR: 0.000008
2025-10-22 01:15:04,586 - INFO - Batch 4800, Loss: 2.5554, LR: 0.000008
2025-10-22 01:15:20,316 - INFO - Batch 4900, Loss: 2.1401, LR: 0.000007
2025-10-22 01:15:36,348 - INFO - Batch 5000, Loss: 2.1581, LR: 0.000007
2025-10-22 01:15:52,394 - INFO - Batch 5100, Loss: 2.2412, LR: 0.000007
2025-10-22 01:16:08,480 - INFO - Batch 5200, Loss: 2.2787, LR: 0.000007
2025-10-22 01:16:24,708 - INFO - Batch 5300, Loss: 2.4043, LR: 0.000007
2025-10-22 01:16:40,824 - INFO - Batch 5400, Loss: 2.0268, LR: 0.000007
2025-10-22 01:16:57,007 - INFO - Batch 5500, Loss: 1.8710, LR: 0.000007
2025-10-22 01:17:13,129 - INFO - Batch 5600, Loss: 2.0726, LR: 0.000007
2025-10-22 01:17:28,868 - INFO - Batch 5700, Loss: 1.9280, LR: 0.000007
2025-10-22 01:17:44,615 - INFO - Batch 5800, Loss: 2.3050, LR: 0.000007
2025-10-22 01:18:00,457 - INFO - Batch 5900, Loss: 2.0751, LR: 0.000007
2025-10-22 01:18:16,554 - INFO - Batch 6000, Loss: 2.0305, LR: 0.000007
2025-10-22 01:18:32,777 - INFO - Batch 6100, Loss: 2.1700, LR: 0.000007
2025-10-22 01:18:49,008 - INFO - Batch 6200, Loss: 2.2633, LR: 0.000007
2025-10-22 01:19:05,242 - INFO - Batch 6300, Loss: 1.9377, LR: 0.000007
2025-10-22 01:19:21,461 - INFO - Batch 6400, Loss: 2.1265, LR: 0.000007
2025-10-22 01:19:29,519 - INFO - Epoch 25/30: Train Loss: 2.1154, Val Loss: 2.0806, LR: 0.000007
2025-10-22 01:19:29,782 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-22 01:19:29,949 - INFO - Batch 0, Loss: 2.0755, LR: 0.000007
2025-10-22 01:19:45,990 - INFO - Batch 100, Loss: 2.1175, LR: 0.000007
2025-10-22 01:20:02,029 - INFO - Batch 200, Loss: 2.0502, LR: 0.000007
2025-10-22 01:20:17,814 - INFO - Batch 300, Loss: 1.8430, LR: 0.000007
2025-10-22 01:20:33,607 - INFO - Batch 400, Loss: 2.2706, LR: 0.000007
2025-10-22 01:20:49,504 - INFO - Batch 500, Loss: 1.9236, LR: 0.000007
2025-10-22 01:21:05,846 - INFO - Batch 600, Loss: 2.2835, LR: 0.000007
2025-10-22 01:21:22,161 - INFO - Batch 700, Loss: 2.0803, LR: 0.000007
2025-10-22 01:21:38,458 - INFO - Batch 800, Loss: 1.9519, LR: 0.000007
2025-10-22 01:21:54,690 - INFO - Batch 900, Loss: 1.9631, LR: 0.000006
2025-10-22 01:22:10,964 - INFO - Batch 1000, Loss: 2.0301, LR: 0.000006
2025-10-22 01:22:27,271 - INFO - Batch 1100, Loss: 2.3138, LR: 0.000006
2025-10-22 01:22:43,526 - INFO - Batch 1200, Loss: 1.9881, LR: 0.000006
2025-10-22 01:22:59,443 - INFO - Batch 1300, Loss: 1.9530, LR: 0.000006
2025-10-22 01:23:15,378 - INFO - Batch 1400, Loss: 2.2963, LR: 0.000006
2025-10-22 01:23:31,287 - INFO - Batch 1500, Loss: 2.2168, LR: 0.000006
2025-10-22 01:23:47,578 - INFO - Batch 1600, Loss: 2.0674, LR: 0.000006
2025-10-22 01:24:03,821 - INFO - Batch 1700, Loss: 2.1704, LR: 0.000006
2025-10-22 01:24:20,008 - INFO - Batch 1800, Loss: 1.9433, LR: 0.000006
2025-10-22 01:24:36,356 - INFO - Batch 1900, Loss: 1.8983, LR: 0.000006
2025-10-22 01:24:52,675 - INFO - Batch 2000, Loss: 2.2528, LR: 0.000006
2025-10-22 01:25:09,009 - INFO - Batch 2100, Loss: 2.0523, LR: 0.000006
2025-10-22 01:25:25,314 - INFO - Batch 2200, Loss: 1.8992, LR: 0.000006
2025-10-22 01:25:41,263 - INFO - Batch 2300, Loss: 2.2122, LR: 0.000006
2025-10-22 01:25:57,195 - INFO - Batch 2400, Loss: 2.0298, LR: 0.000006
2025-10-22 01:26:13,106 - INFO - Batch 2500, Loss: 2.2796, LR: 0.000006
2025-10-22 01:26:29,342 - INFO - Batch 2600, Loss: 2.1749, LR: 0.000006
2025-10-22 01:26:45,619 - INFO - Batch 2700, Loss: 1.9093, LR: 0.000006
2025-10-22 01:27:01,836 - INFO - Batch 2800, Loss: 2.1030, LR: 0.000006
2025-10-22 01:27:18,144 - INFO - Batch 2900, Loss: 2.3233, LR: 0.000006
2025-10-22 01:27:34,312 - INFO - Batch 3000, Loss: 1.9284, LR: 0.000006
2025-10-22 01:27:50,589 - INFO - Batch 3100, Loss: 2.1850, LR: 0.000006
2025-10-22 01:28:06,915 - INFO - Batch 3200, Loss: 2.2959, LR: 0.000006
2025-10-22 01:28:22,875 - INFO - Batch 3300, Loss: 2.1524, LR: 0.000006
2025-10-22 01:28:38,826 - INFO - Batch 3400, Loss: 1.9182, LR: 0.000005
2025-10-22 01:28:54,754 - INFO - Batch 3500, Loss: 2.2634, LR: 0.000005
2025-10-22 01:29:11,028 - INFO - Batch 3600, Loss: 2.1758, LR: 0.000005
2025-10-22 01:29:27,351 - INFO - Batch 3700, Loss: 2.2477, LR: 0.000005
2025-10-22 01:29:43,672 - INFO - Batch 3800, Loss: 2.0602, LR: 0.000005
2025-10-22 01:29:59,971 - INFO - Batch 3900, Loss: 1.9813, LR: 0.000005
2025-10-22 01:30:16,302 - INFO - Batch 4000, Loss: 2.0753, LR: 0.000005
2025-10-22 01:30:32,571 - INFO - Batch 4100, Loss: 2.1025, LR: 0.000005
2025-10-22 01:30:48,867 - INFO - Batch 4200, Loss: 2.1563, LR: 0.000005
2025-10-22 01:31:04,837 - INFO - Batch 4300, Loss: 2.2252, LR: 0.000005
2025-10-22 01:31:20,770 - INFO - Batch 4400, Loss: 2.1414, LR: 0.000005
2025-10-22 01:31:36,695 - INFO - Batch 4500, Loss: 1.9901, LR: 0.000005
2025-10-22 01:31:52,954 - INFO - Batch 4600, Loss: 2.0287, LR: 0.000005
2025-10-22 01:32:09,261 - INFO - Batch 4700, Loss: 1.8219, LR: 0.000005
2025-10-22 01:32:25,561 - INFO - Batch 4800, Loss: 2.2121, LR: 0.000005
2025-10-22 01:32:41,870 - INFO - Batch 4900, Loss: 2.1687, LR: 0.000005
2025-10-22 01:32:58,207 - INFO - Batch 5000, Loss: 2.1982, LR: 0.000005
2025-10-22 01:33:14,501 - INFO - Batch 5100, Loss: 2.0063, LR: 0.000005
2025-10-22 01:33:30,812 - INFO - Batch 5200, Loss: 2.0625, LR: 0.000005
2025-10-22 01:33:46,821 - INFO - Batch 5300, Loss: 2.3821, LR: 0.000005
2025-10-22 01:34:02,753 - INFO - Batch 5400, Loss: 2.2743, LR: 0.000005
2025-10-22 01:34:18,662 - INFO - Batch 5500, Loss: 2.1784, LR: 0.000005
2025-10-22 01:34:34,894 - INFO - Batch 5600, Loss: 1.8632, LR: 0.000005
2025-10-22 01:34:51,207 - INFO - Batch 5700, Loss: 2.0384, LR: 0.000005
2025-10-22 01:35:07,514 - INFO - Batch 5800, Loss: 2.0748, LR: 0.000005
2025-10-22 01:35:23,813 - INFO - Batch 5900, Loss: 2.0212, LR: 0.000005
2025-10-22 01:35:40,123 - INFO - Batch 6000, Loss: 2.2198, LR: 0.000005
2025-10-22 01:35:56,364 - INFO - Batch 6100, Loss: 1.7606, LR: 0.000005
2025-10-22 01:36:12,649 - INFO - Batch 6200, Loss: 2.1095, LR: 0.000004
2025-10-22 01:36:28,628 - INFO - Batch 6300, Loss: 2.0165, LR: 0.000004
2025-10-22 01:36:44,559 - INFO - Batch 6400, Loss: 2.2353, LR: 0.000004
2025-10-22 01:36:52,496 - INFO - Epoch 26/30: Train Loss: 2.1071, Val Loss: 2.0808, LR: 0.000004
2025-10-22 01:36:52,662 - INFO - Batch 0, Loss: 1.8578, LR: 0.000004
2025-10-22 01:37:08,664 - INFO - Batch 100, Loss: 2.0500, LR: 0.000004
2025-10-22 01:37:24,933 - INFO - Batch 200, Loss: 2.1236, LR: 0.000004
2025-10-22 01:37:41,226 - INFO - Batch 300, Loss: 1.9230, LR: 0.000004
2025-10-22 01:37:57,539 - INFO - Batch 400, Loss: 2.3338, LR: 0.000004
2025-10-22 01:38:13,845 - INFO - Batch 500, Loss: 2.2078, LR: 0.000004
2025-10-22 01:38:30,150 - INFO - Batch 600, Loss: 2.0871, LR: 0.000004
2025-10-22 01:38:46,465 - INFO - Batch 700, Loss: 1.9790, LR: 0.000004
2025-10-22 01:39:02,736 - INFO - Batch 800, Loss: 2.2444, LR: 0.000004
2025-10-22 01:39:18,870 - INFO - Batch 900, Loss: 2.0640, LR: 0.000004
2025-10-22 01:39:34,806 - INFO - Batch 1000, Loss: 2.0972, LR: 0.000004
2025-10-22 01:39:50,892 - INFO - Batch 1100, Loss: 2.0344, LR: 0.000004
2025-10-22 01:40:07,232 - INFO - Batch 1200, Loss: 1.7183, LR: 0.000004
2025-10-22 01:40:23,491 - INFO - Batch 1300, Loss: 2.2590, LR: 0.000004
2025-10-22 01:40:39,729 - INFO - Batch 1400, Loss: 1.9912, LR: 0.000004
2025-10-22 01:40:56,043 - INFO - Batch 1500, Loss: 2.1564, LR: 0.000004
2025-10-22 01:41:12,326 - INFO - Batch 1600, Loss: 2.2332, LR: 0.000004
2025-10-22 01:41:28,650 - INFO - Batch 1700, Loss: 2.3906, LR: 0.000004
2025-10-22 01:41:44,778 - INFO - Batch 1800, Loss: 2.0600, LR: 0.000004
2025-10-22 01:42:00,726 - INFO - Batch 1900, Loss: 2.3225, LR: 0.000004
2025-10-22 01:42:16,706 - INFO - Batch 2000, Loss: 1.9300, LR: 0.000004
2025-10-22 01:42:32,739 - INFO - Batch 2100, Loss: 2.1179, LR: 0.000004
2025-10-22 01:42:49,022 - INFO - Batch 2200, Loss: 2.1298, LR: 0.000004
2025-10-22 01:43:05,346 - INFO - Batch 2300, Loss: 2.1324, LR: 0.000004
2025-10-22 01:43:21,663 - INFO - Batch 2400, Loss: 2.1746, LR: 0.000004
2025-10-22 01:43:37,957 - INFO - Batch 2500, Loss: 2.3388, LR: 0.000004
2025-10-22 01:43:54,261 - INFO - Batch 2600, Loss: 1.8761, LR: 0.000004
2025-10-22 01:44:10,570 - INFO - Batch 2700, Loss: 1.9768, LR: 0.000004
2025-10-22 01:44:26,773 - INFO - Batch 2800, Loss: 2.1267, LR: 0.000004
2025-10-22 01:44:42,674 - INFO - Batch 2900, Loss: 1.9537, LR: 0.000003
2025-10-22 01:44:58,584 - INFO - Batch 3000, Loss: 2.3826, LR: 0.000003
2025-10-22 01:45:14,524 - INFO - Batch 3100, Loss: 2.4172, LR: 0.000003
2025-10-22 01:45:30,781 - INFO - Batch 3200, Loss: 2.1965, LR: 0.000003
2025-10-22 01:45:47,118 - INFO - Batch 3300, Loss: 1.9351, LR: 0.000003
2025-10-22 01:46:03,399 - INFO - Batch 3400, Loss: 2.0700, LR: 0.000003
2025-10-22 01:46:19,656 - INFO - Batch 3500, Loss: 2.0584, LR: 0.000003
2025-10-22 01:46:35,892 - INFO - Batch 3600, Loss: 2.2457, LR: 0.000003
2025-10-22 01:46:52,067 - INFO - Batch 3700, Loss: 2.0590, LR: 0.000003
2025-10-22 01:47:08,307 - INFO - Batch 3800, Loss: 2.3136, LR: 0.000003
2025-10-22 01:47:24,271 - INFO - Batch 3900, Loss: 2.2871, LR: 0.000003
2025-10-22 01:47:40,200 - INFO - Batch 4000, Loss: 2.2117, LR: 0.000003
2025-10-22 01:47:56,115 - INFO - Batch 4100, Loss: 2.2213, LR: 0.000003
2025-10-22 01:48:12,435 - INFO - Batch 4200, Loss: 2.1795, LR: 0.000003
2025-10-22 01:48:28,752 - INFO - Batch 4300, Loss: 2.1348, LR: 0.000003
2025-10-22 01:48:45,078 - INFO - Batch 4400, Loss: 2.1165, LR: 0.000003
2025-10-22 01:49:01,418 - INFO - Batch 4500, Loss: 2.0943, LR: 0.000003
2025-10-22 01:49:17,736 - INFO - Batch 4600, Loss: 2.0983, LR: 0.000003
2025-10-22 01:49:34,042 - INFO - Batch 4700, Loss: 2.1006, LR: 0.000003
2025-10-22 01:49:50,303 - INFO - Batch 4800, Loss: 2.2410, LR: 0.000003
2025-10-22 01:50:06,253 - INFO - Batch 4900, Loss: 2.0006, LR: 0.000003
2025-10-22 01:50:22,163 - INFO - Batch 5000, Loss: 1.8892, LR: 0.000003
2025-10-22 01:50:38,090 - INFO - Batch 5100, Loss: 1.8932, LR: 0.000003
2025-10-22 01:50:54,391 - INFO - Batch 5200, Loss: 2.2237, LR: 0.000003
2025-10-22 01:51:10,687 - INFO - Batch 5300, Loss: 2.2459, LR: 0.000003
2025-10-22 01:51:26,998 - INFO - Batch 5400, Loss: 1.9050, LR: 0.000003
2025-10-22 01:51:43,286 - INFO - Batch 5500, Loss: 2.0056, LR: 0.000003
2025-10-22 01:51:59,605 - INFO - Batch 5600, Loss: 2.2074, LR: 0.000003
2025-10-22 01:52:15,922 - INFO - Batch 5700, Loss: 2.1159, LR: 0.000003
2025-10-22 01:52:32,131 - INFO - Batch 5800, Loss: 2.1530, LR: 0.000003
2025-10-22 01:52:48,040 - INFO - Batch 5900, Loss: 2.2627, LR: 0.000003
2025-10-22 01:53:03,954 - INFO - Batch 6000, Loss: 1.9804, LR: 0.000003
2025-10-22 01:53:19,875 - INFO - Batch 6100, Loss: 2.0462, LR: 0.000003
2025-10-22 01:53:36,168 - INFO - Batch 6200, Loss: 2.1811, LR: 0.000003
2025-10-22 01:53:52,468 - INFO - Batch 6300, Loss: 2.2446, LR: 0.000003
2025-10-22 01:54:08,778 - INFO - Batch 6400, Loss: 2.0616, LR: 0.000003
2025-10-22 01:54:16,883 - INFO - Epoch 27/30: Train Loss: 2.0994, Val Loss: 2.0759, LR: 0.000002
2025-10-22 01:54:17,145 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-22 01:54:17,314 - INFO - Batch 0, Loss: 1.9166, LR: 0.000002
2025-10-22 01:54:33,507 - INFO - Batch 100, Loss: 2.2270, LR: 0.000002
2025-10-22 01:54:49,784 - INFO - Batch 200, Loss: 2.1400, LR: 0.000002
2025-10-22 01:55:06,078 - INFO - Batch 300, Loss: 2.1823, LR: 0.000002
2025-10-22 01:55:22,163 - INFO - Batch 400, Loss: 2.1119, LR: 0.000002
2025-10-22 01:55:38,103 - INFO - Batch 500, Loss: 2.1321, LR: 0.000002
2025-10-22 01:55:54,039 - INFO - Batch 600, Loss: 2.1272, LR: 0.000002
2025-10-22 01:56:10,187 - INFO - Batch 700, Loss: 2.0167, LR: 0.000002
2025-10-22 01:56:26,520 - INFO - Batch 800, Loss: 2.1263, LR: 0.000002
2025-10-22 01:56:42,818 - INFO - Batch 900, Loss: 2.0902, LR: 0.000002
2025-10-22 01:56:59,145 - INFO - Batch 1000, Loss: 2.2317, LR: 0.000002
2025-10-22 01:57:15,475 - INFO - Batch 1100, Loss: 2.1648, LR: 0.000002
2025-10-22 01:57:31,786 - INFO - Batch 1200, Loss: 2.1388, LR: 0.000002
2025-10-22 01:57:48,087 - INFO - Batch 1300, Loss: 2.2693, LR: 0.000002
2025-10-22 01:58:04,237 - INFO - Batch 1400, Loss: 1.8808, LR: 0.000002
2025-10-22 01:58:20,263 - INFO - Batch 1500, Loss: 2.0744, LR: 0.000002
2025-10-22 01:58:36,314 - INFO - Batch 1600, Loss: 1.9383, LR: 0.000002
2025-10-22 01:58:52,446 - INFO - Batch 1700, Loss: 2.1298, LR: 0.000002
2025-10-22 01:59:08,715 - INFO - Batch 1800, Loss: 2.0194, LR: 0.000002
2025-10-22 01:59:25,049 - INFO - Batch 1900, Loss: 1.8014, LR: 0.000002
2025-10-22 01:59:41,390 - INFO - Batch 2000, Loss: 2.0683, LR: 0.000002
2025-10-22 01:59:57,657 - INFO - Batch 2100, Loss: 1.8759, LR: 0.000002
2025-10-22 02:00:13,967 - INFO - Batch 2200, Loss: 2.1768, LR: 0.000002
2025-10-22 02:00:30,229 - INFO - Batch 2300, Loss: 2.2034, LR: 0.000002
2025-10-22 02:00:46,329 - INFO - Batch 2400, Loss: 2.1674, LR: 0.000002
2025-10-22 02:01:02,258 - INFO - Batch 2500, Loss: 2.0670, LR: 0.000002
2025-10-22 02:01:18,206 - INFO - Batch 2600, Loss: 1.8362, LR: 0.000002
2025-10-22 02:01:34,259 - INFO - Batch 2700, Loss: 1.9203, LR: 0.000002
2025-10-22 02:01:50,421 - INFO - Batch 2800, Loss: 2.1638, LR: 0.000002
2025-10-22 02:02:06,745 - INFO - Batch 2900, Loss: 1.9269, LR: 0.000002
2025-10-22 02:02:23,036 - INFO - Batch 3000, Loss: 2.1404, LR: 0.000002
2025-10-22 02:02:39,335 - INFO - Batch 3100, Loss: 2.0103, LR: 0.000002
2025-10-22 02:02:55,615 - INFO - Batch 3200, Loss: 1.8312, LR: 0.000002
2025-10-22 02:03:11,918 - INFO - Batch 3300, Loss: 2.0041, LR: 0.000002
2025-10-22 02:03:28,037 - INFO - Batch 3400, Loss: 2.0080, LR: 0.000002
2025-10-22 02:03:43,973 - INFO - Batch 3500, Loss: 1.9084, LR: 0.000002
2025-10-22 02:03:59,906 - INFO - Batch 3600, Loss: 2.3120, LR: 0.000002
2025-10-22 02:04:15,968 - INFO - Batch 3700, Loss: 1.9611, LR: 0.000002
2025-10-22 02:04:32,275 - INFO - Batch 3800, Loss: 2.0069, LR: 0.000002
2025-10-22 02:04:48,561 - INFO - Batch 3900, Loss: 2.2545, LR: 0.000002
2025-10-22 02:05:04,794 - INFO - Batch 4000, Loss: 2.1436, LR: 0.000002
2025-10-22 02:05:20,730 - INFO - Batch 4100, Loss: 2.0210, LR: 0.000002
2025-10-22 02:05:36,681 - INFO - Batch 4200, Loss: 1.9220, LR: 0.000002
2025-10-22 02:05:52,690 - INFO - Batch 4300, Loss: 2.1052, LR: 0.000002
2025-10-22 02:06:08,826 - INFO - Batch 4400, Loss: 1.8028, LR: 0.000001
2025-10-22 02:06:24,737 - INFO - Batch 4500, Loss: 1.8452, LR: 0.000001
2025-10-22 02:06:40,650 - INFO - Batch 4600, Loss: 1.9424, LR: 0.000001
2025-10-22 02:06:56,646 - INFO - Batch 4700, Loss: 2.1991, LR: 0.000001
2025-10-22 02:07:12,961 - INFO - Batch 4800, Loss: 2.0294, LR: 0.000001
2025-10-22 02:07:29,239 - INFO - Batch 4900, Loss: 2.1027, LR: 0.000001
2025-10-22 02:07:45,565 - INFO - Batch 5000, Loss: 2.4128, LR: 0.000001
2025-10-22 02:08:01,871 - INFO - Batch 5100, Loss: 2.3708, LR: 0.000001
2025-10-22 02:08:18,149 - INFO - Batch 5200, Loss: 2.4456, LR: 0.000001
2025-10-22 02:08:34,428 - INFO - Batch 5300, Loss: 2.4334, LR: 0.000001
2025-10-22 02:08:50,617 - INFO - Batch 5400, Loss: 1.9777, LR: 0.000001
2025-10-22 02:09:06,572 - INFO - Batch 5500, Loss: 2.0028, LR: 0.000001
2025-10-22 02:09:22,521 - INFO - Batch 5600, Loss: 2.1344, LR: 0.000001
2025-10-22 02:09:38,538 - INFO - Batch 5700, Loss: 2.0942, LR: 0.000001
2025-10-22 02:09:54,853 - INFO - Batch 5800, Loss: 1.9215, LR: 0.000001
2025-10-22 02:10:11,139 - INFO - Batch 5900, Loss: 1.9073, LR: 0.000001
2025-10-22 02:10:27,421 - INFO - Batch 6000, Loss: 2.1249, LR: 0.000001
2025-10-22 02:10:43,708 - INFO - Batch 6100, Loss: 2.1891, LR: 0.000001
2025-10-22 02:11:00,006 - INFO - Batch 6200, Loss: 2.2008, LR: 0.000001
2025-10-22 02:11:16,264 - INFO - Batch 6300, Loss: 1.9974, LR: 0.000001
2025-10-22 02:11:32,417 - INFO - Batch 6400, Loss: 2.3079, LR: 0.000001
2025-10-22 02:11:40,349 - INFO - Epoch 28/30: Train Loss: 2.0944, Val Loss: 2.0751, LR: 0.000001
2025-10-22 02:11:40,949 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-22 02:11:41,116 - INFO - Batch 0, Loss: 1.8864, LR: 0.000001
2025-10-22 02:11:56,921 - INFO - Batch 100, Loss: 2.1695, LR: 0.000001
2025-10-22 02:12:12,859 - INFO - Batch 200, Loss: 2.0550, LR: 0.000001
2025-10-22 02:12:29,118 - INFO - Batch 300, Loss: 1.8510, LR: 0.000001
2025-10-22 02:12:45,375 - INFO - Batch 400, Loss: 2.1750, LR: 0.000001
2025-10-22 02:13:01,623 - INFO - Batch 500, Loss: 1.9967, LR: 0.000001
2025-10-22 02:13:17,892 - INFO - Batch 600, Loss: 2.0357, LR: 0.000001
2025-10-22 02:13:34,226 - INFO - Batch 700, Loss: 2.0813, LR: 0.000001
2025-10-22 02:13:50,443 - INFO - Batch 800, Loss: 2.0207, LR: 0.000001
2025-10-22 02:14:06,638 - INFO - Batch 900, Loss: 2.0100, LR: 0.000001
2025-10-22 02:14:22,547 - INFO - Batch 1000, Loss: 2.1282, LR: 0.000001
2025-10-22 02:14:38,459 - INFO - Batch 1100, Loss: 2.0466, LR: 0.000001
2025-10-22 02:14:54,360 - INFO - Batch 1200, Loss: 2.1252, LR: 0.000001
2025-10-22 02:15:10,637 - INFO - Batch 1300, Loss: 2.2287, LR: 0.000001
2025-10-22 02:15:26,960 - INFO - Batch 1400, Loss: 2.2517, LR: 0.000001
2025-10-22 02:15:43,100 - INFO - Batch 1500, Loss: 1.9957, LR: 0.000001
2025-10-22 02:15:59,266 - INFO - Batch 1600, Loss: 1.8885, LR: 0.000001
2025-10-22 02:16:15,242 - INFO - Batch 1700, Loss: 2.1821, LR: 0.000001
2025-10-22 02:16:31,197 - INFO - Batch 1800, Loss: 1.9957, LR: 0.000001
2025-10-22 02:16:47,183 - INFO - Batch 1900, Loss: 2.2401, LR: 0.000001
2025-10-22 02:17:02,976 - INFO - Batch 2000, Loss: 2.0312, LR: 0.000001
2025-10-22 02:17:18,839 - INFO - Batch 2100, Loss: 2.1223, LR: 0.000001
2025-10-22 02:17:34,653 - INFO - Batch 2200, Loss: 2.1131, LR: 0.000001
2025-10-22 02:17:50,539 - INFO - Batch 2300, Loss: 2.3448, LR: 0.000001
2025-10-22 02:18:06,514 - INFO - Batch 2400, Loss: 1.8738, LR: 0.000001
2025-10-22 02:18:22,525 - INFO - Batch 2500, Loss: 2.1983, LR: 0.000001
2025-10-22 02:18:38,625 - INFO - Batch 2600, Loss: 2.0524, LR: 0.000001
2025-10-22 02:18:54,693 - INFO - Batch 2700, Loss: 2.0050, LR: 0.000001
2025-10-22 02:19:10,667 - INFO - Batch 2800, Loss: 1.9200, LR: 0.000001
2025-10-22 02:19:26,626 - INFO - Batch 2900, Loss: 1.9000, LR: 0.000001
2025-10-22 02:19:42,515 - INFO - Batch 3000, Loss: 2.2370, LR: 0.000001
2025-10-22 02:19:58,278 - INFO - Batch 3100, Loss: 2.1116, LR: 0.000001
2025-10-22 02:20:14,010 - INFO - Batch 3200, Loss: 1.9714, LR: 0.000001
2025-10-22 02:20:29,769 - INFO - Batch 3300, Loss: 2.0098, LR: 0.000001
2025-10-22 02:20:45,719 - INFO - Batch 3400, Loss: 1.8144, LR: 0.000001
2025-10-22 02:21:01,676 - INFO - Batch 3500, Loss: 2.0004, LR: 0.000001
2025-10-22 02:21:17,668 - INFO - Batch 3600, Loss: 2.0406, LR: 0.000001
2025-10-22 02:21:33,643 - INFO - Batch 3700, Loss: 1.8318, LR: 0.000001
2025-10-22 02:21:49,673 - INFO - Batch 3800, Loss: 2.1854, LR: 0.000001
2025-10-22 02:22:05,945 - INFO - Batch 3900, Loss: 1.9926, LR: 0.000001
2025-10-22 02:22:22,113 - INFO - Batch 4000, Loss: 2.0845, LR: 0.000001
2025-10-22 02:22:37,981 - INFO - Batch 4100, Loss: 2.2167, LR: 0.000001
2025-10-22 02:22:53,804 - INFO - Batch 4200, Loss: 2.1343, LR: 0.000001
2025-10-22 02:23:09,654 - INFO - Batch 4300, Loss: 2.0742, LR: 0.000000
2025-10-22 02:23:25,589 - INFO - Batch 4400, Loss: 2.0629, LR: 0.000000
2025-10-22 02:23:41,612 - INFO - Batch 4500, Loss: 2.2472, LR: 0.000000
2025-10-22 02:23:57,597 - INFO - Batch 4600, Loss: 2.0381, LR: 0.000000
2025-10-22 02:24:13,577 - INFO - Batch 4700, Loss: 2.0415, LR: 0.000000
2025-10-22 02:24:29,572 - INFO - Batch 4800, Loss: 2.1374, LR: 0.000000
2025-10-22 02:24:45,692 - INFO - Batch 4900, Loss: 1.9453, LR: 0.000000
2025-10-22 02:25:01,797 - INFO - Batch 5000, Loss: 2.0934, LR: 0.000000
2025-10-22 02:25:17,746 - INFO - Batch 5100, Loss: 2.3067, LR: 0.000000
2025-10-22 02:25:33,499 - INFO - Batch 5200, Loss: 1.8688, LR: 0.000000
2025-10-22 02:25:49,277 - INFO - Batch 5300, Loss: 2.0142, LR: 0.000000
2025-10-22 02:26:05,062 - INFO - Batch 5400, Loss: 2.2485, LR: 0.000000
2025-10-22 02:26:21,032 - INFO - Batch 5500, Loss: 2.1903, LR: 0.000000
2025-10-22 02:26:37,028 - INFO - Batch 5600, Loss: 1.8350, LR: 0.000000
2025-10-22 02:26:53,020 - INFO - Batch 5700, Loss: 2.0706, LR: 0.000000
2025-10-22 02:27:09,044 - INFO - Batch 5800, Loss: 2.1888, LR: 0.000000
2025-10-22 02:27:25,130 - INFO - Batch 5900, Loss: 2.1498, LR: 0.000000
2025-10-22 02:27:41,068 - INFO - Batch 6000, Loss: 1.9178, LR: 0.000000
2025-10-22 02:27:57,044 - INFO - Batch 6100, Loss: 2.0611, LR: 0.000000
2025-10-22 02:28:13,190 - INFO - Batch 6200, Loss: 2.0877, LR: 0.000000
2025-10-22 02:28:29,126 - INFO - Batch 6300, Loss: 2.0851, LR: 0.000000
2025-10-22 02:28:44,908 - INFO - Batch 6400, Loss: 2.2036, LR: 0.000000
2025-10-22 02:28:52,790 - INFO - Epoch 29/30: Train Loss: 2.0904, Val Loss: 2.0754, LR: 0.000000
2025-10-22 02:28:52,954 - INFO - Batch 0, Loss: 2.0662, LR: 0.000000
2025-10-22 02:29:08,920 - INFO - Batch 100, Loss: 1.8683, LR: 0.000000
2025-10-22 02:29:24,880 - INFO - Batch 200, Loss: 2.2632, LR: 0.000000
2025-10-22 02:29:40,948 - INFO - Batch 300, Loss: 2.1256, LR: 0.000000
2025-10-22 02:29:57,253 - INFO - Batch 400, Loss: 2.3125, LR: 0.000000
2025-10-22 02:30:13,400 - INFO - Batch 500, Loss: 2.0791, LR: 0.000000
2025-10-22 02:30:29,499 - INFO - Batch 600, Loss: 2.2698, LR: 0.000000
2025-10-22 02:30:45,801 - INFO - Batch 700, Loss: 1.9506, LR: 0.000000
2025-10-22 02:31:01,765 - INFO - Batch 800, Loss: 2.0267, LR: 0.000000
2025-10-22 02:31:17,548 - INFO - Batch 900, Loss: 2.1749, LR: 0.000000
2025-10-22 02:31:33,386 - INFO - Batch 1000, Loss: 2.0859, LR: 0.000000
2025-10-22 02:31:49,331 - INFO - Batch 1100, Loss: 2.0270, LR: 0.000000
2025-10-22 02:32:05,578 - INFO - Batch 1200, Loss: 2.0628, LR: 0.000000
2025-10-22 02:32:21,888 - INFO - Batch 1300, Loss: 2.2249, LR: 0.000000
2025-10-22 02:32:38,194 - INFO - Batch 1400, Loss: 2.0576, LR: 0.000000
2025-10-22 02:32:54,538 - INFO - Batch 1500, Loss: 2.0561, LR: 0.000000
2025-10-22 02:33:10,869 - INFO - Batch 1600, Loss: 2.0212, LR: 0.000000
2025-10-22 02:33:27,083 - INFO - Batch 1700, Loss: 2.1137, LR: 0.000000
2025-10-22 02:33:43,054 - INFO - Batch 1800, Loss: 2.0818, LR: 0.000000
2025-10-22 02:33:58,919 - INFO - Batch 1900, Loss: 1.9659, LR: 0.000000
2025-10-22 02:34:14,835 - INFO - Batch 2000, Loss: 2.0528, LR: 0.000000
2025-10-22 02:34:30,999 - INFO - Batch 2100, Loss: 2.1896, LR: 0.000000
2025-10-22 02:34:47,210 - INFO - Batch 2200, Loss: 1.8750, LR: 0.000000
2025-10-22 02:35:03,335 - INFO - Batch 2300, Loss: 1.8293, LR: 0.000000
2025-10-22 02:35:19,544 - INFO - Batch 2400, Loss: 2.0037, LR: 0.000000
2025-10-22 02:35:35,843 - INFO - Batch 2500, Loss: 2.1326, LR: 0.000000
2025-10-22 02:35:52,137 - INFO - Batch 2600, Loss: 1.8416, LR: 0.000000
2025-10-22 02:36:08,435 - INFO - Batch 2700, Loss: 1.8840, LR: 0.000000
2025-10-22 02:36:24,573 - INFO - Batch 2800, Loss: 2.2475, LR: 0.000000
2025-10-22 02:36:40,526 - INFO - Batch 2900, Loss: 2.1220, LR: 0.000000
2025-10-22 02:36:56,530 - INFO - Batch 3000, Loss: 2.3177, LR: 0.000000
2025-10-22 02:37:12,651 - INFO - Batch 3100, Loss: 1.8864, LR: 0.000000
2025-10-22 02:37:28,688 - INFO - Batch 3200, Loss: 2.1057, LR: 0.000000
2025-10-22 02:37:44,649 - INFO - Batch 3300, Loss: 1.8883, LR: 0.000000
2025-10-22 02:38:00,631 - INFO - Batch 3400, Loss: 2.0760, LR: 0.000000
2025-10-22 02:38:16,598 - INFO - Batch 3500, Loss: 1.9765, LR: 0.000000
2025-10-22 02:38:32,581 - INFO - Batch 3600, Loss: 2.2807, LR: 0.000000
2025-10-22 02:38:48,491 - INFO - Batch 3700, Loss: 2.0128, LR: 0.000000
2025-10-22 02:39:04,359 - INFO - Batch 3800, Loss: 2.1166, LR: 0.000000
2025-10-22 02:39:20,143 - INFO - Batch 3900, Loss: 2.0761, LR: 0.000000
2025-10-22 02:39:35,886 - INFO - Batch 4000, Loss: 2.0199, LR: 0.000000
2025-10-22 02:39:51,657 - INFO - Batch 4100, Loss: 2.0041, LR: 0.000000
2025-10-22 02:40:07,590 - INFO - Batch 4200, Loss: 1.9872, LR: 0.000000
2025-10-22 02:40:23,551 - INFO - Batch 4300, Loss: 2.1441, LR: 0.000000
2025-10-22 02:40:39,515 - INFO - Batch 4400, Loss: 2.4596, LR: 0.000000
2025-10-22 02:40:55,384 - INFO - Batch 4500, Loss: 2.1301, LR: 0.000000
2025-10-22 02:41:11,305 - INFO - Batch 4600, Loss: 2.1980, LR: 0.000000
2025-10-22 02:41:27,177 - INFO - Batch 4700, Loss: 1.9684, LR: 0.000000
2025-10-22 02:41:43,038 - INFO - Batch 4800, Loss: 1.8986, LR: 0.000000
2025-10-22 02:41:58,895 - INFO - Batch 4900, Loss: 2.0549, LR: 0.000000
2025-10-22 02:42:14,809 - INFO - Batch 5000, Loss: 2.1814, LR: 0.000000
2025-10-22 02:42:30,554 - INFO - Batch 5100, Loss: 2.0918, LR: 0.000000
2025-10-22 02:42:46,320 - INFO - Batch 5200, Loss: 2.1315, LR: 0.000000
2025-10-22 02:43:02,255 - INFO - Batch 5300, Loss: 1.9777, LR: 0.000000
2025-10-22 02:43:18,436 - INFO - Batch 5400, Loss: 2.1298, LR: 0.000000
2025-10-22 02:43:34,569 - INFO - Batch 5500, Loss: 2.0175, LR: 0.000000
2025-10-22 02:43:50,688 - INFO - Batch 5600, Loss: 2.0390, LR: 0.000000
2025-10-22 02:44:06,899 - INFO - Batch 5700, Loss: 2.0815, LR: 0.000000
2025-10-22 02:44:23,062 - INFO - Batch 5800, Loss: 2.0833, LR: 0.000000
2025-10-22 02:44:39,021 - INFO - Batch 5900, Loss: 2.0850, LR: 0.000000
2025-10-22 02:44:54,934 - INFO - Batch 6000, Loss: 2.2519, LR: 0.000000
2025-10-22 02:45:10,781 - INFO - Batch 6100, Loss: 2.0444, LR: 0.000000
2025-10-22 02:45:26,757 - INFO - Batch 6200, Loss: 2.1876, LR: 0.000000
2025-10-22 02:45:42,877 - INFO - Batch 6300, Loss: 2.3101, LR: 0.000000
2025-10-22 02:45:58,810 - INFO - Batch 6400, Loss: 1.8662, LR: 0.000000
2025-10-22 02:46:06,757 - INFO - Epoch 30/30: Train Loss: 2.0881, Val Loss: 2.0747, LR: 0.000000
2025-10-22 02:46:07,005 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-22 02:46:07,199 - INFO - 模型已保存到: ./checkpoints/checkpoint_epoch_30.pth
2025-10-22 02:46:07,396 - INFO - 模型已保存到: ./checkpoints/final_model.pth
2025-10-22 02:46:07,970 - INFO - 训练完成!
2025-10-23 10:01:05,199 - INFO - 使用设备: cuda:2
2025-10-23 10:01:05,199 - INFO - 加载数据...
2025-10-23 10:01:05,199 - INFO - 加载训练数据...
2025-10-23 10:01:09,002 - INFO - 成功加载 data/IWSLT2017/iwslt2017_train.pkl
2025-10-23 10:01:09,003 - INFO - 训练数据结构:
2025-10-23 10:01:09,003 - INFO - 数据结构分析:
2025-10-23 10:01:09,003 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-23 10:01:09,003 - INFO - 
英语相关语言对:
2025-10-23 10:01:09,003 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-23 10:01:09,003 - INFO -   en->de: 206112 个样本
2025-10-23 10:01:09,003 - INFO - 
德语相关语言对:
2025-10-23 10:01:09,003 - INFO - 德语->其他语言: ['en']
2025-10-23 10:01:09,003 - INFO -   de->en: 206112 个样本
2025-10-23 10:01:09,003 - INFO - 提取 en->de 的翻译对
2025-10-23 10:01:09,003 - INFO - 找到 en->de: 206112 个样本
2025-10-23 10:01:09,037 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-23 10:01:09,037 - INFO - 加载验证数据...
2025-10-23 10:01:09,068 - INFO - 成功加载 data/IWSLT2017/iwslt2017_validation.pkl
2025-10-23 10:01:09,068 - INFO - 提取 en->de 的翻译对
2025-10-23 10:01:09,068 - INFO - 找到 en->de: 888 个样本
2025-10-23 10:01:09,068 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-23 10:01:09,068 - INFO - 加载测试数据...
2025-10-23 10:01:09,150 - INFO - 成功加载 data/IWSLT2017/iwslt2017_test.pkl
2025-10-23 10:01:09,151 - INFO - 提取 en->de 的翻译对
2025-10-23 10:01:09,151 - INFO - 找到 en->de: 8079 个样本
2025-10-23 10:01:09,152 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-23 10:01:09,152 - INFO - 开始构建分词器...
2025-10-23 10:01:09,183 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-23 10:01:10,757 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-23 10:01:13,752 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-23 10:01:13,814 - INFO - 数据加载器创建完成
2025-10-23 10:01:13,814 - INFO - 训练集: 206112 个样本
2025-10-23 10:01:13,814 - INFO - 验证集: 888 个样本
2025-10-23 10:01:13,814 - INFO - 测试集: 8079 个样本
2025-10-23 10:01:15,031 - INFO - 源语言词汇表大小: 5000
2025-10-23 10:01:15,031 - INFO - 目标语言词汇表大小: 5000
2025-10-23 10:01:15,031 - INFO - 创建模型...
2025-10-23 10:01:16,503 - INFO - 总参数数: 15,041,416
2025-10-23 10:01:16,503 - INFO - 可训练参数数: 15,041,416
2025-10-23 10:01:16,504 - INFO - 开始训练...
2025-10-23 10:01:17,520 - INFO - Batch 0, Loss: 8.5814, LR: 0.000000
2025-10-23 10:01:29,631 - INFO - Batch 100, Loss: 8.0524, LR: 0.000005
2025-10-23 10:01:41,737 - INFO - Batch 200, Loss: 7.5594, LR: 0.000010
2025-10-23 10:01:54,118 - INFO - Batch 300, Loss: 7.1740, LR: 0.000015
2025-10-23 10:02:06,501 - INFO - Batch 400, Loss: 6.8133, LR: 0.000020
2025-10-23 10:02:18,946 - INFO - Batch 500, Loss: 6.8655, LR: 0.000025
2025-10-23 10:02:31,156 - INFO - Batch 600, Loss: 6.7730, LR: 0.000030
2025-10-23 10:02:43,413 - INFO - Batch 700, Loss: 6.6172, LR: 0.000035
2025-10-23 10:02:55,637 - INFO - Batch 800, Loss: 6.3731, LR: 0.000040
2025-10-23 10:03:07,807 - INFO - Batch 900, Loss: 6.4287, LR: 0.000045
2025-10-23 10:03:20,242 - INFO - Batch 1000, Loss: 6.5323, LR: 0.000050
2025-10-23 10:03:32,573 - INFO - Batch 1100, Loss: 6.3293, LR: 0.000055
2025-10-23 10:03:44,805 - INFO - Batch 1200, Loss: 6.2985, LR: 0.000060
2025-10-23 10:03:57,049 - INFO - Batch 1300, Loss: 6.2636, LR: 0.000065
2025-10-23 10:04:09,280 - INFO - Batch 1400, Loss: 6.2506, LR: 0.000070
2025-10-23 10:04:21,522 - INFO - Batch 1500, Loss: 6.0095, LR: 0.000075
2025-10-23 10:04:33,790 - INFO - Batch 1600, Loss: 6.1107, LR: 0.000080
2025-10-23 10:04:46,241 - INFO - Batch 1700, Loss: 6.0156, LR: 0.000085
2025-10-23 10:04:58,637 - INFO - Batch 1800, Loss: 5.7954, LR: 0.000090
2025-10-23 10:05:11,035 - INFO - Batch 1900, Loss: 6.0070, LR: 0.000095
2025-10-23 10:05:23,369 - INFO - Batch 2000, Loss: 5.7944, LR: 0.000100
2025-10-23 10:05:35,719 - INFO - Batch 2100, Loss: 5.7980, LR: 0.000100
2025-10-23 10:05:48,093 - INFO - Batch 2200, Loss: 5.6265, LR: 0.000100
2025-10-23 10:06:00,462 - INFO - Batch 2300, Loss: 5.5629, LR: 0.000100
2025-10-23 10:06:12,810 - INFO - Batch 2400, Loss: 5.6509, LR: 0.000100
2025-10-23 10:06:25,171 - INFO - Batch 2500, Loss: 5.4734, LR: 0.000100
2025-10-23 10:06:37,576 - INFO - Batch 2600, Loss: 5.5614, LR: 0.000100
2025-10-23 10:06:49,925 - INFO - Batch 2700, Loss: 5.4209, LR: 0.000100
2025-10-23 10:07:02,256 - INFO - Batch 2800, Loss: 5.2489, LR: 0.000100
2025-10-23 10:07:14,685 - INFO - Batch 2900, Loss: 5.3490, LR: 0.000100
2025-10-23 10:07:27,266 - INFO - Batch 3000, Loss: 5.3837, LR: 0.000100
2025-10-23 10:07:39,730 - INFO - Batch 3100, Loss: 5.4839, LR: 0.000100
2025-10-23 10:07:52,245 - INFO - Batch 3200, Loss: 5.4533, LR: 0.000100
2025-10-23 10:08:04,748 - INFO - Batch 3300, Loss: 5.4702, LR: 0.000100
2025-10-23 10:08:17,219 - INFO - Batch 3400, Loss: 5.2306, LR: 0.000100
2025-10-23 10:08:29,643 - INFO - Batch 3500, Loss: 5.1929, LR: 0.000100
2025-10-23 10:08:42,175 - INFO - Batch 3600, Loss: 5.0529, LR: 0.000100
2025-10-23 10:08:54,693 - INFO - Batch 3700, Loss: 5.3073, LR: 0.000100
2025-10-23 10:09:07,209 - INFO - Batch 3800, Loss: 5.0442, LR: 0.000100
2025-10-23 10:09:19,733 - INFO - Batch 3900, Loss: 5.1600, LR: 0.000100
2025-10-23 10:09:32,270 - INFO - Batch 4000, Loss: 5.1884, LR: 0.000100
2025-10-23 10:09:44,836 - INFO - Batch 4100, Loss: 5.0921, LR: 0.000100
2025-10-23 10:09:57,335 - INFO - Batch 4200, Loss: 5.0253, LR: 0.000100
2025-10-23 10:10:09,668 - INFO - Batch 4300, Loss: 5.0989, LR: 0.000100
2025-10-23 10:10:21,958 - INFO - Batch 4400, Loss: 5.1395, LR: 0.000100
2025-10-23 10:10:34,276 - INFO - Batch 4500, Loss: 5.0861, LR: 0.000100
2025-10-23 10:10:46,807 - INFO - Batch 4600, Loss: 5.1356, LR: 0.000100
2025-10-23 10:10:59,266 - INFO - Batch 4700, Loss: 4.9506, LR: 0.000100
2025-10-23 10:11:11,600 - INFO - Batch 4800, Loss: 5.0570, LR: 0.000100
2025-10-23 10:11:23,896 - INFO - Batch 4900, Loss: 5.0077, LR: 0.000100
2025-10-23 10:11:36,210 - INFO - Batch 5000, Loss: 5.1244, LR: 0.000100
2025-10-23 10:11:48,426 - INFO - Batch 5100, Loss: 4.9775, LR: 0.000100
2025-10-23 10:12:00,664 - INFO - Batch 5200, Loss: 5.1658, LR: 0.000100
2025-10-23 10:12:12,992 - INFO - Batch 5300, Loss: 4.7234, LR: 0.000100
2025-10-23 10:12:25,447 - INFO - Batch 5400, Loss: 4.6002, LR: 0.000100
2025-10-23 10:12:37,903 - INFO - Batch 5500, Loss: 4.7466, LR: 0.000100
2025-10-23 10:12:50,226 - INFO - Batch 5600, Loss: 4.8995, LR: 0.000100
2025-10-23 10:13:02,557 - INFO - Batch 5700, Loss: 4.6375, LR: 0.000100
2025-10-23 10:13:14,922 - INFO - Batch 5800, Loss: 4.8406, LR: 0.000100
2025-10-23 10:13:27,216 - INFO - Batch 5900, Loss: 5.0204, LR: 0.000100
2025-10-23 10:13:39,514 - INFO - Batch 6000, Loss: 4.8586, LR: 0.000100
2025-10-23 10:13:51,822 - INFO - Batch 6100, Loss: 4.9978, LR: 0.000100
2025-10-23 10:14:04,199 - INFO - Batch 6200, Loss: 5.0074, LR: 0.000100
2025-10-23 10:14:16,485 - INFO - Batch 6300, Loss: 4.7871, LR: 0.000100
2025-10-23 10:14:28,781 - INFO - Batch 6400, Loss: 4.8556, LR: 0.000100
2025-10-23 10:14:34,936 - INFO - Epoch 1/30: Train Loss: 5.6155, Val Loss: 4.7914, LR: 0.000100
2025-10-23 10:14:35,125 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 10:14:35,256 - INFO - Batch 0, Loss: 4.7619, LR: 0.000100
2025-10-23 10:14:47,596 - INFO - Batch 100, Loss: 4.4559, LR: 0.000100
2025-10-23 10:14:59,969 - INFO - Batch 200, Loss: 4.6318, LR: 0.000100
2025-10-23 10:15:12,364 - INFO - Batch 300, Loss: 4.8092, LR: 0.000100
2025-10-23 10:15:24,699 - INFO - Batch 400, Loss: 4.5485, LR: 0.000100
2025-10-23 10:15:37,024 - INFO - Batch 500, Loss: 4.8073, LR: 0.000100
2025-10-23 10:15:49,403 - INFO - Batch 600, Loss: 4.8656, LR: 0.000100
2025-10-23 10:16:01,823 - INFO - Batch 700, Loss: 4.8414, LR: 0.000100
2025-10-23 10:16:14,112 - INFO - Batch 800, Loss: 4.5939, LR: 0.000100
2025-10-23 10:16:26,461 - INFO - Batch 900, Loss: 4.7654, LR: 0.000100
2025-10-23 10:16:38,789 - INFO - Batch 1000, Loss: 4.8366, LR: 0.000100
2025-10-23 10:16:51,167 - INFO - Batch 1100, Loss: 4.7429, LR: 0.000100
2025-10-23 10:17:03,500 - INFO - Batch 1200, Loss: 4.6326, LR: 0.000100
2025-10-23 10:17:15,812 - INFO - Batch 1300, Loss: 4.7483, LR: 0.000100
2025-10-23 10:17:28,246 - INFO - Batch 1400, Loss: 4.6624, LR: 0.000100
2025-10-23 10:17:40,549 - INFO - Batch 1500, Loss: 4.4237, LR: 0.000100
2025-10-23 10:17:52,849 - INFO - Batch 1600, Loss: 4.5565, LR: 0.000100
2025-10-23 10:18:05,178 - INFO - Batch 1700, Loss: 4.4005, LR: 0.000100
2025-10-23 10:18:17,480 - INFO - Batch 1800, Loss: 4.6189, LR: 0.000100
2025-10-23 10:18:29,791 - INFO - Batch 1900, Loss: 4.5098, LR: 0.000100
2025-10-23 10:18:42,068 - INFO - Batch 2000, Loss: 4.6357, LR: 0.000100
2025-10-23 10:18:54,357 - INFO - Batch 2100, Loss: 4.3278, LR: 0.000100
2025-10-23 10:19:06,678 - INFO - Batch 2200, Loss: 4.4198, LR: 0.000100
2025-10-23 10:19:18,974 - INFO - Batch 2300, Loss: 4.5596, LR: 0.000100
2025-10-23 10:19:31,249 - INFO - Batch 2400, Loss: 4.7352, LR: 0.000100
2025-10-23 10:19:43,597 - INFO - Batch 2500, Loss: 4.3527, LR: 0.000100
2025-10-23 10:19:55,872 - INFO - Batch 2600, Loss: 4.4897, LR: 0.000100
2025-10-23 10:20:08,148 - INFO - Batch 2700, Loss: 4.5807, LR: 0.000100
2025-10-23 10:20:20,434 - INFO - Batch 2800, Loss: 4.4842, LR: 0.000100
2025-10-23 10:20:32,718 - INFO - Batch 2900, Loss: 4.3412, LR: 0.000100
2025-10-23 10:20:44,985 - INFO - Batch 3000, Loss: 4.5133, LR: 0.000100
2025-10-23 10:20:57,271 - INFO - Batch 3100, Loss: 4.5686, LR: 0.000100
2025-10-23 10:21:09,562 - INFO - Batch 3200, Loss: 4.4772, LR: 0.000100
2025-10-23 10:21:21,839 - INFO - Batch 3300, Loss: 4.5360, LR: 0.000100
2025-10-23 10:21:34,135 - INFO - Batch 3400, Loss: 4.4593, LR: 0.000100
2025-10-23 10:21:46,407 - INFO - Batch 3500, Loss: 4.5625, LR: 0.000100
2025-10-23 10:21:58,689 - INFO - Batch 3600, Loss: 4.4520, LR: 0.000100
2025-10-23 10:22:10,965 - INFO - Batch 3700, Loss: 4.4886, LR: 0.000100
2025-10-23 10:22:23,439 - INFO - Batch 3800, Loss: 4.3314, LR: 0.000100
2025-10-23 10:22:35,738 - INFO - Batch 3900, Loss: 4.3184, LR: 0.000100
2025-10-23 10:22:48,067 - INFO - Batch 4000, Loss: 4.4989, LR: 0.000100
2025-10-23 10:23:00,389 - INFO - Batch 4100, Loss: 4.2545, LR: 0.000100
2025-10-23 10:23:12,753 - INFO - Batch 4200, Loss: 4.3146, LR: 0.000099
2025-10-23 10:23:25,098 - INFO - Batch 4300, Loss: 4.5006, LR: 0.000099
2025-10-23 10:23:37,444 - INFO - Batch 4400, Loss: 3.9045, LR: 0.000099
2025-10-23 10:23:49,797 - INFO - Batch 4500, Loss: 3.9493, LR: 0.000099
2025-10-23 10:24:02,107 - INFO - Batch 4600, Loss: 4.3093, LR: 0.000099
2025-10-23 10:24:14,411 - INFO - Batch 4700, Loss: 4.4597, LR: 0.000099
2025-10-23 10:24:26,735 - INFO - Batch 4800, Loss: 4.2296, LR: 0.000099
2025-10-23 10:24:39,003 - INFO - Batch 4900, Loss: 4.3045, LR: 0.000099
2025-10-23 10:24:51,343 - INFO - Batch 5000, Loss: 3.9749, LR: 0.000099
2025-10-23 10:25:03,607 - INFO - Batch 5100, Loss: 3.9931, LR: 0.000099
2025-10-23 10:25:15,901 - INFO - Batch 5200, Loss: 4.4198, LR: 0.000099
2025-10-23 10:25:28,180 - INFO - Batch 5300, Loss: 3.9487, LR: 0.000099
2025-10-23 10:25:40,458 - INFO - Batch 5400, Loss: 4.2585, LR: 0.000099
2025-10-23 10:25:52,738 - INFO - Batch 5500, Loss: 4.1529, LR: 0.000099
2025-10-23 10:26:05,015 - INFO - Batch 5600, Loss: 4.0565, LR: 0.000099
2025-10-23 10:26:17,337 - INFO - Batch 5700, Loss: 3.9589, LR: 0.000099
2025-10-23 10:26:29,654 - INFO - Batch 5800, Loss: 4.2696, LR: 0.000099
2025-10-23 10:26:41,901 - INFO - Batch 5900, Loss: 3.7338, LR: 0.000099
2025-10-23 10:26:54,127 - INFO - Batch 6000, Loss: 4.0758, LR: 0.000099
2025-10-23 10:27:06,381 - INFO - Batch 6100, Loss: 4.1548, LR: 0.000099
2025-10-23 10:27:18,721 - INFO - Batch 6200, Loss: 3.8933, LR: 0.000099
2025-10-23 10:27:31,023 - INFO - Batch 6300, Loss: 4.0372, LR: 0.000099
2025-10-23 10:27:43,309 - INFO - Batch 6400, Loss: 3.8638, LR: 0.000099
2025-10-23 10:27:49,438 - INFO - Epoch 2/30: Train Loss: 4.4215, Val Loss: 3.9644, LR: 0.000099
2025-10-23 10:27:49,682 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 10:27:49,813 - INFO - Batch 0, Loss: 4.0136, LR: 0.000099
2025-10-23 10:28:02,138 - INFO - Batch 100, Loss: 4.0321, LR: 0.000099
2025-10-23 10:28:14,397 - INFO - Batch 200, Loss: 3.9468, LR: 0.000099
2025-10-23 10:28:26,770 - INFO - Batch 300, Loss: 3.9676, LR: 0.000099
2025-10-23 10:28:39,048 - INFO - Batch 400, Loss: 4.1216, LR: 0.000099
2025-10-23 10:28:51,336 - INFO - Batch 500, Loss: 3.9447, LR: 0.000099
2025-10-23 10:29:03,593 - INFO - Batch 600, Loss: 3.7278, LR: 0.000099
2025-10-23 10:29:15,874 - INFO - Batch 700, Loss: 3.9327, LR: 0.000099
2025-10-23 10:29:28,173 - INFO - Batch 800, Loss: 4.0894, LR: 0.000099
2025-10-23 10:29:40,476 - INFO - Batch 900, Loss: 3.9009, LR: 0.000099
2025-10-23 10:29:52,761 - INFO - Batch 1000, Loss: 3.8719, LR: 0.000099
2025-10-23 10:30:05,057 - INFO - Batch 1100, Loss: 3.9789, LR: 0.000099
2025-10-23 10:30:17,347 - INFO - Batch 1200, Loss: 3.8404, LR: 0.000099
2025-10-23 10:30:29,667 - INFO - Batch 1300, Loss: 3.7553, LR: 0.000099
2025-10-23 10:30:41,965 - INFO - Batch 1400, Loss: 3.9610, LR: 0.000099
2025-10-23 10:30:54,272 - INFO - Batch 1500, Loss: 3.9486, LR: 0.000099
2025-10-23 10:31:06,567 - INFO - Batch 1600, Loss: 3.9891, LR: 0.000099
2025-10-23 10:31:18,876 - INFO - Batch 1700, Loss: 3.7508, LR: 0.000099
2025-10-23 10:31:31,217 - INFO - Batch 1800, Loss: 3.7335, LR: 0.000099
2025-10-23 10:31:43,577 - INFO - Batch 1900, Loss: 3.4902, LR: 0.000099
2025-10-23 10:31:55,946 - INFO - Batch 2000, Loss: 3.8733, LR: 0.000099
2025-10-23 10:32:08,345 - INFO - Batch 2100, Loss: 3.7785, LR: 0.000099
2025-10-23 10:32:20,781 - INFO - Batch 2200, Loss: 3.6971, LR: 0.000099
2025-10-23 10:32:33,172 - INFO - Batch 2300, Loss: 3.8342, LR: 0.000099
2025-10-23 10:32:45,510 - INFO - Batch 2400, Loss: 3.7505, LR: 0.000099
2025-10-23 10:32:57,912 - INFO - Batch 2500, Loss: 3.7459, LR: 0.000099
2025-10-23 10:33:10,272 - INFO - Batch 2600, Loss: 3.6611, LR: 0.000099
2025-10-23 10:33:22,612 - INFO - Batch 2700, Loss: 3.9073, LR: 0.000099
2025-10-23 10:33:34,951 - INFO - Batch 2800, Loss: 3.6434, LR: 0.000099
2025-10-23 10:33:47,320 - INFO - Batch 2900, Loss: 4.0283, LR: 0.000099
2025-10-23 10:33:59,679 - INFO - Batch 3000, Loss: 3.5775, LR: 0.000099
2025-10-23 10:34:12,029 - INFO - Batch 3100, Loss: 3.8649, LR: 0.000099
2025-10-23 10:34:24,428 - INFO - Batch 3200, Loss: 3.5816, LR: 0.000099
2025-10-23 10:34:36,876 - INFO - Batch 3300, Loss: 3.5900, LR: 0.000099
2025-10-23 10:34:49,248 - INFO - Batch 3400, Loss: 3.4943, LR: 0.000099
2025-10-23 10:35:01,610 - INFO - Batch 3500, Loss: 3.7197, LR: 0.000099
2025-10-23 10:35:13,988 - INFO - Batch 3600, Loss: 3.5601, LR: 0.000099
2025-10-23 10:35:26,348 - INFO - Batch 3700, Loss: 3.4294, LR: 0.000099
2025-10-23 10:35:38,695 - INFO - Batch 3800, Loss: 3.6658, LR: 0.000099
2025-10-23 10:35:51,091 - INFO - Batch 3900, Loss: 3.6832, LR: 0.000099
2025-10-23 10:36:03,445 - INFO - Batch 4000, Loss: 3.7702, LR: 0.000099
2025-10-23 10:36:15,774 - INFO - Batch 4100, Loss: 3.6424, LR: 0.000098
2025-10-23 10:36:28,108 - INFO - Batch 4200, Loss: 3.5856, LR: 0.000098
2025-10-23 10:36:40,478 - INFO - Batch 4300, Loss: 3.8250, LR: 0.000098
2025-10-23 10:36:52,819 - INFO - Batch 4400, Loss: 3.5892, LR: 0.000098
2025-10-23 10:37:05,126 - INFO - Batch 4500, Loss: 3.7287, LR: 0.000098
2025-10-23 10:37:17,491 - INFO - Batch 4600, Loss: 3.5062, LR: 0.000098
2025-10-23 10:37:29,785 - INFO - Batch 4700, Loss: 3.6549, LR: 0.000098
2025-10-23 10:37:42,043 - INFO - Batch 4800, Loss: 3.2713, LR: 0.000098
2025-10-23 10:37:54,317 - INFO - Batch 4900, Loss: 3.5813, LR: 0.000098
2025-10-23 10:38:06,583 - INFO - Batch 5000, Loss: 3.4452, LR: 0.000098
2025-10-23 10:38:18,825 - INFO - Batch 5100, Loss: 3.7640, LR: 0.000098
2025-10-23 10:38:31,076 - INFO - Batch 5200, Loss: 3.3448, LR: 0.000098
2025-10-23 10:38:43,389 - INFO - Batch 5300, Loss: 3.4130, LR: 0.000098
2025-10-23 10:38:55,684 - INFO - Batch 5400, Loss: 3.7134, LR: 0.000098
2025-10-23 10:39:07,966 - INFO - Batch 5500, Loss: 3.7050, LR: 0.000098
2025-10-23 10:39:20,217 - INFO - Batch 5600, Loss: 3.8875, LR: 0.000098
2025-10-23 10:39:32,470 - INFO - Batch 5700, Loss: 3.3971, LR: 0.000098
2025-10-23 10:39:44,739 - INFO - Batch 5800, Loss: 3.5658, LR: 0.000098
2025-10-23 10:39:57,006 - INFO - Batch 5900, Loss: 3.5693, LR: 0.000098
2025-10-23 10:40:09,269 - INFO - Batch 6000, Loss: 3.2462, LR: 0.000098
2025-10-23 10:40:21,573 - INFO - Batch 6100, Loss: 3.5482, LR: 0.000098
2025-10-23 10:40:33,843 - INFO - Batch 6200, Loss: 3.3978, LR: 0.000098
2025-10-23 10:40:46,098 - INFO - Batch 6300, Loss: 3.4851, LR: 0.000098
2025-10-23 10:40:58,472 - INFO - Batch 6400, Loss: 3.4442, LR: 0.000098
2025-10-23 10:41:04,712 - INFO - Epoch 3/30: Train Loss: 3.6998, Val Loss: 3.3465, LR: 0.000098
2025-10-23 10:41:04,959 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 10:41:05,090 - INFO - Batch 0, Loss: 3.4564, LR: 0.000098
2025-10-23 10:41:17,622 - INFO - Batch 100, Loss: 3.1163, LR: 0.000098
2025-10-23 10:41:30,165 - INFO - Batch 200, Loss: 3.5620, LR: 0.000098
2025-10-23 10:41:42,698 - INFO - Batch 300, Loss: 3.3592, LR: 0.000098
2025-10-23 10:41:55,009 - INFO - Batch 400, Loss: 3.5562, LR: 0.000098
2025-10-23 10:42:07,236 - INFO - Batch 500, Loss: 3.4282, LR: 0.000098
2025-10-23 10:42:19,518 - INFO - Batch 600, Loss: 3.2579, LR: 0.000098
2025-10-23 10:42:31,751 - INFO - Batch 700, Loss: 3.1995, LR: 0.000098
2025-10-23 10:42:43,998 - INFO - Batch 800, Loss: 3.2977, LR: 0.000098
2025-10-23 10:42:56,220 - INFO - Batch 900, Loss: 3.3641, LR: 0.000098
2025-10-23 10:43:08,477 - INFO - Batch 1000, Loss: 3.3719, LR: 0.000098
2025-10-23 10:43:20,712 - INFO - Batch 1100, Loss: 3.5712, LR: 0.000098
2025-10-23 10:43:32,945 - INFO - Batch 1200, Loss: 3.4223, LR: 0.000098
2025-10-23 10:43:45,221 - INFO - Batch 1300, Loss: 3.2202, LR: 0.000098
2025-10-23 10:43:57,519 - INFO - Batch 1400, Loss: 3.3619, LR: 0.000098
2025-10-23 10:44:09,844 - INFO - Batch 1500, Loss: 3.1319, LR: 0.000098
2025-10-23 10:44:22,146 - INFO - Batch 1600, Loss: 3.5089, LR: 0.000098
2025-10-23 10:44:34,443 - INFO - Batch 1700, Loss: 3.3186, LR: 0.000098
2025-10-23 10:44:46,757 - INFO - Batch 1800, Loss: 3.2599, LR: 0.000098
2025-10-23 10:44:59,069 - INFO - Batch 1900, Loss: 3.2709, LR: 0.000098
2025-10-23 10:45:11,394 - INFO - Batch 2000, Loss: 3.2701, LR: 0.000098
2025-10-23 10:45:23,690 - INFO - Batch 2100, Loss: 3.1540, LR: 0.000097
2025-10-23 10:45:35,993 - INFO - Batch 2200, Loss: 3.3844, LR: 0.000097
2025-10-23 10:45:48,326 - INFO - Batch 2300, Loss: 2.8825, LR: 0.000097
2025-10-23 10:46:00,632 - INFO - Batch 2400, Loss: 3.1053, LR: 0.000097
2025-10-23 10:46:12,920 - INFO - Batch 2500, Loss: 3.0293, LR: 0.000097
2025-10-23 10:46:25,229 - INFO - Batch 2600, Loss: 3.0524, LR: 0.000097
2025-10-23 10:46:37,553 - INFO - Batch 2700, Loss: 3.3231, LR: 0.000097
2025-10-23 10:46:49,871 - INFO - Batch 2800, Loss: 3.4080, LR: 0.000097
2025-10-23 10:47:02,217 - INFO - Batch 2900, Loss: 3.1213, LR: 0.000097
2025-10-23 10:47:14,577 - INFO - Batch 3000, Loss: 2.9771, LR: 0.000097
2025-10-23 10:47:26,937 - INFO - Batch 3100, Loss: 3.0327, LR: 0.000097
2025-10-23 10:47:39,247 - INFO - Batch 3200, Loss: 3.2422, LR: 0.000097
2025-10-23 10:47:51,525 - INFO - Batch 3300, Loss: 3.0994, LR: 0.000097
2025-10-23 10:48:03,808 - INFO - Batch 3400, Loss: 3.3020, LR: 0.000097
2025-10-23 10:48:16,089 - INFO - Batch 3500, Loss: 3.2304, LR: 0.000097
2025-10-23 10:48:28,347 - INFO - Batch 3600, Loss: 3.1617, LR: 0.000097
2025-10-23 10:48:40,619 - INFO - Batch 3700, Loss: 3.0432, LR: 0.000097
2025-10-23 10:48:52,889 - INFO - Batch 3800, Loss: 3.0162, LR: 0.000097
2025-10-23 10:49:05,194 - INFO - Batch 3900, Loss: 3.0424, LR: 0.000097
2025-10-23 10:49:17,525 - INFO - Batch 4000, Loss: 3.2148, LR: 0.000097
2025-10-23 10:49:29,809 - INFO - Batch 4100, Loss: 3.3985, LR: 0.000097
2025-10-23 10:49:42,044 - INFO - Batch 4200, Loss: 3.2112, LR: 0.000097
2025-10-23 10:49:54,267 - INFO - Batch 4300, Loss: 3.2459, LR: 0.000097
2025-10-23 10:50:06,578 - INFO - Batch 4400, Loss: 3.0030, LR: 0.000097
2025-10-23 10:50:19,110 - INFO - Batch 4500, Loss: 2.9002, LR: 0.000097
2025-10-23 10:50:31,634 - INFO - Batch 4600, Loss: 3.1224, LR: 0.000097
2025-10-23 10:50:44,112 - INFO - Batch 4700, Loss: 3.1700, LR: 0.000097
2025-10-23 10:50:56,602 - INFO - Batch 4800, Loss: 2.9240, LR: 0.000097
2025-10-23 10:51:09,058 - INFO - Batch 4900, Loss: 3.1210, LR: 0.000097
2025-10-23 10:51:21,486 - INFO - Batch 5000, Loss: 3.0343, LR: 0.000097
2025-10-23 10:51:33,974 - INFO - Batch 5100, Loss: 3.0214, LR: 0.000097
2025-10-23 10:51:46,368 - INFO - Batch 5200, Loss: 2.9483, LR: 0.000097
2025-10-23 10:51:58,668 - INFO - Batch 5300, Loss: 2.9538, LR: 0.000097
2025-10-23 10:52:10,942 - INFO - Batch 5400, Loss: 3.0986, LR: 0.000097
2025-10-23 10:52:23,348 - INFO - Batch 5500, Loss: 3.1493, LR: 0.000097
2025-10-23 10:52:35,637 - INFO - Batch 5600, Loss: 2.9992, LR: 0.000096
2025-10-23 10:52:47,946 - INFO - Batch 5700, Loss: 3.1454, LR: 0.000096
2025-10-23 10:53:00,340 - INFO - Batch 5800, Loss: 3.1071, LR: 0.000096
2025-10-23 10:53:12,687 - INFO - Batch 5900, Loss: 3.0876, LR: 0.000096
2025-10-23 10:53:24,974 - INFO - Batch 6000, Loss: 2.7654, LR: 0.000096
2025-10-23 10:53:37,306 - INFO - Batch 6100, Loss: 2.9141, LR: 0.000096
2025-10-23 10:53:49,734 - INFO - Batch 6200, Loss: 3.1304, LR: 0.000096
2025-10-23 10:54:02,235 - INFO - Batch 6300, Loss: 3.0465, LR: 0.000096
2025-10-23 10:54:14,708 - INFO - Batch 6400, Loss: 2.8117, LR: 0.000096
2025-10-23 10:54:20,933 - INFO - Epoch 4/30: Train Loss: 3.2184, Val Loss: 2.9284, LR: 0.000096
2025-10-23 10:54:21,193 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 10:54:21,330 - INFO - Batch 0, Loss: 2.9107, LR: 0.000096
2025-10-23 10:54:33,738 - INFO - Batch 100, Loss: 2.9491, LR: 0.000096
2025-10-23 10:54:46,226 - INFO - Batch 200, Loss: 2.7992, LR: 0.000096
2025-10-23 10:54:58,647 - INFO - Batch 300, Loss: 2.9468, LR: 0.000096
2025-10-23 10:55:11,095 - INFO - Batch 400, Loss: 3.1984, LR: 0.000096
2025-10-23 10:55:23,500 - INFO - Batch 500, Loss: 2.7719, LR: 0.000096
2025-10-23 10:55:36,096 - INFO - Batch 600, Loss: 2.9372, LR: 0.000096
2025-10-23 10:55:48,624 - INFO - Batch 700, Loss: 2.9046, LR: 0.000096
2025-10-23 10:56:01,152 - INFO - Batch 800, Loss: 3.0447, LR: 0.000096
2025-10-23 10:56:13,688 - INFO - Batch 900, Loss: 2.9749, LR: 0.000096
2025-10-23 10:56:26,153 - INFO - Batch 1000, Loss: 2.8362, LR: 0.000096
2025-10-23 10:56:38,578 - INFO - Batch 1100, Loss: 3.1188, LR: 0.000096
2025-10-23 10:56:51,041 - INFO - Batch 1200, Loss: 2.7662, LR: 0.000096
2025-10-23 10:57:03,518 - INFO - Batch 1300, Loss: 2.7410, LR: 0.000096
2025-10-23 10:57:15,940 - INFO - Batch 1400, Loss: 3.1062, LR: 0.000096
2025-10-23 10:57:28,395 - INFO - Batch 1500, Loss: 2.8898, LR: 0.000096
2025-10-23 10:57:41,030 - INFO - Batch 1600, Loss: 3.1081, LR: 0.000096
2025-10-23 10:57:53,649 - INFO - Batch 1700, Loss: 2.9642, LR: 0.000096
2025-10-23 10:58:06,245 - INFO - Batch 1800, Loss: 2.9319, LR: 0.000096
2025-10-23 10:58:18,803 - INFO - Batch 1900, Loss: 3.0224, LR: 0.000096
2025-10-23 10:58:31,232 - INFO - Batch 2000, Loss: 3.1469, LR: 0.000096
2025-10-23 10:58:43,631 - INFO - Batch 2100, Loss: 2.8290, LR: 0.000096
2025-10-23 10:58:56,079 - INFO - Batch 2200, Loss: 2.8513, LR: 0.000096
2025-10-23 10:59:08,604 - INFO - Batch 2300, Loss: 3.2547, LR: 0.000095
2025-10-23 10:59:21,095 - INFO - Batch 2400, Loss: 2.9155, LR: 0.000095
2025-10-23 10:59:33,478 - INFO - Batch 2500, Loss: 3.0926, LR: 0.000095
2025-10-23 10:59:45,865 - INFO - Batch 2600, Loss: 2.8886, LR: 0.000095
2025-10-23 10:59:58,245 - INFO - Batch 2700, Loss: 3.1218, LR: 0.000095
2025-10-23 11:00:10,639 - INFO - Batch 2800, Loss: 2.8487, LR: 0.000095
2025-10-23 11:00:23,047 - INFO - Batch 2900, Loss: 2.8488, LR: 0.000095
2025-10-23 11:00:35,493 - INFO - Batch 3000, Loss: 2.8711, LR: 0.000095
2025-10-23 11:00:47,946 - INFO - Batch 3100, Loss: 3.0371, LR: 0.000095
2025-10-23 11:01:00,290 - INFO - Batch 3200, Loss: 2.9935, LR: 0.000095
2025-10-23 11:01:12,855 - INFO - Batch 3300, Loss: 2.9681, LR: 0.000095
2025-10-23 11:01:25,273 - INFO - Batch 3400, Loss: 2.9253, LR: 0.000095
2025-10-23 11:01:37,590 - INFO - Batch 3500, Loss: 3.2309, LR: 0.000095
2025-10-23 11:01:50,074 - INFO - Batch 3600, Loss: 2.8935, LR: 0.000095
2025-10-23 11:02:02,465 - INFO - Batch 3700, Loss: 2.9367, LR: 0.000095
2025-10-23 11:02:14,806 - INFO - Batch 3800, Loss: 3.0269, LR: 0.000095
2025-10-23 11:02:27,229 - INFO - Batch 3900, Loss: 2.6379, LR: 0.000095
2025-10-23 11:02:39,654 - INFO - Batch 4000, Loss: 3.0070, LR: 0.000095
2025-10-23 11:02:52,090 - INFO - Batch 4100, Loss: 3.0109, LR: 0.000095
2025-10-23 11:03:04,549 - INFO - Batch 4200, Loss: 2.8262, LR: 0.000095
2025-10-23 11:03:17,060 - INFO - Batch 4300, Loss: 2.9401, LR: 0.000095
2025-10-23 11:03:29,483 - INFO - Batch 4400, Loss: 2.9466, LR: 0.000095
2025-10-23 11:03:41,882 - INFO - Batch 4500, Loss: 2.7906, LR: 0.000095
2025-10-23 11:03:54,304 - INFO - Batch 4600, Loss: 2.8811, LR: 0.000095
2025-10-23 11:04:06,648 - INFO - Batch 4700, Loss: 2.8802, LR: 0.000095
2025-10-23 11:04:19,038 - INFO - Batch 4800, Loss: 2.9370, LR: 0.000095
2025-10-23 11:04:31,492 - INFO - Batch 4900, Loss: 2.9922, LR: 0.000095
2025-10-23 11:04:43,905 - INFO - Batch 5000, Loss: 2.9493, LR: 0.000095
2025-10-23 11:04:56,296 - INFO - Batch 5100, Loss: 2.8010, LR: 0.000094
2025-10-23 11:05:08,734 - INFO - Batch 5200, Loss: 2.8978, LR: 0.000094
2025-10-23 11:05:21,273 - INFO - Batch 5300, Loss: 2.7003, LR: 0.000094
2025-10-23 11:05:33,673 - INFO - Batch 5400, Loss: 2.9448, LR: 0.000094
2025-10-23 11:05:46,093 - INFO - Batch 5500, Loss: 2.8800, LR: 0.000094
2025-10-23 11:05:58,478 - INFO - Batch 5600, Loss: 2.7962, LR: 0.000094
2025-10-23 11:06:10,863 - INFO - Batch 5700, Loss: 2.8421, LR: 0.000094
2025-10-23 11:06:23,265 - INFO - Batch 5800, Loss: 2.8981, LR: 0.000094
2025-10-23 11:06:35,630 - INFO - Batch 5900, Loss: 2.9418, LR: 0.000094
2025-10-23 11:06:48,031 - INFO - Batch 6000, Loss: 2.7663, LR: 0.000094
2025-10-23 11:07:00,440 - INFO - Batch 6100, Loss: 2.9684, LR: 0.000094
2025-10-23 11:07:12,901 - INFO - Batch 6200, Loss: 3.0252, LR: 0.000094
2025-10-23 11:07:25,445 - INFO - Batch 6300, Loss: 2.7450, LR: 0.000094
2025-10-23 11:07:37,924 - INFO - Batch 6400, Loss: 2.7922, LR: 0.000094
2025-10-23 11:07:44,186 - INFO - Epoch 5/30: Train Loss: 2.9357, Val Loss: 2.7098, LR: 0.000094
2025-10-23 11:07:44,433 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 11:07:44,564 - INFO - Batch 0, Loss: 2.7367, LR: 0.000094
2025-10-23 11:07:56,946 - INFO - Batch 100, Loss: 2.8973, LR: 0.000094
2025-10-23 11:08:09,364 - INFO - Batch 200, Loss: 2.9912, LR: 0.000094
2025-10-23 11:08:21,946 - INFO - Batch 300, Loss: 2.9000, LR: 0.000094
2025-10-23 11:08:34,409 - INFO - Batch 400, Loss: 2.7816, LR: 0.000094
2025-10-23 11:08:46,982 - INFO - Batch 500, Loss: 2.8086, LR: 0.000094
2025-10-23 11:08:59,515 - INFO - Batch 600, Loss: 2.5029, LR: 0.000094
2025-10-23 11:09:12,057 - INFO - Batch 700, Loss: 2.6511, LR: 0.000094
2025-10-23 11:09:24,599 - INFO - Batch 800, Loss: 2.8137, LR: 0.000094
2025-10-23 11:09:37,080 - INFO - Batch 900, Loss: 2.8178, LR: 0.000094
2025-10-23 11:09:49,504 - INFO - Batch 1000, Loss: 2.6764, LR: 0.000094
2025-10-23 11:10:02,006 - INFO - Batch 1100, Loss: 2.8308, LR: 0.000094
2025-10-23 11:10:14,481 - INFO - Batch 1200, Loss: 2.8208, LR: 0.000093
2025-10-23 11:10:26,873 - INFO - Batch 1300, Loss: 2.9775, LR: 0.000093
2025-10-23 11:10:39,332 - INFO - Batch 1400, Loss: 2.7735, LR: 0.000093
2025-10-23 11:10:51,746 - INFO - Batch 1500, Loss: 2.8389, LR: 0.000093
2025-10-23 11:11:04,173 - INFO - Batch 1600, Loss: 2.8227, LR: 0.000093
2025-10-23 11:11:16,588 - INFO - Batch 1700, Loss: 2.5856, LR: 0.000093
2025-10-23 11:11:29,083 - INFO - Batch 1800, Loss: 2.8630, LR: 0.000093
2025-10-23 11:11:41,489 - INFO - Batch 1900, Loss: 2.6641, LR: 0.000093
2025-10-23 11:11:54,034 - INFO - Batch 2000, Loss: 3.0101, LR: 0.000093
2025-10-23 11:12:06,430 - INFO - Batch 2100, Loss: 2.7205, LR: 0.000093
2025-10-23 11:12:18,863 - INFO - Batch 2200, Loss: 2.5279, LR: 0.000093
2025-10-23 11:12:31,313 - INFO - Batch 2300, Loss: 2.8221, LR: 0.000093
2025-10-23 11:12:43,724 - INFO - Batch 2400, Loss: 2.8605, LR: 0.000093
2025-10-23 11:12:56,192 - INFO - Batch 2500, Loss: 2.8334, LR: 0.000093
2025-10-23 11:13:08,626 - INFO - Batch 2600, Loss: 2.7822, LR: 0.000093
2025-10-23 11:13:21,128 - INFO - Batch 2700, Loss: 2.7176, LR: 0.000093
2025-10-23 11:13:33,560 - INFO - Batch 2800, Loss: 2.7444, LR: 0.000093
2025-10-23 11:13:46,165 - INFO - Batch 2900, Loss: 2.6348, LR: 0.000093
2025-10-23 11:13:58,586 - INFO - Batch 3000, Loss: 2.9719, LR: 0.000093
2025-10-23 11:14:11,005 - INFO - Batch 3100, Loss: 2.9062, LR: 0.000093
2025-10-23 11:14:23,425 - INFO - Batch 3200, Loss: 3.0659, LR: 0.000093
2025-10-23 11:14:35,793 - INFO - Batch 3300, Loss: 2.4778, LR: 0.000093
2025-10-23 11:14:48,221 - INFO - Batch 3400, Loss: 2.4558, LR: 0.000093
2025-10-23 11:15:00,621 - INFO - Batch 3500, Loss: 2.5934, LR: 0.000093
2025-10-23 11:15:13,027 - INFO - Batch 3600, Loss: 2.8882, LR: 0.000092
2025-10-23 11:15:25,421 - INFO - Batch 3700, Loss: 2.5935, LR: 0.000092
2025-10-23 11:15:37,718 - INFO - Batch 3800, Loss: 2.7591, LR: 0.000092
2025-10-23 11:15:50,062 - INFO - Batch 3900, Loss: 3.1865, LR: 0.000092
2025-10-23 11:16:02,473 - INFO - Batch 4000, Loss: 2.9126, LR: 0.000092
2025-10-23 11:16:14,850 - INFO - Batch 4100, Loss: 2.8997, LR: 0.000092
2025-10-23 11:16:27,235 - INFO - Batch 4200, Loss: 2.6739, LR: 0.000092
2025-10-23 11:16:39,644 - INFO - Batch 4300, Loss: 2.8683, LR: 0.000092
2025-10-23 11:16:51,668 - INFO - Batch 4400, Loss: 2.6589, LR: 0.000092
2025-10-23 11:17:04,038 - INFO - Batch 4500, Loss: 2.7851, LR: 0.000092
2025-10-23 11:17:16,620 - INFO - Batch 4600, Loss: 2.9823, LR: 0.000092
2025-10-23 11:17:29,036 - INFO - Batch 4700, Loss: 2.7466, LR: 0.000092
2025-10-23 11:17:41,429 - INFO - Batch 4800, Loss: 3.0322, LR: 0.000092
2025-10-23 11:17:53,842 - INFO - Batch 4900, Loss: 2.7594, LR: 0.000092
2025-10-23 11:18:06,245 - INFO - Batch 5000, Loss: 2.8380, LR: 0.000092
2025-10-23 11:18:18,645 - INFO - Batch 5100, Loss: 2.7369, LR: 0.000092
2025-10-23 11:18:31,079 - INFO - Batch 5200, Loss: 2.5716, LR: 0.000092
2025-10-23 11:18:43,477 - INFO - Batch 5300, Loss: 2.6410, LR: 0.000092
2025-10-23 11:18:55,896 - INFO - Batch 5400, Loss: 2.9815, LR: 0.000092
2025-10-23 11:19:08,554 - INFO - Batch 5500, Loss: 2.8608, LR: 0.000092
2025-10-23 11:19:21,022 - INFO - Batch 5600, Loss: 2.5603, LR: 0.000092
2025-10-23 11:19:33,547 - INFO - Batch 5700, Loss: 2.7459, LR: 0.000092
2025-10-23 11:19:46,026 - INFO - Batch 5800, Loss: 2.7801, LR: 0.000092
2025-10-23 11:19:58,609 - INFO - Batch 5900, Loss: 2.7209, LR: 0.000091
2025-10-23 11:20:11,095 - INFO - Batch 6000, Loss: 2.8404, LR: 0.000091
2025-10-23 11:20:23,532 - INFO - Batch 6100, Loss: 2.8052, LR: 0.000091
2025-10-23 11:20:35,896 - INFO - Batch 6200, Loss: 2.6709, LR: 0.000091
2025-10-23 11:20:48,277 - INFO - Batch 6300, Loss: 2.5290, LR: 0.000091
2025-10-23 11:21:00,651 - INFO - Batch 6400, Loss: 2.5649, LR: 0.000091
2025-10-23 11:21:06,871 - INFO - Epoch 6/30: Train Loss: 2.7723, Val Loss: 2.5910, LR: 0.000091
2025-10-23 11:21:07,125 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 11:21:07,258 - INFO - Batch 0, Loss: 2.3821, LR: 0.000091
2025-10-23 11:21:19,651 - INFO - Batch 100, Loss: 2.8268, LR: 0.000091
2025-10-23 11:21:32,102 - INFO - Batch 200, Loss: 2.4490, LR: 0.000091
2025-10-23 11:21:44,601 - INFO - Batch 300, Loss: 2.0863, LR: 0.000091
2025-10-23 11:21:57,102 - INFO - Batch 400, Loss: 2.8257, LR: 0.000091
2025-10-23 11:22:09,688 - INFO - Batch 500, Loss: 2.8156, LR: 0.000091
2025-10-23 11:22:22,267 - INFO - Batch 600, Loss: 2.5703, LR: 0.000091
2025-10-23 11:22:34,804 - INFO - Batch 700, Loss: 2.5632, LR: 0.000091
2025-10-23 11:22:47,457 - INFO - Batch 800, Loss: 2.5585, LR: 0.000091
2025-10-23 11:23:00,037 - INFO - Batch 900, Loss: 2.5887, LR: 0.000091
2025-10-23 11:23:12,627 - INFO - Batch 1000, Loss: 2.3740, LR: 0.000091
2025-10-23 11:23:25,196 - INFO - Batch 1100, Loss: 2.9528, LR: 0.000091
2025-10-23 11:23:37,761 - INFO - Batch 1200, Loss: 2.4632, LR: 0.000091
2025-10-23 11:23:50,321 - INFO - Batch 1300, Loss: 2.6421, LR: 0.000091
2025-10-23 11:24:02,848 - INFO - Batch 1400, Loss: 2.5186, LR: 0.000091
2025-10-23 11:24:15,337 - INFO - Batch 1500, Loss: 2.5006, LR: 0.000090
2025-10-23 11:24:27,730 - INFO - Batch 1600, Loss: 2.4814, LR: 0.000090
2025-10-23 11:24:40,088 - INFO - Batch 1700, Loss: 2.7422, LR: 0.000090
2025-10-23 11:24:52,455 - INFO - Batch 1800, Loss: 2.7501, LR: 0.000090
2025-10-23 11:25:04,791 - INFO - Batch 1900, Loss: 2.6204, LR: 0.000090
2025-10-23 11:25:17,268 - INFO - Batch 2000, Loss: 2.8767, LR: 0.000090
2025-10-23 11:25:29,769 - INFO - Batch 2100, Loss: 2.6261, LR: 0.000090
2025-10-23 11:25:42,263 - INFO - Batch 2200, Loss: 2.6677, LR: 0.000090
2025-10-23 11:25:54,750 - INFO - Batch 2300, Loss: 2.9420, LR: 0.000090
2025-10-23 11:26:07,146 - INFO - Batch 2400, Loss: 2.6234, LR: 0.000090
2025-10-23 11:26:19,577 - INFO - Batch 2500, Loss: 2.9470, LR: 0.000090
2025-10-23 11:26:31,882 - INFO - Batch 2600, Loss: 2.5023, LR: 0.000090
2025-10-23 11:26:44,198 - INFO - Batch 2700, Loss: 2.8146, LR: 0.000090
2025-10-23 11:26:56,500 - INFO - Batch 2800, Loss: 2.7561, LR: 0.000090
2025-10-23 11:27:08,800 - INFO - Batch 2900, Loss: 2.5442, LR: 0.000090
2025-10-23 11:27:21,127 - INFO - Batch 3000, Loss: 2.9984, LR: 0.000090
2025-10-23 11:27:33,419 - INFO - Batch 3100, Loss: 2.8164, LR: 0.000090
2025-10-23 11:27:45,701 - INFO - Batch 3200, Loss: 2.6692, LR: 0.000090
2025-10-23 11:27:57,971 - INFO - Batch 3300, Loss: 2.7901, LR: 0.000090
2025-10-23 11:28:10,271 - INFO - Batch 3400, Loss: 2.5850, LR: 0.000090
2025-10-23 11:28:22,572 - INFO - Batch 3500, Loss: 2.5001, LR: 0.000090
2025-10-23 11:28:34,876 - INFO - Batch 3600, Loss: 2.6226, LR: 0.000089
2025-10-23 11:28:47,180 - INFO - Batch 3700, Loss: 2.7996, LR: 0.000089
2025-10-23 11:28:59,459 - INFO - Batch 3800, Loss: 2.7426, LR: 0.000089
2025-10-23 11:29:11,748 - INFO - Batch 3900, Loss: 2.9179, LR: 0.000089
2025-10-23 11:29:24,034 - INFO - Batch 4000, Loss: 2.4291, LR: 0.000089
2025-10-23 11:29:36,317 - INFO - Batch 4100, Loss: 2.4899, LR: 0.000089
2025-10-23 11:29:48,605 - INFO - Batch 4200, Loss: 2.5271, LR: 0.000089
2025-10-23 11:30:00,877 - INFO - Batch 4300, Loss: 2.9035, LR: 0.000089
2025-10-23 11:30:13,145 - INFO - Batch 4400, Loss: 2.6488, LR: 0.000089
2025-10-23 11:30:25,390 - INFO - Batch 4500, Loss: 2.4894, LR: 0.000089
2025-10-23 11:30:37,699 - INFO - Batch 4600, Loss: 2.6337, LR: 0.000089
2025-10-23 11:30:49,962 - INFO - Batch 4700, Loss: 2.3634, LR: 0.000089
2025-10-23 11:31:02,259 - INFO - Batch 4800, Loss: 2.6356, LR: 0.000089
2025-10-23 11:31:14,565 - INFO - Batch 4900, Loss: 2.7399, LR: 0.000089
2025-10-23 11:31:26,863 - INFO - Batch 5000, Loss: 2.4097, LR: 0.000089
2025-10-23 11:31:39,176 - INFO - Batch 5100, Loss: 2.8236, LR: 0.000089
2025-10-23 11:31:51,488 - INFO - Batch 5200, Loss: 2.5812, LR: 0.000089
2025-10-23 11:32:03,826 - INFO - Batch 5300, Loss: 2.9109, LR: 0.000089
2025-10-23 11:32:16,146 - INFO - Batch 5400, Loss: 2.4235, LR: 0.000089
2025-10-23 11:32:28,581 - INFO - Batch 5500, Loss: 2.2444, LR: 0.000088
2025-10-23 11:32:40,927 - INFO - Batch 5600, Loss: 2.7577, LR: 0.000088
2025-10-23 11:32:53,285 - INFO - Batch 5700, Loss: 2.9283, LR: 0.000088
2025-10-23 11:33:05,625 - INFO - Batch 5800, Loss: 2.5745, LR: 0.000088
2025-10-23 11:33:18,005 - INFO - Batch 5900, Loss: 2.4432, LR: 0.000088
2025-10-23 11:33:30,336 - INFO - Batch 6000, Loss: 2.6038, LR: 0.000088
2025-10-23 11:33:42,688 - INFO - Batch 6100, Loss: 2.7302, LR: 0.000088
2025-10-23 11:33:55,059 - INFO - Batch 6200, Loss: 2.5793, LR: 0.000088
2025-10-23 11:34:07,439 - INFO - Batch 6300, Loss: 2.6616, LR: 0.000088
2025-10-23 11:34:19,831 - INFO - Batch 6400, Loss: 2.8146, LR: 0.000088
2025-10-23 11:34:26,012 - INFO - Epoch 7/30: Train Loss: 2.6684, Val Loss: 2.5078, LR: 0.000088
2025-10-23 11:34:26,268 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 11:34:26,400 - INFO - Batch 0, Loss: 2.7339, LR: 0.000088
2025-10-23 11:34:38,792 - INFO - Batch 100, Loss: 2.4291, LR: 0.000088
2025-10-23 11:34:51,175 - INFO - Batch 200, Loss: 2.3498, LR: 0.000088
2025-10-23 11:35:03,589 - INFO - Batch 300, Loss: 2.7590, LR: 0.000088
2025-10-23 11:35:15,983 - INFO - Batch 400, Loss: 2.6674, LR: 0.000088
2025-10-23 11:35:28,364 - INFO - Batch 500, Loss: 2.6298, LR: 0.000088
2025-10-23 11:35:40,756 - INFO - Batch 600, Loss: 2.4952, LR: 0.000088
2025-10-23 11:35:53,122 - INFO - Batch 700, Loss: 2.5071, LR: 0.000088
2025-10-23 11:36:05,501 - INFO - Batch 800, Loss: 2.7378, LR: 0.000088
2025-10-23 11:36:17,850 - INFO - Batch 900, Loss: 2.7417, LR: 0.000088
2025-10-23 11:36:30,193 - INFO - Batch 1000, Loss: 2.6397, LR: 0.000087
2025-10-23 11:36:42,565 - INFO - Batch 1100, Loss: 2.7879, LR: 0.000087
2025-10-23 11:36:54,941 - INFO - Batch 1200, Loss: 2.5891, LR: 0.000087
2025-10-23 11:37:07,286 - INFO - Batch 1300, Loss: 2.8189, LR: 0.000087
2025-10-23 11:37:19,677 - INFO - Batch 1400, Loss: 2.6036, LR: 0.000087
2025-10-23 11:37:32,300 - INFO - Batch 1500, Loss: 2.6973, LR: 0.000087
2025-10-23 11:37:44,768 - INFO - Batch 1600, Loss: 2.5907, LR: 0.000087
2025-10-23 11:37:57,085 - INFO - Batch 1700, Loss: 2.4674, LR: 0.000087
2025-10-23 11:38:09,377 - INFO - Batch 1800, Loss: 2.6957, LR: 0.000087
2025-10-23 11:38:21,655 - INFO - Batch 1900, Loss: 2.4010, LR: 0.000087
2025-10-23 11:38:33,931 - INFO - Batch 2000, Loss: 2.4985, LR: 0.000087
2025-10-23 11:38:46,193 - INFO - Batch 2100, Loss: 2.3295, LR: 0.000087
2025-10-23 11:38:58,455 - INFO - Batch 2200, Loss: 2.4072, LR: 0.000087
2025-10-23 11:39:10,705 - INFO - Batch 2300, Loss: 2.5964, LR: 0.000087
2025-10-23 11:39:22,961 - INFO - Batch 2400, Loss: 2.3234, LR: 0.000087
2025-10-23 11:39:35,247 - INFO - Batch 2500, Loss: 2.5247, LR: 0.000087
2025-10-23 11:39:47,576 - INFO - Batch 2600, Loss: 2.5989, LR: 0.000087
2025-10-23 11:39:59,900 - INFO - Batch 2700, Loss: 2.5780, LR: 0.000087
2025-10-23 11:40:12,210 - INFO - Batch 2800, Loss: 2.3460, LR: 0.000086
2025-10-23 11:40:24,493 - INFO - Batch 2900, Loss: 2.5906, LR: 0.000086
2025-10-23 11:40:36,774 - INFO - Batch 3000, Loss: 2.2447, LR: 0.000086
2025-10-23 11:40:49,079 - INFO - Batch 3100, Loss: 2.7544, LR: 0.000086
2025-10-23 11:41:01,398 - INFO - Batch 3200, Loss: 2.5711, LR: 0.000086
2025-10-23 11:41:13,706 - INFO - Batch 3300, Loss: 2.6583, LR: 0.000086
2025-10-23 11:41:25,965 - INFO - Batch 3400, Loss: 2.7720, LR: 0.000086
2025-10-23 11:41:38,289 - INFO - Batch 3500, Loss: 2.4652, LR: 0.000086
2025-10-23 11:41:50,624 - INFO - Batch 3600, Loss: 2.7080, LR: 0.000086
2025-10-23 11:42:02,900 - INFO - Batch 3700, Loss: 2.7286, LR: 0.000086
2025-10-23 11:42:15,226 - INFO - Batch 3800, Loss: 2.3832, LR: 0.000086
2025-10-23 11:42:27,622 - INFO - Batch 3900, Loss: 2.7506, LR: 0.000086
2025-10-23 11:42:40,113 - INFO - Batch 4000, Loss: 2.6130, LR: 0.000086
2025-10-23 11:42:52,628 - INFO - Batch 4100, Loss: 2.3550, LR: 0.000086
2025-10-23 11:43:05,041 - INFO - Batch 4200, Loss: 2.7836, LR: 0.000086
2025-10-23 11:43:17,493 - INFO - Batch 4300, Loss: 2.5445, LR: 0.000086
2025-10-23 11:43:29,942 - INFO - Batch 4400, Loss: 2.4679, LR: 0.000086
2025-10-23 11:43:42,296 - INFO - Batch 4500, Loss: 2.6262, LR: 0.000085
2025-10-23 11:43:54,648 - INFO - Batch 4600, Loss: 2.4055, LR: 0.000085
2025-10-23 11:44:07,003 - INFO - Batch 4700, Loss: 2.6919, LR: 0.000085
2025-10-23 11:44:19,350 - INFO - Batch 4800, Loss: 2.8680, LR: 0.000085
2025-10-23 11:44:31,679 - INFO - Batch 4900, Loss: 2.4690, LR: 0.000085
2025-10-23 11:44:44,010 - INFO - Batch 5000, Loss: 2.7610, LR: 0.000085
2025-10-23 11:44:56,341 - INFO - Batch 5100, Loss: 2.6177, LR: 0.000085
2025-10-23 11:45:08,731 - INFO - Batch 5200, Loss: 2.6136, LR: 0.000085
2025-10-23 11:45:21,128 - INFO - Batch 5300, Loss: 2.7199, LR: 0.000085
2025-10-23 11:45:33,503 - INFO - Batch 5400, Loss: 2.3497, LR: 0.000085
2025-10-23 11:45:45,869 - INFO - Batch 5500, Loss: 2.4871, LR: 0.000085
2025-10-23 11:45:58,217 - INFO - Batch 5600, Loss: 2.4667, LR: 0.000085
2025-10-23 11:46:10,602 - INFO - Batch 5700, Loss: 2.6404, LR: 0.000085
2025-10-23 11:46:22,960 - INFO - Batch 5800, Loss: 2.8518, LR: 0.000085
2025-10-23 11:46:35,331 - INFO - Batch 5900, Loss: 2.7060, LR: 0.000085
2025-10-23 11:46:47,700 - INFO - Batch 6000, Loss: 2.1230, LR: 0.000085
2025-10-23 11:47:00,051 - INFO - Batch 6100, Loss: 2.7466, LR: 0.000085
2025-10-23 11:47:12,399 - INFO - Batch 6200, Loss: 2.3363, LR: 0.000084
2025-10-23 11:47:24,818 - INFO - Batch 6300, Loss: 2.5834, LR: 0.000084
2025-10-23 11:47:37,178 - INFO - Batch 6400, Loss: 2.6014, LR: 0.000084
2025-10-23 11:47:43,355 - INFO - Epoch 8/30: Train Loss: 2.5953, Val Loss: 2.4452, LR: 0.000084
2025-10-23 11:47:43,627 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 11:47:43,757 - INFO - Batch 0, Loss: 2.4626, LR: 0.000084
2025-10-23 11:47:56,182 - INFO - Batch 100, Loss: 2.6216, LR: 0.000084
2025-10-23 11:48:08,571 - INFO - Batch 200, Loss: 2.8970, LR: 0.000084
2025-10-23 11:48:20,970 - INFO - Batch 300, Loss: 2.5910, LR: 0.000084
2025-10-23 11:48:33,355 - INFO - Batch 400, Loss: 2.3874, LR: 0.000084
2025-10-23 11:48:45,728 - INFO - Batch 500, Loss: 2.4273, LR: 0.000084
2025-10-23 11:48:58,130 - INFO - Batch 600, Loss: 2.5851, LR: 0.000084
2025-10-23 11:49:10,553 - INFO - Batch 700, Loss: 2.5262, LR: 0.000084
2025-10-23 11:49:22,936 - INFO - Batch 800, Loss: 2.4731, LR: 0.000084
2025-10-23 11:49:35,331 - INFO - Batch 900, Loss: 2.4816, LR: 0.000084
2025-10-23 11:49:47,688 - INFO - Batch 1000, Loss: 2.3046, LR: 0.000084
2025-10-23 11:50:00,040 - INFO - Batch 1100, Loss: 2.5662, LR: 0.000084
2025-10-23 11:50:12,460 - INFO - Batch 1200, Loss: 2.7514, LR: 0.000084
2025-10-23 11:50:24,817 - INFO - Batch 1300, Loss: 2.6524, LR: 0.000084
2025-10-23 11:50:37,211 - INFO - Batch 1400, Loss: 2.2822, LR: 0.000083
2025-10-23 11:50:49,577 - INFO - Batch 1500, Loss: 2.6469, LR: 0.000083
2025-10-23 11:51:01,964 - INFO - Batch 1600, Loss: 2.6582, LR: 0.000083
2025-10-23 11:51:14,333 - INFO - Batch 1700, Loss: 2.5234, LR: 0.000083
2025-10-23 11:51:26,679 - INFO - Batch 1800, Loss: 2.4211, LR: 0.000083
2025-10-23 11:51:39,069 - INFO - Batch 1900, Loss: 2.6806, LR: 0.000083
2025-10-23 11:51:51,441 - INFO - Batch 2000, Loss: 2.7449, LR: 0.000083
2025-10-23 11:52:03,814 - INFO - Batch 2100, Loss: 2.4400, LR: 0.000083
2025-10-23 11:52:16,203 - INFO - Batch 2200, Loss: 2.5903, LR: 0.000083
2025-10-23 11:52:28,669 - INFO - Batch 2300, Loss: 2.4292, LR: 0.000083
2025-10-23 11:52:41,191 - INFO - Batch 2400, Loss: 2.5978, LR: 0.000083
2025-10-23 11:52:53,698 - INFO - Batch 2500, Loss: 2.6264, LR: 0.000083
2025-10-23 11:53:06,073 - INFO - Batch 2600, Loss: 2.4774, LR: 0.000083
2025-10-23 11:53:18,410 - INFO - Batch 2700, Loss: 2.6224, LR: 0.000083
2025-10-23 11:53:30,697 - INFO - Batch 2800, Loss: 2.3864, LR: 0.000083
2025-10-23 11:53:42,964 - INFO - Batch 2900, Loss: 2.4966, LR: 0.000083
2025-10-23 11:53:55,469 - INFO - Batch 3000, Loss: 2.5900, LR: 0.000083
2025-10-23 11:54:07,797 - INFO - Batch 3100, Loss: 2.3554, LR: 0.000082
2025-10-23 11:54:20,075 - INFO - Batch 3200, Loss: 2.7093, LR: 0.000082
2025-10-23 11:54:32,344 - INFO - Batch 3300, Loss: 2.5980, LR: 0.000082
2025-10-23 11:54:44,626 - INFO - Batch 3400, Loss: 2.4645, LR: 0.000082
2025-10-23 11:54:56,933 - INFO - Batch 3500, Loss: 2.6762, LR: 0.000082
2025-10-23 11:55:09,233 - INFO - Batch 3600, Loss: 2.5251, LR: 0.000082
2025-10-23 11:55:21,530 - INFO - Batch 3700, Loss: 2.6583, LR: 0.000082
2025-10-23 11:55:33,811 - INFO - Batch 3800, Loss: 2.6794, LR: 0.000082
2025-10-23 11:55:46,100 - INFO - Batch 3900, Loss: 2.4038, LR: 0.000082
2025-10-23 11:55:58,386 - INFO - Batch 4000, Loss: 2.7015, LR: 0.000082
2025-10-23 11:56:10,690 - INFO - Batch 4100, Loss: 2.5947, LR: 0.000082
2025-10-23 11:56:23,040 - INFO - Batch 4200, Loss: 2.4167, LR: 0.000082
2025-10-23 11:56:35,413 - INFO - Batch 4300, Loss: 2.3381, LR: 0.000082
2025-10-23 11:56:47,781 - INFO - Batch 4400, Loss: 2.6231, LR: 0.000082
2025-10-23 11:57:00,105 - INFO - Batch 4500, Loss: 2.5487, LR: 0.000082
2025-10-23 11:57:12,465 - INFO - Batch 4600, Loss: 2.7589, LR: 0.000081
2025-10-23 11:57:24,873 - INFO - Batch 4700, Loss: 2.8762, LR: 0.000081
2025-10-23 11:57:37,219 - INFO - Batch 4800, Loss: 2.5732, LR: 0.000081
2025-10-23 11:57:49,606 - INFO - Batch 4900, Loss: 2.3131, LR: 0.000081
2025-10-23 11:58:01,964 - INFO - Batch 5000, Loss: 2.7853, LR: 0.000081
2025-10-23 11:58:14,289 - INFO - Batch 5100, Loss: 2.3042, LR: 0.000081
2025-10-23 11:58:26,614 - INFO - Batch 5200, Loss: 2.2098, LR: 0.000081
2025-10-23 11:58:38,934 - INFO - Batch 5300, Loss: 2.4196, LR: 0.000081
2025-10-23 11:58:51,273 - INFO - Batch 5400, Loss: 2.5729, LR: 0.000081
2025-10-23 11:59:03,634 - INFO - Batch 5500, Loss: 2.7357, LR: 0.000081
2025-10-23 11:59:16,049 - INFO - Batch 5600, Loss: 2.6567, LR: 0.000081
2025-10-23 11:59:28,432 - INFO - Batch 5700, Loss: 2.6651, LR: 0.000081
2025-10-23 11:59:40,805 - INFO - Batch 5800, Loss: 2.6851, LR: 0.000081
2025-10-23 11:59:53,151 - INFO - Batch 5900, Loss: 2.6852, LR: 0.000081
2025-10-23 12:00:05,522 - INFO - Batch 6000, Loss: 2.3841, LR: 0.000081
2025-10-23 12:00:17,876 - INFO - Batch 6100, Loss: 2.2976, LR: 0.000081
2025-10-23 12:00:30,237 - INFO - Batch 6200, Loss: 2.6432, LR: 0.000080
2025-10-23 12:00:42,584 - INFO - Batch 6300, Loss: 2.6057, LR: 0.000080
2025-10-23 12:00:54,924 - INFO - Batch 6400, Loss: 2.5269, LR: 0.000080
2025-10-23 12:01:01,080 - INFO - Epoch 9/30: Train Loss: 2.5412, Val Loss: 2.3972, LR: 0.000080
2025-10-23 12:01:01,375 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 12:01:01,503 - INFO - Batch 0, Loss: 2.5745, LR: 0.000080
2025-10-23 12:01:13,804 - INFO - Batch 100, Loss: 2.4633, LR: 0.000080
2025-10-23 12:01:26,101 - INFO - Batch 200, Loss: 2.5067, LR: 0.000080
2025-10-23 12:01:38,406 - INFO - Batch 300, Loss: 2.6529, LR: 0.000080
2025-10-23 12:01:50,717 - INFO - Batch 400, Loss: 2.6311, LR: 0.000080
2025-10-23 12:02:03,035 - INFO - Batch 500, Loss: 2.5763, LR: 0.000080
2025-10-23 12:02:15,346 - INFO - Batch 600, Loss: 2.6787, LR: 0.000080
2025-10-23 12:02:27,765 - INFO - Batch 700, Loss: 2.2919, LR: 0.000080
2025-10-23 12:02:40,058 - INFO - Batch 800, Loss: 2.4028, LR: 0.000080
2025-10-23 12:02:52,361 - INFO - Batch 900, Loss: 2.6765, LR: 0.000080
2025-10-23 12:03:04,686 - INFO - Batch 1000, Loss: 2.3917, LR: 0.000080
2025-10-23 12:03:16,985 - INFO - Batch 1100, Loss: 2.3385, LR: 0.000080
2025-10-23 12:03:29,281 - INFO - Batch 1200, Loss: 2.5546, LR: 0.000080
2025-10-23 12:03:41,573 - INFO - Batch 1300, Loss: 2.5759, LR: 0.000079
2025-10-23 12:03:53,875 - INFO - Batch 1400, Loss: 2.5993, LR: 0.000079
2025-10-23 12:04:06,125 - INFO - Batch 1500, Loss: 2.4558, LR: 0.000079
2025-10-23 12:04:18,367 - INFO - Batch 1600, Loss: 2.3443, LR: 0.000079
2025-10-23 12:04:30,609 - INFO - Batch 1700, Loss: 2.5333, LR: 0.000079
2025-10-23 12:04:42,874 - INFO - Batch 1800, Loss: 2.4643, LR: 0.000079
2025-10-23 12:04:55,124 - INFO - Batch 1900, Loss: 2.6669, LR: 0.000079
2025-10-23 12:05:07,375 - INFO - Batch 2000, Loss: 2.6059, LR: 0.000079
2025-10-23 12:05:19,644 - INFO - Batch 2100, Loss: 2.4580, LR: 0.000079
2025-10-23 12:05:31,942 - INFO - Batch 2200, Loss: 2.3222, LR: 0.000079
2025-10-23 12:05:44,331 - INFO - Batch 2300, Loss: 2.3702, LR: 0.000079
2025-10-23 12:05:56,865 - INFO - Batch 2400, Loss: 2.3523, LR: 0.000079
2025-10-23 12:06:09,212 - INFO - Batch 2500, Loss: 2.5713, LR: 0.000079
2025-10-23 12:06:21,497 - INFO - Batch 2600, Loss: 2.5021, LR: 0.000079
2025-10-23 12:06:33,893 - INFO - Batch 2700, Loss: 2.4874, LR: 0.000079
2025-10-23 12:06:46,162 - INFO - Batch 2800, Loss: 2.5783, LR: 0.000078
2025-10-23 12:06:58,459 - INFO - Batch 2900, Loss: 2.3677, LR: 0.000078
2025-10-23 12:07:10,780 - INFO - Batch 3000, Loss: 2.4650, LR: 0.000078
2025-10-23 12:07:23,133 - INFO - Batch 3100, Loss: 2.6197, LR: 0.000078
2025-10-23 12:07:35,447 - INFO - Batch 3200, Loss: 2.3845, LR: 0.000078
2025-10-23 12:07:47,798 - INFO - Batch 3300, Loss: 2.6488, LR: 0.000078
2025-10-23 12:08:00,142 - INFO - Batch 3400, Loss: 2.6560, LR: 0.000078
2025-10-23 12:08:12,519 - INFO - Batch 3500, Loss: 2.5170, LR: 0.000078
2025-10-23 12:08:24,916 - INFO - Batch 3600, Loss: 2.4960, LR: 0.000078
2025-10-23 12:08:37,394 - INFO - Batch 3700, Loss: 2.3488, LR: 0.000078
2025-10-23 12:08:49,788 - INFO - Batch 3800, Loss: 2.4807, LR: 0.000078
2025-10-23 12:09:02,265 - INFO - Batch 3900, Loss: 2.4533, LR: 0.000078
2025-10-23 12:09:14,704 - INFO - Batch 4000, Loss: 2.4415, LR: 0.000078
2025-10-23 12:09:27,142 - INFO - Batch 4100, Loss: 2.4170, LR: 0.000078
2025-10-23 12:09:39,577 - INFO - Batch 4200, Loss: 2.6532, LR: 0.000077
2025-10-23 12:09:52,038 - INFO - Batch 4300, Loss: 2.3720, LR: 0.000077
2025-10-23 12:10:04,418 - INFO - Batch 4400, Loss: 2.2997, LR: 0.000077
2025-10-23 12:10:16,836 - INFO - Batch 4500, Loss: 2.5019, LR: 0.000077
2025-10-23 12:10:29,293 - INFO - Batch 4600, Loss: 2.4277, LR: 0.000077
2025-10-23 12:10:41,746 - INFO - Batch 4700, Loss: 2.4773, LR: 0.000077
2025-10-23 12:10:54,239 - INFO - Batch 4800, Loss: 2.5841, LR: 0.000077
2025-10-23 12:11:06,713 - INFO - Batch 4900, Loss: 2.5221, LR: 0.000077
2025-10-23 12:11:19,262 - INFO - Batch 5000, Loss: 2.4592, LR: 0.000077
2025-10-23 12:11:31,731 - INFO - Batch 5100, Loss: 2.7749, LR: 0.000077
2025-10-23 12:11:44,183 - INFO - Batch 5200, Loss: 2.5280, LR: 0.000077
2025-10-23 12:11:56,636 - INFO - Batch 5300, Loss: 2.3843, LR: 0.000077
2025-10-23 12:12:09,093 - INFO - Batch 5400, Loss: 2.6199, LR: 0.000077
2025-10-23 12:12:21,607 - INFO - Batch 5500, Loss: 2.5566, LR: 0.000077
2025-10-23 12:12:34,030 - INFO - Batch 5600, Loss: 2.6281, LR: 0.000077
2025-10-23 12:12:46,598 - INFO - Batch 5700, Loss: 2.3468, LR: 0.000076
2025-10-23 12:12:59,037 - INFO - Batch 5800, Loss: 2.5559, LR: 0.000076
2025-10-23 12:13:11,448 - INFO - Batch 5900, Loss: 2.4314, LR: 0.000076
2025-10-23 12:13:23,894 - INFO - Batch 6000, Loss: 2.3451, LR: 0.000076
2025-10-23 12:13:36,436 - INFO - Batch 6100, Loss: 2.3639, LR: 0.000076
2025-10-23 12:13:49,055 - INFO - Batch 6200, Loss: 2.5241, LR: 0.000076
2025-10-23 12:14:01,625 - INFO - Batch 6300, Loss: 2.3583, LR: 0.000076
2025-10-23 12:14:14,023 - INFO - Batch 6400, Loss: 2.5940, LR: 0.000076
2025-10-23 12:14:20,219 - INFO - Epoch 10/30: Train Loss: 2.4991, Val Loss: 2.3621, LR: 0.000076
2025-10-23 12:14:20,478 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 12:14:20,681 - INFO - 模型已保存到: ./checkpoints/checkpoint_epoch_10.pth
2025-10-23 12:14:20,810 - INFO - Batch 0, Loss: 2.3216, LR: 0.000076
2025-10-23 12:14:33,382 - INFO - Batch 100, Loss: 2.4449, LR: 0.000076
2025-10-23 12:14:45,924 - INFO - Batch 200, Loss: 2.5358, LR: 0.000076
2025-10-23 12:14:58,456 - INFO - Batch 300, Loss: 2.2983, LR: 0.000076
2025-10-23 12:15:11,025 - INFO - Batch 400, Loss: 2.5947, LR: 0.000076
2025-10-23 12:15:23,398 - INFO - Batch 500, Loss: 2.3828, LR: 0.000076
2025-10-23 12:15:35,817 - INFO - Batch 600, Loss: 2.0217, LR: 0.000076
2025-10-23 12:15:48,301 - INFO - Batch 700, Loss: 2.5493, LR: 0.000075
2025-10-23 12:16:00,755 - INFO - Batch 800, Loss: 2.6588, LR: 0.000075
2025-10-23 12:16:13,221 - INFO - Batch 900, Loss: 2.2074, LR: 0.000075
2025-10-23 12:16:25,702 - INFO - Batch 1000, Loss: 2.2606, LR: 0.000075
2025-10-23 12:16:38,163 - INFO - Batch 1100, Loss: 2.4182, LR: 0.000075
2025-10-23 12:16:50,633 - INFO - Batch 1200, Loss: 2.4593, LR: 0.000075
2025-10-23 12:17:03,084 - INFO - Batch 1300, Loss: 2.4782, LR: 0.000075
2025-10-23 12:17:15,516 - INFO - Batch 1400, Loss: 2.5033, LR: 0.000075
2025-10-23 12:17:27,961 - INFO - Batch 1500, Loss: 2.2496, LR: 0.000075
2025-10-23 12:17:40,312 - INFO - Batch 1600, Loss: 2.5360, LR: 0.000075
2025-10-23 12:17:52,702 - INFO - Batch 1700, Loss: 2.4156, LR: 0.000075
2025-10-23 12:18:05,057 - INFO - Batch 1800, Loss: 2.2223, LR: 0.000075
2025-10-23 12:18:17,449 - INFO - Batch 1900, Loss: 2.6659, LR: 0.000075
2025-10-23 12:18:29,821 - INFO - Batch 2000, Loss: 2.5782, LR: 0.000075
2025-10-23 12:18:42,381 - INFO - Batch 2100, Loss: 2.4507, LR: 0.000074
2025-10-23 12:18:54,888 - INFO - Batch 2200, Loss: 2.4595, LR: 0.000074
2025-10-23 12:19:07,456 - INFO - Batch 2300, Loss: 2.7485, LR: 0.000074
2025-10-23 12:19:20,069 - INFO - Batch 2400, Loss: 2.3436, LR: 0.000074
2025-10-23 12:19:32,689 - INFO - Batch 2500, Loss: 2.3521, LR: 0.000074
2025-10-23 12:19:45,300 - INFO - Batch 2600, Loss: 2.4951, LR: 0.000074
2025-10-23 12:19:57,912 - INFO - Batch 2700, Loss: 2.2008, LR: 0.000074
2025-10-23 12:20:10,546 - INFO - Batch 2800, Loss: 2.4991, LR: 0.000074
2025-10-23 12:20:23,164 - INFO - Batch 2900, Loss: 2.6593, LR: 0.000074
2025-10-23 12:20:35,781 - INFO - Batch 3000, Loss: 2.7465, LR: 0.000074
2025-10-23 12:20:48,423 - INFO - Batch 3100, Loss: 2.1868, LR: 0.000074
2025-10-23 12:21:01,066 - INFO - Batch 3200, Loss: 2.4239, LR: 0.000074
2025-10-23 12:21:13,621 - INFO - Batch 3300, Loss: 2.4744, LR: 0.000074
2025-10-23 12:21:26,113 - INFO - Batch 3400, Loss: 2.5581, LR: 0.000074
2025-10-23 12:21:38,570 - INFO - Batch 3500, Loss: 2.3855, LR: 0.000073
2025-10-23 12:21:51,045 - INFO - Batch 3600, Loss: 3.0465, LR: 0.000073
2025-10-23 12:22:03,547 - INFO - Batch 3700, Loss: 2.7145, LR: 0.000073
2025-10-23 12:22:16,048 - INFO - Batch 3800, Loss: 2.6871, LR: 0.000073
2025-10-23 12:22:28,554 - INFO - Batch 3900, Loss: 2.5003, LR: 0.000073
2025-10-23 12:22:40,988 - INFO - Batch 4000, Loss: 2.5915, LR: 0.000073
2025-10-23 12:22:53,401 - INFO - Batch 4100, Loss: 2.4414, LR: 0.000073
2025-10-23 12:23:05,810 - INFO - Batch 4200, Loss: 2.4816, LR: 0.000073
2025-10-23 12:23:18,228 - INFO - Batch 4300, Loss: 2.2137, LR: 0.000073
2025-10-23 12:23:30,613 - INFO - Batch 4400, Loss: 2.3143, LR: 0.000073
2025-10-23 12:23:43,000 - INFO - Batch 4500, Loss: 2.3810, LR: 0.000073
2025-10-23 12:23:55,375 - INFO - Batch 4600, Loss: 2.2121, LR: 0.000073
2025-10-23 12:24:07,767 - INFO - Batch 4700, Loss: 2.5402, LR: 0.000073
2025-10-23 12:24:20,150 - INFO - Batch 4800, Loss: 2.7406, LR: 0.000072
2025-10-23 12:24:32,509 - INFO - Batch 4900, Loss: 2.4263, LR: 0.000072
2025-10-23 12:24:44,927 - INFO - Batch 5000, Loss: 2.4163, LR: 0.000072
2025-10-23 12:24:57,395 - INFO - Batch 5100, Loss: 2.1900, LR: 0.000072
2025-10-23 12:25:09,840 - INFO - Batch 5200, Loss: 2.5177, LR: 0.000072
2025-10-23 12:25:22,237 - INFO - Batch 5300, Loss: 2.4595, LR: 0.000072
2025-10-23 12:25:34,622 - INFO - Batch 5400, Loss: 2.6874, LR: 0.000072
2025-10-23 12:25:46,977 - INFO - Batch 5500, Loss: 2.7101, LR: 0.000072
2025-10-23 12:25:59,322 - INFO - Batch 5600, Loss: 2.3800, LR: 0.000072
2025-10-23 12:26:11,669 - INFO - Batch 5700, Loss: 2.5765, LR: 0.000072
2025-10-23 12:26:24,029 - INFO - Batch 5800, Loss: 2.7524, LR: 0.000072
2025-10-23 12:26:36,506 - INFO - Batch 5900, Loss: 2.4853, LR: 0.000072
2025-10-23 12:26:49,084 - INFO - Batch 6000, Loss: 2.9458, LR: 0.000072
2025-10-23 12:27:01,664 - INFO - Batch 6100, Loss: 2.4932, LR: 0.000072
2025-10-23 12:27:14,236 - INFO - Batch 6200, Loss: 2.2492, LR: 0.000071
2025-10-23 12:27:26,825 - INFO - Batch 6300, Loss: 2.4895, LR: 0.000071
2025-10-23 12:27:39,417 - INFO - Batch 6400, Loss: 2.5089, LR: 0.000071
2025-10-23 12:27:45,676 - INFO - Epoch 11/30: Train Loss: 2.4628, Val Loss: 2.3323, LR: 0.000071
2025-10-23 12:27:45,936 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 12:27:46,066 - INFO - Batch 0, Loss: 1.9606, LR: 0.000071
2025-10-23 12:27:58,650 - INFO - Batch 100, Loss: 2.4287, LR: 0.000071
2025-10-23 12:28:11,234 - INFO - Batch 200, Loss: 2.4769, LR: 0.000071
2025-10-23 12:28:23,798 - INFO - Batch 300, Loss: 2.1858, LR: 0.000071
2025-10-23 12:28:36,205 - INFO - Batch 400, Loss: 2.3611, LR: 0.000071
2025-10-23 12:28:48,497 - INFO - Batch 500, Loss: 2.5072, LR: 0.000071
2025-10-23 12:29:00,780 - INFO - Batch 600, Loss: 2.3106, LR: 0.000071
2025-10-23 12:29:13,067 - INFO - Batch 700, Loss: 2.7275, LR: 0.000071
2025-10-23 12:29:25,349 - INFO - Batch 800, Loss: 2.3786, LR: 0.000071
2025-10-23 12:29:37,588 - INFO - Batch 900, Loss: 2.3043, LR: 0.000071
2025-10-23 12:29:49,832 - INFO - Batch 1000, Loss: 2.5324, LR: 0.000071
2025-10-23 12:30:02,085 - INFO - Batch 1100, Loss: 2.5219, LR: 0.000070
2025-10-23 12:30:14,519 - INFO - Batch 1200, Loss: 2.2144, LR: 0.000070
2025-10-23 12:30:27,050 - INFO - Batch 1300, Loss: 2.7419, LR: 0.000070
2025-10-23 12:30:39,418 - INFO - Batch 1400, Loss: 2.4982, LR: 0.000070
2025-10-23 12:30:51,744 - INFO - Batch 1500, Loss: 2.3811, LR: 0.000070
2025-10-23 12:31:04,058 - INFO - Batch 1600, Loss: 2.3543, LR: 0.000070
2025-10-23 12:31:16,357 - INFO - Batch 1700, Loss: 2.7755, LR: 0.000070
2025-10-23 12:31:28,711 - INFO - Batch 1800, Loss: 2.2438, LR: 0.000070
2025-10-23 12:31:41,069 - INFO - Batch 1900, Loss: 2.1903, LR: 0.000070
2025-10-23 12:31:53,407 - INFO - Batch 2000, Loss: 2.3206, LR: 0.000070
2025-10-23 12:32:05,793 - INFO - Batch 2100, Loss: 2.5429, LR: 0.000070
2025-10-23 12:32:18,167 - INFO - Batch 2200, Loss: 2.4793, LR: 0.000070
2025-10-23 12:32:30,535 - INFO - Batch 2300, Loss: 2.3526, LR: 0.000070
2025-10-23 12:32:42,908 - INFO - Batch 2400, Loss: 2.6382, LR: 0.000069
2025-10-23 12:32:55,273 - INFO - Batch 2500, Loss: 2.4694, LR: 0.000069
2025-10-23 12:33:07,646 - INFO - Batch 2600, Loss: 2.6442, LR: 0.000069
2025-10-23 12:33:20,048 - INFO - Batch 2700, Loss: 2.3892, LR: 0.000069
2025-10-23 12:33:32,441 - INFO - Batch 2800, Loss: 2.4024, LR: 0.000069
2025-10-23 12:33:44,841 - INFO - Batch 2900, Loss: 2.4489, LR: 0.000069
2025-10-23 12:33:57,245 - INFO - Batch 3000, Loss: 2.5572, LR: 0.000069
2025-10-23 12:34:09,640 - INFO - Batch 3100, Loss: 2.6472, LR: 0.000069
2025-10-23 12:34:22,006 - INFO - Batch 3200, Loss: 2.8740, LR: 0.000069
2025-10-23 12:34:34,397 - INFO - Batch 3300, Loss: 2.4979, LR: 0.000069
2025-10-23 12:34:46,805 - INFO - Batch 3400, Loss: 2.5289, LR: 0.000069
2025-10-23 12:34:59,251 - INFO - Batch 3500, Loss: 2.5076, LR: 0.000069
2025-10-23 12:35:11,910 - INFO - Batch 3600, Loss: 2.3881, LR: 0.000069
2025-10-23 12:35:24,248 - INFO - Batch 3700, Loss: 2.3462, LR: 0.000068
2025-10-23 12:35:36,633 - INFO - Batch 3800, Loss: 2.5897, LR: 0.000068
2025-10-23 12:35:49,029 - INFO - Batch 3900, Loss: 2.3166, LR: 0.000068
2025-10-23 12:36:01,380 - INFO - Batch 4000, Loss: 2.4537, LR: 0.000068
2025-10-23 12:36:13,822 - INFO - Batch 4100, Loss: 2.4880, LR: 0.000068
2025-10-23 12:36:26,203 - INFO - Batch 4200, Loss: 2.2980, LR: 0.000068
2025-10-23 12:36:38,597 - INFO - Batch 4300, Loss: 2.3305, LR: 0.000068
2025-10-23 12:36:51,001 - INFO - Batch 4400, Loss: 2.5555, LR: 0.000068
2025-10-23 12:37:03,373 - INFO - Batch 4500, Loss: 2.4962, LR: 0.000068
2025-10-23 12:37:15,847 - INFO - Batch 4600, Loss: 2.5053, LR: 0.000068
2025-10-23 12:37:28,333 - INFO - Batch 4700, Loss: 2.5621, LR: 0.000068
2025-10-23 12:37:40,756 - INFO - Batch 4800, Loss: 2.6577, LR: 0.000068
2025-10-23 12:37:53,153 - INFO - Batch 4900, Loss: 2.4425, LR: 0.000068
2025-10-23 12:38:05,562 - INFO - Batch 5000, Loss: 2.3389, LR: 0.000067
2025-10-23 12:38:18,023 - INFO - Batch 5100, Loss: 2.5850, LR: 0.000067
2025-10-23 12:38:30,442 - INFO - Batch 5200, Loss: 2.2334, LR: 0.000067
2025-10-23 12:38:42,858 - INFO - Batch 5300, Loss: 2.2326, LR: 0.000067
2025-10-23 12:38:55,270 - INFO - Batch 5400, Loss: 2.5846, LR: 0.000067
2025-10-23 12:39:07,676 - INFO - Batch 5500, Loss: 2.5485, LR: 0.000067
2025-10-23 12:39:20,073 - INFO - Batch 5600, Loss: 2.1697, LR: 0.000067
2025-10-23 12:39:32,441 - INFO - Batch 5700, Loss: 2.2301, LR: 0.000067
2025-10-23 12:39:44,795 - INFO - Batch 5800, Loss: 2.5657, LR: 0.000067
2025-10-23 12:39:57,142 - INFO - Batch 5900, Loss: 2.2930, LR: 0.000067
2025-10-23 12:40:09,484 - INFO - Batch 6000, Loss: 2.2353, LR: 0.000067
2025-10-23 12:40:21,816 - INFO - Batch 6100, Loss: 2.6336, LR: 0.000067
2025-10-23 12:40:34,133 - INFO - Batch 6200, Loss: 2.3746, LR: 0.000067
2025-10-23 12:40:46,444 - INFO - Batch 6300, Loss: 2.3952, LR: 0.000066
2025-10-23 12:40:58,791 - INFO - Batch 6400, Loss: 2.5638, LR: 0.000066
2025-10-23 12:41:04,944 - INFO - Epoch 12/30: Train Loss: 2.4334, Val Loss: 2.3069, LR: 0.000066
2025-10-23 12:41:05,206 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 12:41:05,336 - INFO - Batch 0, Loss: 2.4692, LR: 0.000066
2025-10-23 12:41:17,921 - INFO - Batch 100, Loss: 2.0926, LR: 0.000066
2025-10-23 12:41:30,475 - INFO - Batch 200, Loss: 2.4476, LR: 0.000066
2025-10-23 12:41:42,854 - INFO - Batch 300, Loss: 2.3405, LR: 0.000066
2025-10-23 12:41:55,122 - INFO - Batch 400, Loss: 2.2798, LR: 0.000066
2025-10-23 12:42:07,402 - INFO - Batch 500, Loss: 2.3889, LR: 0.000066
2025-10-23 12:42:19,768 - INFO - Batch 600, Loss: 2.5220, LR: 0.000066
2025-10-23 12:42:32,079 - INFO - Batch 700, Loss: 2.3115, LR: 0.000066
2025-10-23 12:42:44,387 - INFO - Batch 800, Loss: 2.0475, LR: 0.000066
2025-10-23 12:42:56,670 - INFO - Batch 900, Loss: 2.3590, LR: 0.000066
2025-10-23 12:43:09,081 - INFO - Batch 1000, Loss: 2.2931, LR: 0.000066
2025-10-23 12:43:21,379 - INFO - Batch 1100, Loss: 2.3602, LR: 0.000066
2025-10-23 12:43:33,661 - INFO - Batch 1200, Loss: 2.5436, LR: 0.000065
2025-10-23 12:43:45,984 - INFO - Batch 1300, Loss: 2.5758, LR: 0.000065
2025-10-23 12:43:58,286 - INFO - Batch 1400, Loss: 2.5541, LR: 0.000065
2025-10-23 12:44:10,603 - INFO - Batch 1500, Loss: 2.2015, LR: 0.000065
2025-10-23 12:44:22,928 - INFO - Batch 1600, Loss: 2.3991, LR: 0.000065
2025-10-23 12:44:35,234 - INFO - Batch 1700, Loss: 2.6625, LR: 0.000065
2025-10-23 12:44:47,567 - INFO - Batch 1800, Loss: 2.1137, LR: 0.000065
2025-10-23 12:44:59,936 - INFO - Batch 1900, Loss: 2.4321, LR: 0.000065
2025-10-23 12:45:12,300 - INFO - Batch 2000, Loss: 2.1548, LR: 0.000065
2025-10-23 12:45:24,802 - INFO - Batch 2100, Loss: 2.5385, LR: 0.000065
2025-10-23 12:45:37,324 - INFO - Batch 2200, Loss: 2.6240, LR: 0.000065
2025-10-23 12:45:49,863 - INFO - Batch 2300, Loss: 2.3887, LR: 0.000065
2025-10-23 12:46:02,394 - INFO - Batch 2400, Loss: 2.2768, LR: 0.000065
2025-10-23 12:46:14,941 - INFO - Batch 2500, Loss: 2.3828, LR: 0.000064
2025-10-23 12:46:27,574 - INFO - Batch 2600, Loss: 2.3265, LR: 0.000064
2025-10-23 12:46:40,217 - INFO - Batch 2700, Loss: 2.1405, LR: 0.000064
2025-10-23 12:46:52,804 - INFO - Batch 2800, Loss: 2.2570, LR: 0.000064
2025-10-23 12:47:05,301 - INFO - Batch 2900, Loss: 2.2129, LR: 0.000064
2025-10-23 12:47:17,818 - INFO - Batch 3000, Loss: 2.6241, LR: 0.000064
2025-10-23 12:47:30,351 - INFO - Batch 3100, Loss: 2.6178, LR: 0.000064
2025-10-23 12:47:42,874 - INFO - Batch 3200, Loss: 2.1216, LR: 0.000064
2025-10-23 12:47:55,430 - INFO - Batch 3300, Loss: 2.1530, LR: 0.000064
2025-10-23 12:48:07,976 - INFO - Batch 3400, Loss: 2.3796, LR: 0.000064
2025-10-23 12:48:20,515 - INFO - Batch 3500, Loss: 2.3989, LR: 0.000064
2025-10-23 12:48:33,053 - INFO - Batch 3600, Loss: 2.4380, LR: 0.000064
2025-10-23 12:48:45,606 - INFO - Batch 3700, Loss: 2.1684, LR: 0.000063
2025-10-23 12:48:58,145 - INFO - Batch 3800, Loss: 2.4012, LR: 0.000063
2025-10-23 12:49:10,678 - INFO - Batch 3900, Loss: 2.2154, LR: 0.000063
2025-10-23 12:49:23,221 - INFO - Batch 4000, Loss: 2.2950, LR: 0.000063
2025-10-23 12:49:35,759 - INFO - Batch 4100, Loss: 2.4063, LR: 0.000063
2025-10-23 12:49:48,255 - INFO - Batch 4200, Loss: 2.3034, LR: 0.000063
2025-10-23 12:50:00,729 - INFO - Batch 4300, Loss: 2.5090, LR: 0.000063
2025-10-23 12:50:13,204 - INFO - Batch 4400, Loss: 2.4536, LR: 0.000063
2025-10-23 12:50:25,675 - INFO - Batch 4500, Loss: 2.2617, LR: 0.000063
2025-10-23 12:50:38,151 - INFO - Batch 4600, Loss: 2.1682, LR: 0.000063
2025-10-23 12:50:50,619 - INFO - Batch 4700, Loss: 2.2676, LR: 0.000063
2025-10-23 12:51:03,114 - INFO - Batch 4800, Loss: 2.3904, LR: 0.000063
2025-10-23 12:51:15,589 - INFO - Batch 4900, Loss: 2.3057, LR: 0.000063
2025-10-23 12:51:28,091 - INFO - Batch 5000, Loss: 2.2389, LR: 0.000062
2025-10-23 12:51:40,414 - INFO - Batch 5100, Loss: 2.1614, LR: 0.000062
2025-10-23 12:51:52,729 - INFO - Batch 5200, Loss: 2.5235, LR: 0.000062
2025-10-23 12:52:05,335 - INFO - Batch 5300, Loss: 2.2205, LR: 0.000062
2025-10-23 12:52:17,817 - INFO - Batch 5400, Loss: 2.2010, LR: 0.000062
2025-10-23 12:52:30,291 - INFO - Batch 5500, Loss: 2.2034, LR: 0.000062
2025-10-23 12:52:42,587 - INFO - Batch 5600, Loss: 2.3015, LR: 0.000062
2025-10-23 12:52:54,979 - INFO - Batch 5700, Loss: 2.5245, LR: 0.000062
2025-10-23 12:53:07,353 - INFO - Batch 5800, Loss: 2.3361, LR: 0.000062
2025-10-23 12:53:19,645 - INFO - Batch 5900, Loss: 2.1024, LR: 0.000062
2025-10-23 12:53:32,002 - INFO - Batch 6000, Loss: 2.4396, LR: 0.000062
2025-10-23 12:53:44,421 - INFO - Batch 6100, Loss: 2.7522, LR: 0.000062
2025-10-23 12:53:56,905 - INFO - Batch 6200, Loss: 2.3165, LR: 0.000061
2025-10-23 12:54:09,215 - INFO - Batch 6300, Loss: 2.5605, LR: 0.000061
2025-10-23 12:54:21,508 - INFO - Batch 6400, Loss: 2.3814, LR: 0.000061
2025-10-23 12:54:27,641 - INFO - Epoch 13/30: Train Loss: 2.4082, Val Loss: 2.2914, LR: 0.000061
2025-10-23 12:54:27,899 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 12:54:28,027 - INFO - Batch 0, Loss: 2.4264, LR: 0.000061
2025-10-23 12:54:40,349 - INFO - Batch 100, Loss: 2.3280, LR: 0.000061
2025-10-23 12:54:52,600 - INFO - Batch 200, Loss: 2.1788, LR: 0.000061
2025-10-23 12:55:04,865 - INFO - Batch 300, Loss: 2.5453, LR: 0.000061
2025-10-23 12:55:17,373 - INFO - Batch 400, Loss: 2.3530, LR: 0.000061
2025-10-23 12:55:29,676 - INFO - Batch 500, Loss: 2.3852, LR: 0.000061
2025-10-23 12:55:41,946 - INFO - Batch 600, Loss: 2.4255, LR: 0.000061
2025-10-23 12:55:54,200 - INFO - Batch 700, Loss: 2.2728, LR: 0.000061
2025-10-23 12:56:06,458 - INFO - Batch 800, Loss: 2.1705, LR: 0.000061
2025-10-23 12:56:18,887 - INFO - Batch 900, Loss: 2.4283, LR: 0.000061
2025-10-23 12:56:31,344 - INFO - Batch 1000, Loss: 2.4834, LR: 0.000061
2025-10-23 12:56:43,629 - INFO - Batch 1100, Loss: 2.3098, LR: 0.000060
2025-10-23 12:56:55,949 - INFO - Batch 1200, Loss: 2.0673, LR: 0.000060
2025-10-23 12:57:08,313 - INFO - Batch 1300, Loss: 2.0514, LR: 0.000060
2025-10-23 12:57:20,749 - INFO - Batch 1400, Loss: 2.5855, LR: 0.000060
2025-10-23 12:57:33,164 - INFO - Batch 1500, Loss: 2.5372, LR: 0.000060
2025-10-23 12:57:45,627 - INFO - Batch 1600, Loss: 2.6310, LR: 0.000060
2025-10-23 12:57:58,198 - INFO - Batch 1700, Loss: 2.6134, LR: 0.000060
2025-10-23 12:58:10,779 - INFO - Batch 1800, Loss: 2.3501, LR: 0.000060
2025-10-23 12:58:23,313 - INFO - Batch 1900, Loss: 2.2964, LR: 0.000060
2025-10-23 12:58:35,814 - INFO - Batch 2000, Loss: 2.3879, LR: 0.000060
2025-10-23 12:58:48,428 - INFO - Batch 2100, Loss: 2.2913, LR: 0.000060
2025-10-23 12:59:00,879 - INFO - Batch 2200, Loss: 2.3674, LR: 0.000060
2025-10-23 12:59:13,358 - INFO - Batch 2300, Loss: 2.3556, LR: 0.000059
2025-10-23 12:59:26,036 - INFO - Batch 2400, Loss: 2.3833, LR: 0.000059
2025-10-23 12:59:38,473 - INFO - Batch 2500, Loss: 2.1956, LR: 0.000059
2025-10-23 12:59:50,952 - INFO - Batch 2600, Loss: 2.5788, LR: 0.000059
2025-10-23 13:00:03,393 - INFO - Batch 2700, Loss: 2.3686, LR: 0.000059
2025-10-23 13:00:15,793 - INFO - Batch 2800, Loss: 2.2476, LR: 0.000059
2025-10-23 13:00:28,171 - INFO - Batch 2900, Loss: 2.3274, LR: 0.000059
2025-10-23 13:00:40,569 - INFO - Batch 3000, Loss: 2.4081, LR: 0.000059
2025-10-23 13:00:52,984 - INFO - Batch 3100, Loss: 2.3594, LR: 0.000059
2025-10-23 13:01:05,387 - INFO - Batch 3200, Loss: 2.1931, LR: 0.000059
2025-10-23 13:01:17,791 - INFO - Batch 3300, Loss: 2.2856, LR: 0.000059
2025-10-23 13:01:30,186 - INFO - Batch 3400, Loss: 2.4243, LR: 0.000059
2025-10-23 13:01:42,541 - INFO - Batch 3500, Loss: 2.3280, LR: 0.000058
2025-10-23 13:01:54,907 - INFO - Batch 3600, Loss: 2.4385, LR: 0.000058
2025-10-23 13:02:07,280 - INFO - Batch 3700, Loss: 2.4016, LR: 0.000058
2025-10-23 13:02:19,774 - INFO - Batch 3800, Loss: 2.3632, LR: 0.000058
2025-10-23 13:02:32,275 - INFO - Batch 3900, Loss: 2.2341, LR: 0.000058
2025-10-23 13:02:44,867 - INFO - Batch 4000, Loss: 2.4967, LR: 0.000058
2025-10-23 13:02:57,334 - INFO - Batch 4100, Loss: 2.0825, LR: 0.000058
2025-10-23 13:03:09,871 - INFO - Batch 4200, Loss: 2.3870, LR: 0.000058
2025-10-23 13:03:22,441 - INFO - Batch 4300, Loss: 2.4099, LR: 0.000058
2025-10-23 13:03:34,946 - INFO - Batch 4400, Loss: 2.4878, LR: 0.000058
2025-10-23 13:03:47,552 - INFO - Batch 4500, Loss: 2.3907, LR: 0.000058
2025-10-23 13:04:00,032 - INFO - Batch 4600, Loss: 2.4500, LR: 0.000058
2025-10-23 13:04:12,542 - INFO - Batch 4700, Loss: 2.1781, LR: 0.000058
2025-10-23 13:04:25,122 - INFO - Batch 4800, Loss: 2.4104, LR: 0.000057
2025-10-23 13:04:37,629 - INFO - Batch 4900, Loss: 2.1978, LR: 0.000057
2025-10-23 13:04:50,162 - INFO - Batch 5000, Loss: 2.5893, LR: 0.000057
2025-10-23 13:05:02,700 - INFO - Batch 5100, Loss: 2.5518, LR: 0.000057
2025-10-23 13:05:15,267 - INFO - Batch 5200, Loss: 2.5877, LR: 0.000057
2025-10-23 13:05:27,769 - INFO - Batch 5300, Loss: 2.4109, LR: 0.000057
2025-10-23 13:05:40,274 - INFO - Batch 5400, Loss: 2.4084, LR: 0.000057
2025-10-23 13:05:52,773 - INFO - Batch 5500, Loss: 2.5240, LR: 0.000057
2025-10-23 13:06:05,287 - INFO - Batch 5600, Loss: 2.3808, LR: 0.000057
2025-10-23 13:06:17,788 - INFO - Batch 5700, Loss: 2.5674, LR: 0.000057
2025-10-23 13:06:30,297 - INFO - Batch 5800, Loss: 2.3912, LR: 0.000057
2025-10-23 13:06:42,823 - INFO - Batch 5900, Loss: 2.3884, LR: 0.000057
2025-10-23 13:06:55,354 - INFO - Batch 6000, Loss: 2.2239, LR: 0.000056
2025-10-23 13:07:07,835 - INFO - Batch 6100, Loss: 2.2470, LR: 0.000056
2025-10-23 13:07:20,316 - INFO - Batch 6200, Loss: 2.3161, LR: 0.000056
2025-10-23 13:07:32,599 - INFO - Batch 6300, Loss: 2.3584, LR: 0.000056
2025-10-23 13:07:44,926 - INFO - Batch 6400, Loss: 2.3077, LR: 0.000056
2025-10-23 13:07:51,095 - INFO - Epoch 14/30: Train Loss: 2.3865, Val Loss: 2.2700, LR: 0.000056
2025-10-23 13:07:51,366 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 13:07:51,494 - INFO - Batch 0, Loss: 2.4315, LR: 0.000056
2025-10-23 13:08:03,809 - INFO - Batch 100, Loss: 2.2387, LR: 0.000056
2025-10-23 13:08:16,111 - INFO - Batch 200, Loss: 2.4088, LR: 0.000056
2025-10-23 13:08:28,422 - INFO - Batch 300, Loss: 2.3241, LR: 0.000056
2025-10-23 13:08:40,805 - INFO - Batch 400, Loss: 2.2233, LR: 0.000056
2025-10-23 13:08:53,272 - INFO - Batch 500, Loss: 2.5027, LR: 0.000056
2025-10-23 13:09:05,801 - INFO - Batch 600, Loss: 2.4946, LR: 0.000056
2025-10-23 13:09:18,340 - INFO - Batch 700, Loss: 2.7176, LR: 0.000056
2025-10-23 13:09:30,703 - INFO - Batch 800, Loss: 2.4159, LR: 0.000055
2025-10-23 13:09:43,074 - INFO - Batch 900, Loss: 2.2833, LR: 0.000055
2025-10-23 13:09:55,446 - INFO - Batch 1000, Loss: 2.5956, LR: 0.000055
2025-10-23 13:10:07,798 - INFO - Batch 1100, Loss: 2.5033, LR: 0.000055
2025-10-23 13:10:20,225 - INFO - Batch 1200, Loss: 2.3640, LR: 0.000055
2025-10-23 13:10:32,706 - INFO - Batch 1300, Loss: 2.2466, LR: 0.000055
2025-10-23 13:10:45,274 - INFO - Batch 1400, Loss: 2.4657, LR: 0.000055
2025-10-23 13:10:57,926 - INFO - Batch 1500, Loss: 2.3870, LR: 0.000055
2025-10-23 13:11:10,491 - INFO - Batch 1600, Loss: 2.4190, LR: 0.000055
2025-10-23 13:11:23,037 - INFO - Batch 1700, Loss: 2.2463, LR: 0.000055
2025-10-23 13:11:35,556 - INFO - Batch 1800, Loss: 1.9784, LR: 0.000055
2025-10-23 13:11:48,053 - INFO - Batch 1900, Loss: 2.5701, LR: 0.000055
2025-10-23 13:12:00,558 - INFO - Batch 2000, Loss: 2.3402, LR: 0.000054
2025-10-23 13:12:13,059 - INFO - Batch 2100, Loss: 2.2402, LR: 0.000054
2025-10-23 13:12:25,578 - INFO - Batch 2200, Loss: 2.4340, LR: 0.000054
2025-10-23 13:12:38,098 - INFO - Batch 2300, Loss: 2.5587, LR: 0.000054
2025-10-23 13:12:50,602 - INFO - Batch 2400, Loss: 2.2494, LR: 0.000054
2025-10-23 13:13:03,096 - INFO - Batch 2500, Loss: 2.4730, LR: 0.000054
2025-10-23 13:13:15,562 - INFO - Batch 2600, Loss: 2.3462, LR: 0.000054
2025-10-23 13:13:28,041 - INFO - Batch 2700, Loss: 2.3541, LR: 0.000054
2025-10-23 13:13:40,545 - INFO - Batch 2800, Loss: 1.9472, LR: 0.000054
2025-10-23 13:13:53,125 - INFO - Batch 2900, Loss: 2.2059, LR: 0.000054
2025-10-23 13:14:05,634 - INFO - Batch 3000, Loss: 2.3172, LR: 0.000054
2025-10-23 13:14:18,069 - INFO - Batch 3100, Loss: 2.3978, LR: 0.000054
2025-10-23 13:14:30,510 - INFO - Batch 3200, Loss: 2.3732, LR: 0.000053
2025-10-23 13:14:42,920 - INFO - Batch 3300, Loss: 2.5745, LR: 0.000053
2025-10-23 13:14:55,327 - INFO - Batch 3400, Loss: 2.4406, LR: 0.000053
2025-10-23 13:15:07,756 - INFO - Batch 3500, Loss: 2.3059, LR: 0.000053
2025-10-23 13:15:20,183 - INFO - Batch 3600, Loss: 2.2325, LR: 0.000053
2025-10-23 13:15:32,608 - INFO - Batch 3700, Loss: 2.3232, LR: 0.000053
2025-10-23 13:15:45,097 - INFO - Batch 3800, Loss: 2.4494, LR: 0.000053
2025-10-23 13:15:57,482 - INFO - Batch 3900, Loss: 2.4660, LR: 0.000053
2025-10-23 13:16:09,861 - INFO - Batch 4000, Loss: 2.4616, LR: 0.000053
2025-10-23 13:16:22,273 - INFO - Batch 4100, Loss: 2.2922, LR: 0.000053
2025-10-23 13:16:34,753 - INFO - Batch 4200, Loss: 2.4232, LR: 0.000053
2025-10-23 13:16:47,165 - INFO - Batch 4300, Loss: 2.2885, LR: 0.000053
2025-10-23 13:16:59,545 - INFO - Batch 4400, Loss: 2.4833, LR: 0.000052
2025-10-23 13:17:11,953 - INFO - Batch 4500, Loss: 2.3676, LR: 0.000052
2025-10-23 13:17:24,383 - INFO - Batch 4600, Loss: 2.2432, LR: 0.000052
2025-10-23 13:17:36,724 - INFO - Batch 4700, Loss: 2.3032, LR: 0.000052
2025-10-23 13:17:49,095 - INFO - Batch 4800, Loss: 2.6451, LR: 0.000052
2025-10-23 13:18:01,474 - INFO - Batch 4900, Loss: 2.5764, LR: 0.000052
2025-10-23 13:18:13,874 - INFO - Batch 5000, Loss: 2.4413, LR: 0.000052
2025-10-23 13:18:26,322 - INFO - Batch 5100, Loss: 2.1953, LR: 0.000052
2025-10-23 13:18:38,675 - INFO - Batch 5200, Loss: 2.3382, LR: 0.000052
2025-10-23 13:18:51,037 - INFO - Batch 5300, Loss: 2.5240, LR: 0.000052
2025-10-23 13:19:03,401 - INFO - Batch 5400, Loss: 2.2065, LR: 0.000052
2025-10-23 13:19:15,790 - INFO - Batch 5500, Loss: 2.3287, LR: 0.000052
2025-10-23 13:19:28,199 - INFO - Batch 5600, Loss: 2.3643, LR: 0.000052
2025-10-23 13:19:40,603 - INFO - Batch 5700, Loss: 2.2369, LR: 0.000051
2025-10-23 13:19:52,976 - INFO - Batch 5800, Loss: 2.4492, LR: 0.000051
2025-10-23 13:20:05,365 - INFO - Batch 5900, Loss: 2.3636, LR: 0.000051
2025-10-23 13:20:17,742 - INFO - Batch 6000, Loss: 2.4114, LR: 0.000051
2025-10-23 13:20:30,164 - INFO - Batch 6100, Loss: 2.6013, LR: 0.000051
2025-10-23 13:20:42,657 - INFO - Batch 6200, Loss: 2.5391, LR: 0.000051
2025-10-23 13:20:55,053 - INFO - Batch 6300, Loss: 2.4100, LR: 0.000051
2025-10-23 13:21:07,595 - INFO - Batch 6400, Loss: 2.1107, LR: 0.000051
2025-10-23 13:21:13,844 - INFO - Epoch 15/30: Train Loss: 2.3658, Val Loss: 2.2593, LR: 0.000051
2025-10-23 13:21:14,105 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 13:21:14,235 - INFO - Batch 0, Loss: 1.9959, LR: 0.000051
2025-10-23 13:21:26,657 - INFO - Batch 100, Loss: 2.1640, LR: 0.000051
2025-10-23 13:21:39,088 - INFO - Batch 200, Loss: 2.3764, LR: 0.000051
2025-10-23 13:21:51,534 - INFO - Batch 300, Loss: 2.2975, LR: 0.000051
2025-10-23 13:22:03,955 - INFO - Batch 400, Loss: 2.4920, LR: 0.000050
2025-10-23 13:22:16,431 - INFO - Batch 500, Loss: 2.2250, LR: 0.000050
2025-10-23 13:22:28,942 - INFO - Batch 600, Loss: 2.5937, LR: 0.000050
2025-10-23 13:22:41,404 - INFO - Batch 700, Loss: 2.3307, LR: 0.000050
2025-10-23 13:22:53,865 - INFO - Batch 800, Loss: 2.1201, LR: 0.000050
2025-10-23 13:23:06,290 - INFO - Batch 900, Loss: 2.3009, LR: 0.000050
2025-10-23 13:23:18,737 - INFO - Batch 1000, Loss: 2.5924, LR: 0.000050
2025-10-23 13:23:31,165 - INFO - Batch 1100, Loss: 2.2876, LR: 0.000050
2025-10-23 13:23:43,604 - INFO - Batch 1200, Loss: 2.2623, LR: 0.000050
2025-10-23 13:23:56,130 - INFO - Batch 1300, Loss: 2.2851, LR: 0.000050
2025-10-23 13:24:08,622 - INFO - Batch 1400, Loss: 2.0907, LR: 0.000050
2025-10-23 13:24:21,101 - INFO - Batch 1500, Loss: 2.2218, LR: 0.000050
2025-10-23 13:24:33,589 - INFO - Batch 1600, Loss: 2.2341, LR: 0.000050
2025-10-23 13:24:46,045 - INFO - Batch 1700, Loss: 2.3285, LR: 0.000049
2025-10-23 13:24:58,518 - INFO - Batch 1800, Loss: 2.3805, LR: 0.000049
2025-10-23 13:25:11,040 - INFO - Batch 1900, Loss: 2.4667, LR: 0.000049
2025-10-23 13:25:23,570 - INFO - Batch 2000, Loss: 2.0052, LR: 0.000049
2025-10-23 13:25:36,094 - INFO - Batch 2100, Loss: 2.3017, LR: 0.000049
2025-10-23 13:25:48,728 - INFO - Batch 2200, Loss: 2.2736, LR: 0.000049
2025-10-23 13:26:01,287 - INFO - Batch 2300, Loss: 2.2444, LR: 0.000049
2025-10-23 13:26:13,826 - INFO - Batch 2400, Loss: 2.5302, LR: 0.000049
2025-10-23 13:26:26,290 - INFO - Batch 2500, Loss: 2.2471, LR: 0.000049
2025-10-23 13:26:38,777 - INFO - Batch 2600, Loss: 2.4359, LR: 0.000049
2025-10-23 13:26:51,269 - INFO - Batch 2700, Loss: 2.4039, LR: 0.000049
2025-10-23 13:27:03,766 - INFO - Batch 2800, Loss: 2.4056, LR: 0.000049
2025-10-23 13:27:16,265 - INFO - Batch 2900, Loss: 2.5523, LR: 0.000048
2025-10-23 13:27:28,816 - INFO - Batch 3000, Loss: 2.2786, LR: 0.000048
2025-10-23 13:27:41,328 - INFO - Batch 3100, Loss: 2.2404, LR: 0.000048
2025-10-23 13:27:53,854 - INFO - Batch 3200, Loss: 2.3917, LR: 0.000048
2025-10-23 13:28:06,378 - INFO - Batch 3300, Loss: 2.2251, LR: 0.000048
2025-10-23 13:28:18,862 - INFO - Batch 3400, Loss: 2.4622, LR: 0.000048
2025-10-23 13:28:31,340 - INFO - Batch 3500, Loss: 2.4167, LR: 0.000048
2025-10-23 13:28:43,839 - INFO - Batch 3600, Loss: 2.5271, LR: 0.000048
2025-10-23 13:28:56,418 - INFO - Batch 3700, Loss: 2.3851, LR: 0.000048
2025-10-23 13:29:09,023 - INFO - Batch 3800, Loss: 2.2591, LR: 0.000048
2025-10-23 13:29:21,581 - INFO - Batch 3900, Loss: 2.1860, LR: 0.000048
2025-10-23 13:29:34,164 - INFO - Batch 4000, Loss: 2.3197, LR: 0.000048
2025-10-23 13:29:46,678 - INFO - Batch 4100, Loss: 2.5493, LR: 0.000047
2025-10-23 13:29:59,188 - INFO - Batch 4200, Loss: 2.5356, LR: 0.000047
2025-10-23 13:30:11,689 - INFO - Batch 4300, Loss: 2.4617, LR: 0.000047
2025-10-23 13:30:24,148 - INFO - Batch 4400, Loss: 2.4787, LR: 0.000047
2025-10-23 13:30:36,634 - INFO - Batch 4500, Loss: 2.5811, LR: 0.000047
2025-10-23 13:30:49,100 - INFO - Batch 4600, Loss: 2.5438, LR: 0.000047
2025-10-23 13:31:01,582 - INFO - Batch 4700, Loss: 2.3468, LR: 0.000047
2025-10-23 13:31:14,062 - INFO - Batch 4800, Loss: 2.8617, LR: 0.000047
2025-10-23 13:31:26,522 - INFO - Batch 4900, Loss: 2.4738, LR: 0.000047
2025-10-23 13:31:38,960 - INFO - Batch 5000, Loss: 2.2410, LR: 0.000047
2025-10-23 13:31:51,403 - INFO - Batch 5100, Loss: 2.1789, LR: 0.000047
2025-10-23 13:32:03,958 - INFO - Batch 5200, Loss: 2.5147, LR: 0.000047
2025-10-23 13:32:16,401 - INFO - Batch 5300, Loss: 2.0340, LR: 0.000046
2025-10-23 13:32:28,976 - INFO - Batch 5400, Loss: 2.7121, LR: 0.000046
2025-10-23 13:32:41,402 - INFO - Batch 5500, Loss: 2.0971, LR: 0.000046
2025-10-23 13:32:53,857 - INFO - Batch 5600, Loss: 2.3182, LR: 0.000046
2025-10-23 13:33:06,295 - INFO - Batch 5700, Loss: 2.2437, LR: 0.000046
2025-10-23 13:33:18,705 - INFO - Batch 5800, Loss: 2.1277, LR: 0.000046
2025-10-23 13:33:31,061 - INFO - Batch 5900, Loss: 2.4910, LR: 0.000046
2025-10-23 13:33:43,538 - INFO - Batch 6000, Loss: 2.3454, LR: 0.000046
2025-10-23 13:33:55,951 - INFO - Batch 6100, Loss: 2.2635, LR: 0.000046
2025-10-23 13:34:08,538 - INFO - Batch 6200, Loss: 2.1430, LR: 0.000046
2025-10-23 13:34:21,066 - INFO - Batch 6300, Loss: 2.4287, LR: 0.000046
2025-10-23 13:34:33,531 - INFO - Batch 6400, Loss: 2.6795, LR: 0.000046
2025-10-23 13:34:39,752 - INFO - Epoch 16/30: Train Loss: 2.3489, Val Loss: 2.2463, LR: 0.000046
2025-10-23 13:34:40,028 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 13:34:40,159 - INFO - Batch 0, Loss: 2.2297, LR: 0.000046
2025-10-23 13:34:52,555 - INFO - Batch 100, Loss: 2.2778, LR: 0.000045
2025-10-23 13:35:04,930 - INFO - Batch 200, Loss: 2.3683, LR: 0.000045
2025-10-23 13:35:17,289 - INFO - Batch 300, Loss: 2.2504, LR: 0.000045
2025-10-23 13:35:29,667 - INFO - Batch 400, Loss: 2.2213, LR: 0.000045
2025-10-23 13:35:42,042 - INFO - Batch 500, Loss: 2.4340, LR: 0.000045
2025-10-23 13:35:54,404 - INFO - Batch 600, Loss: 2.3662, LR: 0.000045
2025-10-23 13:36:06,770 - INFO - Batch 700, Loss: 2.1571, LR: 0.000045
2025-10-23 13:36:19,147 - INFO - Batch 800, Loss: 2.3436, LR: 0.000045
2025-10-23 13:36:31,497 - INFO - Batch 900, Loss: 2.3911, LR: 0.000045
2025-10-23 13:36:43,959 - INFO - Batch 1000, Loss: 2.5703, LR: 0.000045
2025-10-23 13:36:56,386 - INFO - Batch 1100, Loss: 2.4036, LR: 0.000045
2025-10-23 13:37:09,030 - INFO - Batch 1200, Loss: 2.3603, LR: 0.000045
2025-10-23 13:37:21,689 - INFO - Batch 1300, Loss: 2.1608, LR: 0.000044
2025-10-23 13:37:34,189 - INFO - Batch 1400, Loss: 2.4150, LR: 0.000044
2025-10-23 13:37:46,698 - INFO - Batch 1500, Loss: 2.2778, LR: 0.000044
2025-10-23 13:37:59,220 - INFO - Batch 1600, Loss: 2.4070, LR: 0.000044
2025-10-23 13:38:11,696 - INFO - Batch 1700, Loss: 2.1365, LR: 0.000044
2025-10-23 13:38:24,174 - INFO - Batch 1800, Loss: 2.5450, LR: 0.000044
2025-10-23 13:38:36,693 - INFO - Batch 1900, Loss: 2.0891, LR: 0.000044
2025-10-23 13:38:49,193 - INFO - Batch 2000, Loss: 2.6355, LR: 0.000044
2025-10-23 13:39:01,644 - INFO - Batch 2100, Loss: 2.2809, LR: 0.000044
2025-10-23 13:39:14,101 - INFO - Batch 2200, Loss: 2.4945, LR: 0.000044
2025-10-23 13:39:26,563 - INFO - Batch 2300, Loss: 2.3899, LR: 0.000044
2025-10-23 13:39:39,081 - INFO - Batch 2400, Loss: 2.5129, LR: 0.000044
2025-10-23 13:39:51,616 - INFO - Batch 2500, Loss: 2.6980, LR: 0.000043
2025-10-23 13:40:04,177 - INFO - Batch 2600, Loss: 2.4475, LR: 0.000043
2025-10-23 13:40:16,735 - INFO - Batch 2700, Loss: 2.1888, LR: 0.000043
2025-10-23 13:40:29,233 - INFO - Batch 2800, Loss: 2.3949, LR: 0.000043
2025-10-23 13:40:41,703 - INFO - Batch 2900, Loss: 2.5899, LR: 0.000043
2025-10-23 13:40:54,158 - INFO - Batch 3000, Loss: 2.4567, LR: 0.000043
2025-10-23 13:41:06,605 - INFO - Batch 3100, Loss: 2.3232, LR: 0.000043
2025-10-23 13:41:19,058 - INFO - Batch 3200, Loss: 2.1774, LR: 0.000043
2025-10-23 13:41:31,485 - INFO - Batch 3300, Loss: 2.3948, LR: 0.000043
2025-10-23 13:41:43,915 - INFO - Batch 3400, Loss: 2.1714, LR: 0.000043
2025-10-23 13:41:56,301 - INFO - Batch 3500, Loss: 2.5723, LR: 0.000043
2025-10-23 13:42:08,709 - INFO - Batch 3600, Loss: 2.3880, LR: 0.000043
2025-10-23 13:42:21,261 - INFO - Batch 3700, Loss: 2.3640, LR: 0.000043
2025-10-23 13:42:33,699 - INFO - Batch 3800, Loss: 2.5883, LR: 0.000042
2025-10-23 13:42:46,133 - INFO - Batch 3900, Loss: 2.5063, LR: 0.000042
2025-10-23 13:42:58,632 - INFO - Batch 4000, Loss: 2.2853, LR: 0.000042
2025-10-23 13:43:11,094 - INFO - Batch 4100, Loss: 2.3859, LR: 0.000042
2025-10-23 13:43:23,497 - INFO - Batch 4200, Loss: 2.0488, LR: 0.000042
2025-10-23 13:43:35,889 - INFO - Batch 4300, Loss: 2.2838, LR: 0.000042
2025-10-23 13:43:48,237 - INFO - Batch 4400, Loss: 2.5157, LR: 0.000042
2025-10-23 13:44:00,605 - INFO - Batch 4500, Loss: 2.4526, LR: 0.000042
2025-10-23 13:44:12,953 - INFO - Batch 4600, Loss: 2.6271, LR: 0.000042
2025-10-23 13:44:25,316 - INFO - Batch 4700, Loss: 2.5089, LR: 0.000042
2025-10-23 13:44:37,728 - INFO - Batch 4800, Loss: 2.1705, LR: 0.000042
2025-10-23 13:44:50,134 - INFO - Batch 4900, Loss: 2.3219, LR: 0.000042
2025-10-23 13:45:02,521 - INFO - Batch 5000, Loss: 2.2548, LR: 0.000041
2025-10-23 13:45:14,914 - INFO - Batch 5100, Loss: 2.3383, LR: 0.000041
2025-10-23 13:45:27,481 - INFO - Batch 5200, Loss: 2.3224, LR: 0.000041
2025-10-23 13:45:39,945 - INFO - Batch 5300, Loss: 2.3564, LR: 0.000041
2025-10-23 13:45:52,344 - INFO - Batch 5400, Loss: 2.5331, LR: 0.000041
2025-10-23 13:46:04,734 - INFO - Batch 5500, Loss: 2.1612, LR: 0.000041
2025-10-23 13:46:17,186 - INFO - Batch 5600, Loss: 2.3238, LR: 0.000041
2025-10-23 13:46:29,646 - INFO - Batch 5700, Loss: 2.1939, LR: 0.000041
2025-10-23 13:46:42,057 - INFO - Batch 5800, Loss: 2.5936, LR: 0.000041
2025-10-23 13:46:54,439 - INFO - Batch 5900, Loss: 2.3881, LR: 0.000041
2025-10-23 13:47:06,890 - INFO - Batch 6000, Loss: 2.2653, LR: 0.000041
2025-10-23 13:47:19,358 - INFO - Batch 6100, Loss: 2.1693, LR: 0.000041
2025-10-23 13:47:31,730 - INFO - Batch 6200, Loss: 2.6238, LR: 0.000040
2025-10-23 13:47:44,087 - INFO - Batch 6300, Loss: 2.3744, LR: 0.000040
2025-10-23 13:47:56,441 - INFO - Batch 6400, Loss: 2.2684, LR: 0.000040
2025-10-23 13:48:02,607 - INFO - Epoch 17/30: Train Loss: 2.3319, Val Loss: 2.2305, LR: 0.000040
2025-10-23 13:48:02,971 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 13:48:03,102 - INFO - Batch 0, Loss: 2.4208, LR: 0.000040
2025-10-23 13:48:15,545 - INFO - Batch 100, Loss: 2.5041, LR: 0.000040
2025-10-23 13:48:27,934 - INFO - Batch 200, Loss: 2.5778, LR: 0.000040
2025-10-23 13:48:40,329 - INFO - Batch 300, Loss: 2.2946, LR: 0.000040
2025-10-23 13:48:52,738 - INFO - Batch 400, Loss: 2.2401, LR: 0.000040
2025-10-23 13:49:05,149 - INFO - Batch 500, Loss: 2.3114, LR: 0.000040
2025-10-23 13:49:17,589 - INFO - Batch 600, Loss: 2.5440, LR: 0.000040
2025-10-23 13:49:30,055 - INFO - Batch 700, Loss: 2.5164, LR: 0.000040
2025-10-23 13:49:42,583 - INFO - Batch 800, Loss: 2.2020, LR: 0.000040
2025-10-23 13:49:55,032 - INFO - Batch 900, Loss: 2.1066, LR: 0.000040
2025-10-23 13:50:07,555 - INFO - Batch 1000, Loss: 2.4697, LR: 0.000039
2025-10-23 13:50:20,082 - INFO - Batch 1100, Loss: 2.5499, LR: 0.000039
2025-10-23 13:50:32,589 - INFO - Batch 1200, Loss: 2.2288, LR: 0.000039
2025-10-23 13:50:45,100 - INFO - Batch 1300, Loss: 2.2922, LR: 0.000039
2025-10-23 13:50:57,609 - INFO - Batch 1400, Loss: 2.4181, LR: 0.000039
2025-10-23 13:51:10,121 - INFO - Batch 1500, Loss: 2.4120, LR: 0.000039
2025-10-23 13:51:22,649 - INFO - Batch 1600, Loss: 2.2847, LR: 0.000039
2025-10-23 13:51:35,166 - INFO - Batch 1700, Loss: 2.3057, LR: 0.000039
2025-10-23 13:51:47,655 - INFO - Batch 1800, Loss: 2.4159, LR: 0.000039
2025-10-23 13:52:00,174 - INFO - Batch 1900, Loss: 2.3739, LR: 0.000039
2025-10-23 13:52:12,848 - INFO - Batch 2000, Loss: 2.5169, LR: 0.000039
2025-10-23 13:52:25,461 - INFO - Batch 2100, Loss: 2.2214, LR: 0.000039
2025-10-23 13:52:38,069 - INFO - Batch 2200, Loss: 2.3429, LR: 0.000039
2025-10-23 13:52:50,595 - INFO - Batch 2300, Loss: 2.1596, LR: 0.000038
2025-10-23 13:53:03,092 - INFO - Batch 2400, Loss: 2.2438, LR: 0.000038
2025-10-23 13:53:15,590 - INFO - Batch 2500, Loss: 2.3926, LR: 0.000038
2025-10-23 13:53:28,124 - INFO - Batch 2600, Loss: 2.3878, LR: 0.000038
2025-10-23 13:53:40,745 - INFO - Batch 2700, Loss: 2.3115, LR: 0.000038
2025-10-23 13:53:53,425 - INFO - Batch 2800, Loss: 2.2619, LR: 0.000038
2025-10-23 13:54:05,904 - INFO - Batch 2900, Loss: 2.5816, LR: 0.000038
2025-10-23 13:54:18,409 - INFO - Batch 3000, Loss: 2.3440, LR: 0.000038
2025-10-23 13:54:30,910 - INFO - Batch 3100, Loss: 2.4227, LR: 0.000038
2025-10-23 13:54:43,390 - INFO - Batch 3200, Loss: 2.0055, LR: 0.000038
2025-10-23 13:54:55,868 - INFO - Batch 3300, Loss: 2.6618, LR: 0.000038
2025-10-23 13:55:08,334 - INFO - Batch 3400, Loss: 2.3551, LR: 0.000038
2025-10-23 13:55:20,813 - INFO - Batch 3500, Loss: 2.3877, LR: 0.000037
2025-10-23 13:55:33,277 - INFO - Batch 3600, Loss: 2.4549, LR: 0.000037
2025-10-23 13:55:45,762 - INFO - Batch 3700, Loss: 2.4379, LR: 0.000037
2025-10-23 13:55:58,245 - INFO - Batch 3800, Loss: 2.2246, LR: 0.000037
2025-10-23 13:56:10,805 - INFO - Batch 3900, Loss: 2.2390, LR: 0.000037
2025-10-23 13:56:23,404 - INFO - Batch 4000, Loss: 2.1761, LR: 0.000037
2025-10-23 13:56:36,026 - INFO - Batch 4100, Loss: 2.2570, LR: 0.000037
2025-10-23 13:56:48,593 - INFO - Batch 4200, Loss: 2.3071, LR: 0.000037
2025-10-23 13:57:01,221 - INFO - Batch 4300, Loss: 2.3449, LR: 0.000037
2025-10-23 13:57:13,864 - INFO - Batch 4400, Loss: 2.3383, LR: 0.000037
2025-10-23 13:57:26,513 - INFO - Batch 4500, Loss: 2.3781, LR: 0.000037
2025-10-23 13:57:39,204 - INFO - Batch 4600, Loss: 2.3531, LR: 0.000037
2025-10-23 13:57:51,808 - INFO - Batch 4700, Loss: 2.2832, LR: 0.000037
2025-10-23 13:58:04,386 - INFO - Batch 4800, Loss: 2.4278, LR: 0.000036
2025-10-23 13:58:16,845 - INFO - Batch 4900, Loss: 2.4828, LR: 0.000036
2025-10-23 13:58:29,493 - INFO - Batch 5000, Loss: 2.3553, LR: 0.000036
2025-10-23 13:58:42,085 - INFO - Batch 5100, Loss: 2.1585, LR: 0.000036
2025-10-23 13:58:54,562 - INFO - Batch 5200, Loss: 2.3302, LR: 0.000036
2025-10-23 13:59:06,977 - INFO - Batch 5300, Loss: 2.2755, LR: 0.000036
2025-10-23 13:59:19,401 - INFO - Batch 5400, Loss: 2.2075, LR: 0.000036
2025-10-23 13:59:31,810 - INFO - Batch 5500, Loss: 2.1678, LR: 0.000036
2025-10-23 13:59:44,297 - INFO - Batch 5600, Loss: 2.3404, LR: 0.000036
2025-10-23 13:59:56,849 - INFO - Batch 5700, Loss: 2.3560, LR: 0.000036
2025-10-23 14:00:09,278 - INFO - Batch 5800, Loss: 2.3149, LR: 0.000036
2025-10-23 14:00:21,744 - INFO - Batch 5900, Loss: 1.9731, LR: 0.000036
2025-10-23 14:00:34,241 - INFO - Batch 6000, Loss: 2.4129, LR: 0.000036
2025-10-23 14:00:46,697 - INFO - Batch 6100, Loss: 2.4217, LR: 0.000035
2025-10-23 14:00:59,219 - INFO - Batch 6200, Loss: 2.4563, LR: 0.000035
2025-10-23 14:01:11,662 - INFO - Batch 6300, Loss: 2.3570, LR: 0.000035
2025-10-23 14:01:24,135 - INFO - Batch 6400, Loss: 2.4219, LR: 0.000035
2025-10-23 14:01:30,380 - INFO - Epoch 18/30: Train Loss: 2.3164, Val Loss: 2.2242, LR: 0.000035
2025-10-23 14:01:30,656 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 14:01:30,793 - INFO - Batch 0, Loss: 2.3145, LR: 0.000035
2025-10-23 14:01:43,306 - INFO - Batch 100, Loss: 2.4678, LR: 0.000035
2025-10-23 14:01:55,809 - INFO - Batch 200, Loss: 2.3265, LR: 0.000035
2025-10-23 14:02:08,374 - INFO - Batch 300, Loss: 2.2651, LR: 0.000035
2025-10-23 14:02:20,917 - INFO - Batch 400, Loss: 2.2979, LR: 0.000035
2025-10-23 14:02:33,438 - INFO - Batch 500, Loss: 2.3716, LR: 0.000035
2025-10-23 14:02:46,033 - INFO - Batch 600, Loss: 2.3295, LR: 0.000035
2025-10-23 14:02:58,596 - INFO - Batch 700, Loss: 2.0116, LR: 0.000035
2025-10-23 14:03:11,207 - INFO - Batch 800, Loss: 2.2738, LR: 0.000035
2025-10-23 14:03:23,839 - INFO - Batch 900, Loss: 2.3879, LR: 0.000034
2025-10-23 14:03:36,468 - INFO - Batch 1000, Loss: 2.1078, LR: 0.000034
2025-10-23 14:03:49,071 - INFO - Batch 1100, Loss: 2.3907, LR: 0.000034
2025-10-23 14:04:01,641 - INFO - Batch 1200, Loss: 2.0586, LR: 0.000034
2025-10-23 14:04:14,347 - INFO - Batch 1300, Loss: 2.1178, LR: 0.000034
2025-10-23 14:04:27,023 - INFO - Batch 1400, Loss: 2.1235, LR: 0.000034
2025-10-23 14:04:39,709 - INFO - Batch 1500, Loss: 2.3993, LR: 0.000034
2025-10-23 14:04:52,543 - INFO - Batch 1600, Loss: 2.4967, LR: 0.000034
2025-10-23 14:05:05,270 - INFO - Batch 1700, Loss: 2.1616, LR: 0.000034
2025-10-23 14:05:17,994 - INFO - Batch 1800, Loss: 2.1420, LR: 0.000034
2025-10-23 14:05:30,656 - INFO - Batch 1900, Loss: 2.1655, LR: 0.000034
2025-10-23 14:05:43,329 - INFO - Batch 2000, Loss: 2.0994, LR: 0.000034
2025-10-23 14:05:56,025 - INFO - Batch 2100, Loss: 2.2667, LR: 0.000034
2025-10-23 14:06:08,711 - INFO - Batch 2200, Loss: 2.0946, LR: 0.000033
2025-10-23 14:06:21,448 - INFO - Batch 2300, Loss: 2.3102, LR: 0.000033
2025-10-23 14:06:34,044 - INFO - Batch 2400, Loss: 2.2324, LR: 0.000033
2025-10-23 14:06:46,661 - INFO - Batch 2500, Loss: 2.1639, LR: 0.000033
2025-10-23 14:06:59,265 - INFO - Batch 2600, Loss: 2.2064, LR: 0.000033
2025-10-23 14:07:11,857 - INFO - Batch 2700, Loss: 2.4546, LR: 0.000033
2025-10-23 14:07:24,538 - INFO - Batch 2800, Loss: 1.9641, LR: 0.000033
2025-10-23 14:07:37,205 - INFO - Batch 2900, Loss: 2.2885, LR: 0.000033
2025-10-23 14:07:49,737 - INFO - Batch 3000, Loss: 2.2252, LR: 0.000033
2025-10-23 14:08:02,299 - INFO - Batch 3100, Loss: 2.4576, LR: 0.000033
2025-10-23 14:08:14,827 - INFO - Batch 3200, Loss: 2.5004, LR: 0.000033
2025-10-23 14:08:27,335 - INFO - Batch 3300, Loss: 2.3863, LR: 0.000033
2025-10-23 14:08:39,881 - INFO - Batch 3400, Loss: 2.3302, LR: 0.000033
2025-10-23 14:08:52,417 - INFO - Batch 3500, Loss: 2.2000, LR: 0.000032
2025-10-23 14:09:04,968 - INFO - Batch 3600, Loss: 2.2288, LR: 0.000032
2025-10-23 14:09:17,522 - INFO - Batch 3700, Loss: 2.1980, LR: 0.000032
2025-10-23 14:09:30,248 - INFO - Batch 3800, Loss: 2.1927, LR: 0.000032
2025-10-23 14:09:42,941 - INFO - Batch 3900, Loss: 2.2346, LR: 0.000032
2025-10-23 14:09:55,660 - INFO - Batch 4000, Loss: 2.1622, LR: 0.000032
2025-10-23 14:10:08,366 - INFO - Batch 4100, Loss: 2.3137, LR: 0.000032
2025-10-23 14:10:20,914 - INFO - Batch 4200, Loss: 2.3309, LR: 0.000032
2025-10-23 14:10:33,428 - INFO - Batch 4300, Loss: 2.5989, LR: 0.000032
2025-10-23 14:10:45,953 - INFO - Batch 4400, Loss: 2.3507, LR: 0.000032
2025-10-23 14:10:58,433 - INFO - Batch 4500, Loss: 2.3632, LR: 0.000032
2025-10-23 14:11:10,922 - INFO - Batch 4600, Loss: 2.2362, LR: 0.000032
2025-10-23 14:11:23,410 - INFO - Batch 4700, Loss: 2.1367, LR: 0.000032
2025-10-23 14:11:35,964 - INFO - Batch 4800, Loss: 2.2780, LR: 0.000031
2025-10-23 14:11:48,486 - INFO - Batch 4900, Loss: 2.2130, LR: 0.000031
2025-10-23 14:12:01,018 - INFO - Batch 5000, Loss: 2.2435, LR: 0.000031
2025-10-23 14:12:13,533 - INFO - Batch 5100, Loss: 2.1881, LR: 0.000031
2025-10-23 14:12:26,093 - INFO - Batch 5200, Loss: 2.3463, LR: 0.000031
2025-10-23 14:12:38,555 - INFO - Batch 5300, Loss: 2.2853, LR: 0.000031
2025-10-23 14:12:51,051 - INFO - Batch 5400, Loss: 2.5501, LR: 0.000031
2025-10-23 14:13:03,515 - INFO - Batch 5500, Loss: 1.8393, LR: 0.000031
2025-10-23 14:13:15,973 - INFO - Batch 5600, Loss: 2.2261, LR: 0.000031
2025-10-23 14:13:28,445 - INFO - Batch 5700, Loss: 2.3393, LR: 0.000031
2025-10-23 14:13:40,910 - INFO - Batch 5800, Loss: 2.1810, LR: 0.000031
2025-10-23 14:13:53,498 - INFO - Batch 5900, Loss: 2.1630, LR: 0.000031
2025-10-23 14:14:05,975 - INFO - Batch 6000, Loss: 2.1057, LR: 0.000031
2025-10-23 14:14:18,538 - INFO - Batch 6100, Loss: 2.0492, LR: 0.000030
2025-10-23 14:14:31,104 - INFO - Batch 6200, Loss: 2.6097, LR: 0.000030
2025-10-23 14:14:43,696 - INFO - Batch 6300, Loss: 2.3493, LR: 0.000030
2025-10-23 14:14:56,276 - INFO - Batch 6400, Loss: 2.3325, LR: 0.000030
2025-10-23 14:15:02,554 - INFO - Epoch 19/30: Train Loss: 2.3013, Val Loss: 2.2106, LR: 0.000030
2025-10-23 14:15:02,832 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 14:15:02,965 - INFO - Batch 0, Loss: 2.3517, LR: 0.000030
2025-10-23 14:15:15,598 - INFO - Batch 100, Loss: 2.2054, LR: 0.000030
2025-10-23 14:15:28,221 - INFO - Batch 200, Loss: 2.2103, LR: 0.000030
2025-10-23 14:15:40,923 - INFO - Batch 300, Loss: 2.2756, LR: 0.000030
2025-10-23 14:15:53,660 - INFO - Batch 400, Loss: 2.3504, LR: 0.000030
2025-10-23 14:16:06,239 - INFO - Batch 500, Loss: 2.0440, LR: 0.000030
2025-10-23 14:16:18,926 - INFO - Batch 600, Loss: 2.2325, LR: 0.000030
2025-10-23 14:16:31,708 - INFO - Batch 700, Loss: 2.2787, LR: 0.000030
2025-10-23 14:16:44,466 - INFO - Batch 800, Loss: 2.3808, LR: 0.000030
2025-10-23 14:16:57,263 - INFO - Batch 900, Loss: 2.3333, LR: 0.000030
2025-10-23 14:17:10,110 - INFO - Batch 1000, Loss: 2.1820, LR: 0.000029
2025-10-23 14:17:22,928 - INFO - Batch 1100, Loss: 2.5716, LR: 0.000029
2025-10-23 14:17:35,623 - INFO - Batch 1200, Loss: 2.2727, LR: 0.000029
2025-10-23 14:17:48,303 - INFO - Batch 1300, Loss: 2.2516, LR: 0.000029
2025-10-23 14:18:01,129 - INFO - Batch 1400, Loss: 2.1258, LR: 0.000029
2025-10-23 14:18:13,917 - INFO - Batch 1500, Loss: 2.1315, LR: 0.000029
2025-10-23 14:18:26,728 - INFO - Batch 1600, Loss: 2.1777, LR: 0.000029
2025-10-23 14:18:39,437 - INFO - Batch 1700, Loss: 2.4408, LR: 0.000029
2025-10-23 14:18:52,270 - INFO - Batch 1800, Loss: 2.1630, LR: 0.000029
2025-10-23 14:19:04,998 - INFO - Batch 1900, Loss: 2.2543, LR: 0.000029
2025-10-23 14:19:17,741 - INFO - Batch 2000, Loss: 2.1447, LR: 0.000029
2025-10-23 14:19:30,488 - INFO - Batch 2100, Loss: 2.2942, LR: 0.000029
2025-10-23 14:19:43,191 - INFO - Batch 2200, Loss: 2.4185, LR: 0.000029
2025-10-23 14:19:55,834 - INFO - Batch 2300, Loss: 2.2351, LR: 0.000028
2025-10-23 14:20:08,574 - INFO - Batch 2400, Loss: 2.2076, LR: 0.000028
2025-10-23 14:20:21,297 - INFO - Batch 2500, Loss: 2.3840, LR: 0.000028
2025-10-23 14:20:33,921 - INFO - Batch 2600, Loss: 2.5106, LR: 0.000028
2025-10-23 14:20:46,558 - INFO - Batch 2700, Loss: 2.7100, LR: 0.000028
2025-10-23 14:20:59,218 - INFO - Batch 2800, Loss: 2.3909, LR: 0.000028
2025-10-23 14:21:11,849 - INFO - Batch 2900, Loss: 2.0385, LR: 0.000028
2025-10-23 14:21:24,610 - INFO - Batch 3000, Loss: 2.4317, LR: 0.000028
2025-10-23 14:21:37,362 - INFO - Batch 3100, Loss: 2.3802, LR: 0.000028
2025-10-23 14:21:50,117 - INFO - Batch 3200, Loss: 2.2824, LR: 0.000028
2025-10-23 14:22:02,882 - INFO - Batch 3300, Loss: 2.5130, LR: 0.000028
2025-10-23 14:22:15,486 - INFO - Batch 3400, Loss: 2.3291, LR: 0.000028
2025-10-23 14:22:28,166 - INFO - Batch 3500, Loss: 2.5178, LR: 0.000028
2025-10-23 14:22:40,750 - INFO - Batch 3600, Loss: 2.5687, LR: 0.000028
2025-10-23 14:22:53,318 - INFO - Batch 3700, Loss: 2.0695, LR: 0.000027
2025-10-23 14:23:05,904 - INFO - Batch 3800, Loss: 2.1255, LR: 0.000027
2025-10-23 14:23:18,637 - INFO - Batch 3900, Loss: 2.2460, LR: 0.000027
2025-10-23 14:23:31,357 - INFO - Batch 4000, Loss: 2.3014, LR: 0.000027
2025-10-23 14:23:44,104 - INFO - Batch 4100, Loss: 2.3320, LR: 0.000027
2025-10-23 14:23:56,798 - INFO - Batch 4200, Loss: 2.3122, LR: 0.000027
2025-10-23 14:24:09,453 - INFO - Batch 4300, Loss: 2.5127, LR: 0.000027
2025-10-23 14:24:22,147 - INFO - Batch 4400, Loss: 2.5078, LR: 0.000027
2025-10-23 14:24:34,789 - INFO - Batch 4500, Loss: 2.2520, LR: 0.000027
2025-10-23 14:24:47,435 - INFO - Batch 4600, Loss: 2.5411, LR: 0.000027
2025-10-23 14:25:00,103 - INFO - Batch 4700, Loss: 2.4899, LR: 0.000027
2025-10-23 14:25:12,747 - INFO - Batch 4800, Loss: 2.0483, LR: 0.000027
2025-10-23 14:25:25,345 - INFO - Batch 4900, Loss: 2.2381, LR: 0.000027
2025-10-23 14:25:37,890 - INFO - Batch 5000, Loss: 2.2793, LR: 0.000027
2025-10-23 14:25:50,441 - INFO - Batch 5100, Loss: 2.2059, LR: 0.000026
2025-10-23 14:26:03,098 - INFO - Batch 5200, Loss: 2.2883, LR: 0.000026
2025-10-23 14:26:15,807 - INFO - Batch 5300, Loss: 2.1575, LR: 0.000026
2025-10-23 14:26:28,494 - INFO - Batch 5400, Loss: 2.2814, LR: 0.000026
2025-10-23 14:26:41,123 - INFO - Batch 5500, Loss: 2.4293, LR: 0.000026
2025-10-23 14:26:53,694 - INFO - Batch 5600, Loss: 2.1655, LR: 0.000026
2025-10-23 14:27:06,293 - INFO - Batch 5700, Loss: 2.2079, LR: 0.000026
2025-10-23 14:27:19,038 - INFO - Batch 5800, Loss: 2.2883, LR: 0.000026
2025-10-23 14:27:31,662 - INFO - Batch 5900, Loss: 2.3608, LR: 0.000026
2025-10-23 14:27:44,188 - INFO - Batch 6000, Loss: 2.3017, LR: 0.000026
2025-10-23 14:27:56,800 - INFO - Batch 6100, Loss: 2.0048, LR: 0.000026
2025-10-23 14:28:09,392 - INFO - Batch 6200, Loss: 2.4348, LR: 0.000026
2025-10-23 14:28:22,000 - INFO - Batch 6300, Loss: 2.2987, LR: 0.000026
2025-10-23 14:28:34,609 - INFO - Batch 6400, Loss: 2.3149, LR: 0.000026
2025-10-23 14:28:40,873 - INFO - Epoch 20/30: Train Loss: 2.2884, Val Loss: 2.1964, LR: 0.000025
2025-10-23 14:28:41,142 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 14:28:41,349 - INFO - 模型已保存到: ./checkpoints/checkpoint_epoch_20.pth
2025-10-23 14:28:41,479 - INFO - Batch 0, Loss: 2.1600, LR: 0.000025
2025-10-23 14:28:54,051 - INFO - Batch 100, Loss: 2.2338, LR: 0.000025
2025-10-23 14:29:06,629 - INFO - Batch 200, Loss: 2.3268, LR: 0.000025
2025-10-23 14:29:19,290 - INFO - Batch 300, Loss: 2.0668, LR: 0.000025
2025-10-23 14:29:32,000 - INFO - Batch 400, Loss: 2.3599, LR: 0.000025
2025-10-23 14:29:44,713 - INFO - Batch 500, Loss: 2.4106, LR: 0.000025
2025-10-23 14:29:57,433 - INFO - Batch 600, Loss: 2.2488, LR: 0.000025
2025-10-23 14:30:10,157 - INFO - Batch 700, Loss: 2.3696, LR: 0.000025
2025-10-23 14:30:22,863 - INFO - Batch 800, Loss: 2.2922, LR: 0.000025
2025-10-23 14:30:35,484 - INFO - Batch 900, Loss: 2.3259, LR: 0.000025
2025-10-23 14:30:48,113 - INFO - Batch 1000, Loss: 2.2266, LR: 0.000025
2025-10-23 14:31:00,727 - INFO - Batch 1100, Loss: 2.1930, LR: 0.000025
2025-10-23 14:31:13,324 - INFO - Batch 1200, Loss: 2.2850, LR: 0.000025
2025-10-23 14:31:26,105 - INFO - Batch 1300, Loss: 2.2979, LR: 0.000025
2025-10-23 14:31:38,885 - INFO - Batch 1400, Loss: 2.0080, LR: 0.000024
2025-10-23 14:31:51,635 - INFO - Batch 1500, Loss: 2.1402, LR: 0.000024
2025-10-23 14:32:04,354 - INFO - Batch 1600, Loss: 2.0728, LR: 0.000024
2025-10-23 14:32:17,080 - INFO - Batch 1700, Loss: 2.0809, LR: 0.000024
2025-10-23 14:32:29,893 - INFO - Batch 1800, Loss: 2.4075, LR: 0.000024
2025-10-23 14:32:42,735 - INFO - Batch 1900, Loss: 2.1327, LR: 0.000024
2025-10-23 14:32:55,516 - INFO - Batch 2000, Loss: 2.1997, LR: 0.000024
2025-10-23 14:33:08,334 - INFO - Batch 2100, Loss: 2.4137, LR: 0.000024
2025-10-23 14:33:21,069 - INFO - Batch 2200, Loss: 2.1902, LR: 0.000024
2025-10-23 14:33:33,894 - INFO - Batch 2300, Loss: 2.2738, LR: 0.000024
2025-10-23 14:33:46,806 - INFO - Batch 2400, Loss: 1.9863, LR: 0.000024
2025-10-23 14:33:59,523 - INFO - Batch 2500, Loss: 2.1635, LR: 0.000024
2025-10-23 14:34:12,232 - INFO - Batch 2600, Loss: 2.2476, LR: 0.000024
2025-10-23 14:34:25,002 - INFO - Batch 2700, Loss: 2.0480, LR: 0.000024
2025-10-23 14:34:37,743 - INFO - Batch 2800, Loss: 2.3327, LR: 0.000023
2025-10-23 14:34:50,414 - INFO - Batch 2900, Loss: 2.3698, LR: 0.000023
2025-10-23 14:35:03,126 - INFO - Batch 3000, Loss: 2.1449, LR: 0.000023
2025-10-23 14:35:15,848 - INFO - Batch 3100, Loss: 2.3776, LR: 0.000023
2025-10-23 14:35:28,585 - INFO - Batch 3200, Loss: 2.1276, LR: 0.000023
2025-10-23 14:35:41,257 - INFO - Batch 3300, Loss: 2.4103, LR: 0.000023
2025-10-23 14:35:53,994 - INFO - Batch 3400, Loss: 2.4004, LR: 0.000023
2025-10-23 14:36:06,824 - INFO - Batch 3500, Loss: 2.3777, LR: 0.000023
2025-10-23 14:36:19,496 - INFO - Batch 3600, Loss: 2.3434, LR: 0.000023
2025-10-23 14:36:32,187 - INFO - Batch 3700, Loss: 2.1197, LR: 0.000023
2025-10-23 14:36:44,845 - INFO - Batch 3800, Loss: 2.4471, LR: 0.000023
2025-10-23 14:36:57,609 - INFO - Batch 3900, Loss: 2.1436, LR: 0.000023
2025-10-23 14:37:10,324 - INFO - Batch 4000, Loss: 2.2526, LR: 0.000023
2025-10-23 14:37:23,071 - INFO - Batch 4100, Loss: 2.3539, LR: 0.000023
2025-10-23 14:37:35,902 - INFO - Batch 4200, Loss: 2.0691, LR: 0.000023
2025-10-23 14:37:48,712 - INFO - Batch 4300, Loss: 2.2688, LR: 0.000022
2025-10-23 14:38:01,445 - INFO - Batch 4400, Loss: 2.1035, LR: 0.000022
2025-10-23 14:38:14,112 - INFO - Batch 4500, Loss: 2.1767, LR: 0.000022
2025-10-23 14:38:26,746 - INFO - Batch 4600, Loss: 2.1383, LR: 0.000022
2025-10-23 14:38:39,392 - INFO - Batch 4700, Loss: 2.3149, LR: 0.000022
2025-10-23 14:38:52,183 - INFO - Batch 4800, Loss: 2.0096, LR: 0.000022
2025-10-23 14:39:04,735 - INFO - Batch 4900, Loss: 2.0139, LR: 0.000022
2025-10-23 14:39:17,164 - INFO - Batch 5000, Loss: 2.1889, LR: 0.000022
2025-10-23 14:39:29,594 - INFO - Batch 5100, Loss: 2.3154, LR: 0.000022
2025-10-23 14:39:42,044 - INFO - Batch 5200, Loss: 2.2445, LR: 0.000022
2025-10-23 14:39:54,575 - INFO - Batch 5300, Loss: 2.3530, LR: 0.000022
2025-10-23 14:40:07,040 - INFO - Batch 5400, Loss: 2.3760, LR: 0.000022
2025-10-23 14:40:19,498 - INFO - Batch 5500, Loss: 2.3466, LR: 0.000022
2025-10-23 14:40:31,953 - INFO - Batch 5600, Loss: 2.1129, LR: 0.000022
2025-10-23 14:40:44,515 - INFO - Batch 5700, Loss: 2.2985, LR: 0.000022
2025-10-23 14:40:57,006 - INFO - Batch 5800, Loss: 2.2697, LR: 0.000021
2025-10-23 14:41:09,554 - INFO - Batch 5900, Loss: 2.3935, LR: 0.000021
2025-10-23 14:41:22,123 - INFO - Batch 6000, Loss: 2.3559, LR: 0.000021
2025-10-23 14:41:34,715 - INFO - Batch 6100, Loss: 2.1945, LR: 0.000021
2025-10-23 14:41:47,306 - INFO - Batch 6200, Loss: 2.1535, LR: 0.000021
2025-10-23 14:41:59,882 - INFO - Batch 6300, Loss: 2.6446, LR: 0.000021
2025-10-23 14:42:12,449 - INFO - Batch 6400, Loss: 2.4477, LR: 0.000021
2025-10-23 14:42:18,777 - INFO - Epoch 21/30: Train Loss: 2.2756, Val Loss: 2.1894, LR: 0.000021
2025-10-23 14:42:19,076 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 14:42:19,219 - INFO - Batch 0, Loss: 2.3501, LR: 0.000021
2025-10-23 14:42:31,691 - INFO - Batch 100, Loss: 2.4608, LR: 0.000021
2025-10-23 14:42:44,120 - INFO - Batch 200, Loss: 2.2286, LR: 0.000021
2025-10-23 14:42:56,595 - INFO - Batch 300, Loss: 2.1727, LR: 0.000021
2025-10-23 14:43:09,001 - INFO - Batch 400, Loss: 2.2329, LR: 0.000021
2025-10-23 14:43:21,362 - INFO - Batch 500, Loss: 2.4664, LR: 0.000021
2025-10-23 14:43:33,701 - INFO - Batch 600, Loss: 2.1597, LR: 0.000021
2025-10-23 14:43:46,152 - INFO - Batch 700, Loss: 2.1850, LR: 0.000021
2025-10-23 14:43:58,637 - INFO - Batch 800, Loss: 2.2489, LR: 0.000020
2025-10-23 14:44:11,158 - INFO - Batch 900, Loss: 2.2140, LR: 0.000020
2025-10-23 14:44:23,619 - INFO - Batch 1000, Loss: 2.3596, LR: 0.000020
2025-10-23 14:44:36,017 - INFO - Batch 1100, Loss: 2.3774, LR: 0.000020
2025-10-23 14:44:48,394 - INFO - Batch 1200, Loss: 2.2626, LR: 0.000020
2025-10-23 14:45:00,720 - INFO - Batch 1300, Loss: 2.2455, LR: 0.000020
2025-10-23 14:45:13,114 - INFO - Batch 1400, Loss: 2.2460, LR: 0.000020
2025-10-23 14:45:25,483 - INFO - Batch 1500, Loss: 2.3148, LR: 0.000020
2025-10-23 14:45:37,857 - INFO - Batch 1600, Loss: 2.0202, LR: 0.000020
2025-10-23 14:45:50,341 - INFO - Batch 1700, Loss: 2.2024, LR: 0.000020
2025-10-23 14:46:02,854 - INFO - Batch 1800, Loss: 2.3421, LR: 0.000020
2025-10-23 14:46:15,321 - INFO - Batch 1900, Loss: 2.0160, LR: 0.000020
2025-10-23 14:46:27,711 - INFO - Batch 2000, Loss: 2.3005, LR: 0.000020
2025-10-23 14:46:40,091 - INFO - Batch 2100, Loss: 2.2631, LR: 0.000020
2025-10-23 14:46:52,105 - INFO - Batch 2200, Loss: 1.9131, LR: 0.000020
2025-10-23 14:47:04,124 - INFO - Batch 2300, Loss: 2.3610, LR: 0.000019
2025-10-23 14:47:16,183 - INFO - Batch 2400, Loss: 2.0554, LR: 0.000019
2025-10-23 14:47:28,653 - INFO - Batch 2500, Loss: 2.4940, LR: 0.000019
2025-10-23 14:47:41,117 - INFO - Batch 2600, Loss: 2.1692, LR: 0.000019
2025-10-23 14:47:53,686 - INFO - Batch 2700, Loss: 2.2881, LR: 0.000019
2025-10-23 14:48:06,069 - INFO - Batch 2800, Loss: 2.3859, LR: 0.000019
2025-10-23 14:48:18,443 - INFO - Batch 2900, Loss: 2.2409, LR: 0.000019
2025-10-23 14:48:31,038 - INFO - Batch 3000, Loss: 2.4556, LR: 0.000019
2025-10-23 14:48:43,539 - INFO - Batch 3100, Loss: 2.5306, LR: 0.000019
2025-10-23 14:48:55,993 - INFO - Batch 3200, Loss: 2.2210, LR: 0.000019
2025-10-23 14:49:08,317 - INFO - Batch 3300, Loss: 2.5151, LR: 0.000019
2025-10-23 14:49:20,886 - INFO - Batch 3400, Loss: 2.3130, LR: 0.000019
2025-10-23 14:49:33,635 - INFO - Batch 3500, Loss: 2.2852, LR: 0.000019
2025-10-23 14:49:46,338 - INFO - Batch 3600, Loss: 2.1898, LR: 0.000019
2025-10-23 14:49:59,033 - INFO - Batch 3700, Loss: 2.0490, LR: 0.000019
2025-10-23 14:50:11,635 - INFO - Batch 3800, Loss: 2.3922, LR: 0.000019
2025-10-23 14:50:24,216 - INFO - Batch 3900, Loss: 2.2386, LR: 0.000018
2025-10-23 14:50:36,778 - INFO - Batch 4000, Loss: 2.4213, LR: 0.000018
2025-10-23 14:50:49,265 - INFO - Batch 4100, Loss: 2.2712, LR: 0.000018
2025-10-23 14:51:01,804 - INFO - Batch 4200, Loss: 2.4225, LR: 0.000018
2025-10-23 14:51:14,414 - INFO - Batch 4300, Loss: 2.4570, LR: 0.000018
2025-10-23 14:51:26,917 - INFO - Batch 4400, Loss: 2.0708, LR: 0.000018
2025-10-23 14:51:39,382 - INFO - Batch 4500, Loss: 2.1554, LR: 0.000018
2025-10-23 14:51:51,898 - INFO - Batch 4600, Loss: 2.2933, LR: 0.000018
2025-10-23 14:52:04,548 - INFO - Batch 4700, Loss: 2.2261, LR: 0.000018
2025-10-23 14:52:17,238 - INFO - Batch 4800, Loss: 2.3255, LR: 0.000018
2025-10-23 14:52:29,826 - INFO - Batch 4900, Loss: 2.1690, LR: 0.000018
2025-10-23 14:52:42,318 - INFO - Batch 5000, Loss: 2.4482, LR: 0.000018
2025-10-23 14:52:54,919 - INFO - Batch 5100, Loss: 2.5167, LR: 0.000018
2025-10-23 14:53:07,601 - INFO - Batch 5200, Loss: 2.2450, LR: 0.000018
2025-10-23 14:53:20,213 - INFO - Batch 5300, Loss: 2.0727, LR: 0.000018
2025-10-23 14:53:32,806 - INFO - Batch 5400, Loss: 2.1756, LR: 0.000018
2025-10-23 14:53:45,341 - INFO - Batch 5500, Loss: 2.5654, LR: 0.000017
2025-10-23 14:53:57,954 - INFO - Batch 5600, Loss: 2.1146, LR: 0.000017
2025-10-23 14:54:10,533 - INFO - Batch 5700, Loss: 2.2342, LR: 0.000017
2025-10-23 14:54:23,184 - INFO - Batch 5800, Loss: 2.2586, LR: 0.000017
2025-10-23 14:54:35,842 - INFO - Batch 5900, Loss: 2.0010, LR: 0.000017
2025-10-23 14:54:48,377 - INFO - Batch 6000, Loss: 2.2441, LR: 0.000017
2025-10-23 14:55:00,942 - INFO - Batch 6100, Loss: 2.3320, LR: 0.000017
2025-10-23 14:55:13,556 - INFO - Batch 6200, Loss: 2.4095, LR: 0.000017
2025-10-23 14:55:27,316 - INFO - Batch 6300, Loss: 2.2953, LR: 0.000017
2025-10-23 14:55:41,778 - INFO - Batch 6400, Loss: 2.0585, LR: 0.000017
2025-10-23 14:55:48,995 - INFO - Epoch 22/30: Train Loss: 2.2637, Val Loss: 2.1872, LR: 0.000017
2025-10-23 14:55:49,276 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 14:55:49,426 - INFO - Batch 0, Loss: 2.1907, LR: 0.000017
2025-10-23 14:56:03,321 - INFO - Batch 100, Loss: 2.4301, LR: 0.000017
2025-10-23 14:56:18,115 - INFO - Batch 200, Loss: 2.1877, LR: 0.000017
2025-10-23 14:56:32,568 - INFO - Batch 300, Loss: 2.2491, LR: 0.000017
2025-10-23 14:56:47,862 - INFO - Batch 400, Loss: 2.4665, LR: 0.000017
2025-10-23 14:57:02,692 - INFO - Batch 500, Loss: 1.9682, LR: 0.000017
2025-10-23 14:57:17,288 - INFO - Batch 600, Loss: 2.2566, LR: 0.000017
2025-10-23 14:57:32,250 - INFO - Batch 700, Loss: 2.2306, LR: 0.000016
2025-10-23 14:57:46,842 - INFO - Batch 800, Loss: 2.1587, LR: 0.000016
2025-10-23 14:58:01,649 - INFO - Batch 900, Loss: 2.3584, LR: 0.000016
2025-10-23 14:58:16,179 - INFO - Batch 1000, Loss: 2.1067, LR: 0.000016
2025-10-23 14:58:31,191 - INFO - Batch 1100, Loss: 2.1028, LR: 0.000016
2025-10-23 14:58:45,939 - INFO - Batch 1200, Loss: 2.2900, LR: 0.000016
2025-10-23 14:59:00,626 - INFO - Batch 1300, Loss: 1.9527, LR: 0.000016
2025-10-23 14:59:15,022 - INFO - Batch 1400, Loss: 2.4614, LR: 0.000016
2025-10-23 14:59:30,057 - INFO - Batch 1500, Loss: 2.3283, LR: 0.000016
2025-10-23 14:59:44,679 - INFO - Batch 1600, Loss: 2.2760, LR: 0.000016
2025-10-23 14:59:59,570 - INFO - Batch 1700, Loss: 2.1864, LR: 0.000016
2025-10-23 15:00:15,060 - INFO - Batch 1800, Loss: 2.3684, LR: 0.000016
2025-10-23 15:00:35,356 - INFO - Batch 1900, Loss: 2.1848, LR: 0.000016
2025-10-23 15:00:57,481 - INFO - Batch 2000, Loss: 2.2654, LR: 0.000016
2025-10-23 15:01:19,534 - INFO - Batch 2100, Loss: 2.3911, LR: 0.000016
2025-10-23 15:01:39,002 - INFO - Batch 2200, Loss: 1.9786, LR: 0.000016
2025-10-23 15:01:57,619 - INFO - Batch 2300, Loss: 1.9396, LR: 0.000015
2025-10-23 15:02:15,700 - INFO - Batch 2400, Loss: 2.2216, LR: 0.000015
2025-10-23 15:02:31,866 - INFO - Batch 2500, Loss: 2.3863, LR: 0.000015
2025-10-23 15:02:55,344 - INFO - Batch 2600, Loss: 1.8318, LR: 0.000015
2025-10-23 15:03:14,216 - INFO - Batch 2700, Loss: 2.0753, LR: 0.000015
2025-10-23 15:03:34,063 - INFO - Batch 2800, Loss: 2.5916, LR: 0.000015
2025-10-23 15:03:52,930 - INFO - Batch 2900, Loss: 2.2270, LR: 0.000015
2025-10-23 15:04:10,521 - INFO - Batch 3000, Loss: 2.2387, LR: 0.000015
2025-10-23 15:04:25,983 - INFO - Batch 3100, Loss: 2.0220, LR: 0.000015
2025-10-23 15:04:42,988 - INFO - Batch 3200, Loss: 2.3991, LR: 0.000015
2025-10-23 15:05:01,487 - INFO - Batch 3300, Loss: 2.2231, LR: 0.000015
2025-10-23 15:05:19,887 - INFO - Batch 3400, Loss: 2.3685, LR: 0.000015
2025-10-23 15:05:38,025 - INFO - Batch 3500, Loss: 2.2225, LR: 0.000015
2025-10-23 15:05:55,466 - INFO - Batch 3600, Loss: 2.2553, LR: 0.000015
2025-10-23 15:06:15,379 - INFO - Batch 3700, Loss: 2.1557, LR: 0.000015
2025-10-23 15:06:35,564 - INFO - Batch 3800, Loss: 2.2463, LR: 0.000015
2025-10-23 15:06:54,373 - INFO - Batch 3900, Loss: 2.2275, LR: 0.000015
2025-10-23 15:07:15,818 - INFO - Batch 4000, Loss: 2.4407, LR: 0.000014
2025-10-23 15:07:33,181 - INFO - Batch 4100, Loss: 2.6708, LR: 0.000014
2025-10-23 15:07:51,234 - INFO - Batch 4200, Loss: 2.2227, LR: 0.000014
2025-10-23 15:08:09,524 - INFO - Batch 4300, Loss: 2.3136, LR: 0.000014
2025-10-23 15:08:32,653 - INFO - Batch 4400, Loss: 2.3211, LR: 0.000014
2025-10-23 15:08:49,515 - INFO - Batch 4500, Loss: 2.3019, LR: 0.000014
2025-10-23 15:09:07,951 - INFO - Batch 4600, Loss: 2.3545, LR: 0.000014
2025-10-23 15:09:21,263 - INFO - Batch 4700, Loss: 2.2460, LR: 0.000014
2025-10-23 15:09:33,963 - INFO - Batch 4800, Loss: 2.0628, LR: 0.000014
2025-10-23 15:09:46,811 - INFO - Batch 4900, Loss: 2.2655, LR: 0.000014
2025-10-23 15:10:00,737 - INFO - Batch 5000, Loss: 2.2244, LR: 0.000014
2025-10-23 15:10:14,528 - INFO - Batch 5100, Loss: 2.2176, LR: 0.000014
2025-10-23 15:10:28,321 - INFO - Batch 5200, Loss: 2.3398, LR: 0.000014
2025-10-23 15:10:41,386 - INFO - Batch 5300, Loss: 2.2088, LR: 0.000014
2025-10-23 15:10:54,150 - INFO - Batch 5400, Loss: 2.2652, LR: 0.000014
2025-10-23 15:11:09,908 - INFO - Batch 5500, Loss: 2.2606, LR: 0.000014
2025-10-23 15:11:28,450 - INFO - Batch 5600, Loss: 1.9834, LR: 0.000014
2025-10-23 15:11:45,644 - INFO - Batch 5700, Loss: 2.1525, LR: 0.000014
2025-10-23 15:12:07,254 - INFO - Batch 5800, Loss: 2.2002, LR: 0.000013
2025-10-23 15:12:27,582 - INFO - Batch 5900, Loss: 2.2364, LR: 0.000013
2025-10-23 15:12:48,834 - INFO - Batch 6000, Loss: 2.1450, LR: 0.000013
2025-10-23 15:13:07,231 - INFO - Batch 6100, Loss: 2.3268, LR: 0.000013
2025-10-23 15:13:28,685 - INFO - Batch 6200, Loss: 2.3830, LR: 0.000013
2025-10-23 15:13:46,875 - INFO - Batch 6300, Loss: 2.1208, LR: 0.000013
2025-10-23 15:14:03,931 - INFO - Batch 6400, Loss: 2.2015, LR: 0.000013
2025-10-23 15:14:10,674 - INFO - Epoch 23/30: Train Loss: 2.2530, Val Loss: 2.1782, LR: 0.000013
2025-10-23 15:14:11,477 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 15:14:11,656 - INFO - Batch 0, Loss: 2.3291, LR: 0.000013
2025-10-23 15:14:30,683 - INFO - Batch 100, Loss: 2.0434, LR: 0.000013
2025-10-23 15:14:48,584 - INFO - Batch 200, Loss: 2.0844, LR: 0.000013
2025-10-23 15:15:04,780 - INFO - Batch 300, Loss: 2.0698, LR: 0.000013
2025-10-23 15:15:25,114 - INFO - Batch 400, Loss: 2.3078, LR: 0.000013
2025-10-23 15:15:44,795 - INFO - Batch 500, Loss: 2.0524, LR: 0.000013
2025-10-23 15:16:04,155 - INFO - Batch 600, Loss: 2.2108, LR: 0.000013
2025-10-23 15:16:26,697 - INFO - Batch 700, Loss: 2.0295, LR: 0.000013
2025-10-23 15:16:48,022 - INFO - Batch 800, Loss: 2.2328, LR: 0.000013
2025-10-23 15:17:05,119 - INFO - Batch 900, Loss: 2.1936, LR: 0.000013
2025-10-23 15:17:24,147 - INFO - Batch 1000, Loss: 2.0514, LR: 0.000013
2025-10-23 15:17:42,230 - INFO - Batch 1100, Loss: 2.4702, LR: 0.000012
2025-10-23 15:18:00,302 - INFO - Batch 1200, Loss: 2.2576, LR: 0.000012
2025-10-23 15:18:19,367 - INFO - Batch 1300, Loss: 2.2681, LR: 0.000012
2025-10-23 15:18:39,368 - INFO - Batch 1400, Loss: 2.1889, LR: 0.000012
2025-10-23 15:18:58,978 - INFO - Batch 1500, Loss: 2.0771, LR: 0.000012
2025-10-23 15:19:17,205 - INFO - Batch 1600, Loss: 2.0186, LR: 0.000012
2025-10-23 15:19:32,708 - INFO - Batch 1700, Loss: 2.1079, LR: 0.000012
2025-10-23 15:19:53,059 - INFO - Batch 1800, Loss: 2.1486, LR: 0.000012
2025-10-23 15:20:13,478 - INFO - Batch 1900, Loss: 2.1538, LR: 0.000012
2025-10-23 15:20:31,997 - INFO - Batch 2000, Loss: 2.1634, LR: 0.000012
2025-10-23 15:20:49,971 - INFO - Batch 2100, Loss: 2.3424, LR: 0.000012
2025-10-23 15:21:07,799 - INFO - Batch 2200, Loss: 2.3343, LR: 0.000012
2025-10-23 15:21:32,362 - INFO - Batch 2300, Loss: 2.3513, LR: 0.000012
2025-10-23 15:21:53,013 - INFO - Batch 2400, Loss: 2.1334, LR: 0.000012
2025-10-23 15:22:12,720 - INFO - Batch 2500, Loss: 2.0206, LR: 0.000012
2025-10-23 15:22:34,945 - INFO - Batch 2600, Loss: 2.2703, LR: 0.000012
2025-10-23 15:22:57,728 - INFO - Batch 2700, Loss: 2.2816, LR: 0.000012
2025-10-23 15:23:15,634 - INFO - Batch 2800, Loss: 2.1729, LR: 0.000012
2025-10-23 15:23:33,503 - INFO - Batch 2900, Loss: 1.9707, LR: 0.000012
2025-10-23 15:23:55,401 - INFO - Batch 3000, Loss: 2.3106, LR: 0.000011
2025-10-23 15:24:13,850 - INFO - Batch 3100, Loss: 2.3823, LR: 0.000011
2025-10-23 15:24:32,775 - INFO - Batch 3200, Loss: 2.4555, LR: 0.000011
2025-10-23 15:24:51,072 - INFO - Batch 3300, Loss: 2.2571, LR: 0.000011
2025-10-23 15:25:10,586 - INFO - Batch 3400, Loss: 2.0266, LR: 0.000011
2025-10-23 15:25:32,997 - INFO - Batch 3500, Loss: 2.3881, LR: 0.000011
2025-10-23 15:25:55,101 - INFO - Batch 3600, Loss: 1.8617, LR: 0.000011
2025-10-23 15:26:15,201 - INFO - Batch 3700, Loss: 2.2785, LR: 0.000011
2025-10-23 15:26:34,657 - INFO - Batch 3800, Loss: 2.4131, LR: 0.000011
2025-10-23 15:26:55,854 - INFO - Batch 3900, Loss: 2.1762, LR: 0.000011
2025-10-23 15:27:16,082 - INFO - Batch 4000, Loss: 2.0323, LR: 0.000011
2025-10-23 15:27:35,374 - INFO - Batch 4100, Loss: 2.3776, LR: 0.000011
2025-10-23 15:27:53,681 - INFO - Batch 4200, Loss: 2.1434, LR: 0.000011
2025-10-23 15:28:12,775 - INFO - Batch 4300, Loss: 2.3223, LR: 0.000011
2025-10-23 15:28:28,802 - INFO - Batch 4400, Loss: 2.0829, LR: 0.000011
2025-10-23 15:28:45,551 - INFO - Batch 4500, Loss: 2.3179, LR: 0.000011
2025-10-23 15:29:03,515 - INFO - Batch 4600, Loss: 2.2244, LR: 0.000011
2025-10-23 15:29:24,904 - INFO - Batch 4700, Loss: 2.0851, LR: 0.000011
2025-10-23 15:29:42,566 - INFO - Batch 4800, Loss: 2.0946, LR: 0.000011
2025-10-23 15:30:02,594 - INFO - Batch 4900, Loss: 2.2279, LR: 0.000011
2025-10-23 15:30:19,949 - INFO - Batch 5000, Loss: 2.1170, LR: 0.000010
2025-10-23 15:30:41,276 - INFO - Batch 5100, Loss: 2.1939, LR: 0.000010
2025-10-23 15:30:54,965 - INFO - Batch 5200, Loss: 2.1483, LR: 0.000010
2025-10-23 15:31:08,721 - INFO - Batch 5300, Loss: 2.3123, LR: 0.000010
2025-10-23 15:31:21,419 - INFO - Batch 5400, Loss: 2.1007, LR: 0.000010
2025-10-23 15:31:37,348 - INFO - Batch 5500, Loss: 2.1174, LR: 0.000010
2025-10-23 15:31:58,220 - INFO - Batch 5600, Loss: 2.1278, LR: 0.000010
2025-10-23 15:32:19,905 - INFO - Batch 5700, Loss: 2.3173, LR: 0.000010
2025-10-23 15:32:41,523 - INFO - Batch 5800, Loss: 2.3202, LR: 0.000010
2025-10-23 15:32:58,657 - INFO - Batch 5900, Loss: 2.4996, LR: 0.000010
2025-10-23 15:33:17,926 - INFO - Batch 6000, Loss: 2.1670, LR: 0.000010
2025-10-23 15:33:35,700 - INFO - Batch 6100, Loss: 2.6381, LR: 0.000010
2025-10-23 15:33:55,852 - INFO - Batch 6200, Loss: 2.2820, LR: 0.000010
2025-10-23 15:34:13,532 - INFO - Batch 6300, Loss: 2.2768, LR: 0.000010
2025-10-23 15:34:34,006 - INFO - Batch 6400, Loss: 2.1969, LR: 0.000010
2025-10-23 15:34:43,867 - INFO - Epoch 24/30: Train Loss: 2.2425, Val Loss: 2.1756, LR: 0.000010
2025-10-23 15:34:44,421 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 15:34:44,654 - INFO - Batch 0, Loss: 2.2383, LR: 0.000010
2025-10-23 15:35:06,234 - INFO - Batch 100, Loss: 2.1512, LR: 0.000010
2025-10-23 15:35:24,359 - INFO - Batch 200, Loss: 2.1931, LR: 0.000010
2025-10-23 15:35:44,902 - INFO - Batch 300, Loss: 2.3092, LR: 0.000010
2025-10-23 15:36:03,150 - INFO - Batch 400, Loss: 2.1905, LR: 0.000010
2025-10-23 15:36:24,476 - INFO - Batch 500, Loss: 2.2952, LR: 0.000010
2025-10-23 15:36:41,765 - INFO - Batch 600, Loss: 2.2868, LR: 0.000009
2025-10-23 15:36:59,768 - INFO - Batch 700, Loss: 2.2972, LR: 0.000009
2025-10-23 15:37:21,522 - INFO - Batch 800, Loss: 2.3124, LR: 0.000009
2025-10-23 15:37:40,178 - INFO - Batch 900, Loss: 2.1319, LR: 0.000009
2025-10-23 15:37:59,750 - INFO - Batch 1000, Loss: 2.2810, LR: 0.000009
2025-10-23 15:38:20,753 - INFO - Batch 1100, Loss: 2.2298, LR: 0.000009
2025-10-23 15:38:40,842 - INFO - Batch 1200, Loss: 2.3534, LR: 0.000009
2025-10-23 15:38:56,136 - INFO - Batch 1300, Loss: 2.3025, LR: 0.000009
2025-10-23 15:39:09,814 - INFO - Batch 1400, Loss: 2.0816, LR: 0.000009
2025-10-23 15:39:23,845 - INFO - Batch 1500, Loss: 2.2559, LR: 0.000009
2025-10-23 15:39:42,562 - INFO - Batch 1600, Loss: 2.2914, LR: 0.000009
2025-10-23 15:40:01,048 - INFO - Batch 1700, Loss: 2.3449, LR: 0.000009
2025-10-23 15:40:24,325 - INFO - Batch 1800, Loss: 2.4463, LR: 0.000009
2025-10-23 15:40:42,381 - INFO - Batch 1900, Loss: 2.4357, LR: 0.000009
2025-10-23 15:40:59,546 - INFO - Batch 2000, Loss: 2.3543, LR: 0.000009
2025-10-23 15:41:19,295 - INFO - Batch 2100, Loss: 2.0292, LR: 0.000009
2025-10-23 15:41:40,219 - INFO - Batch 2200, Loss: 2.3337, LR: 0.000009
2025-10-23 15:42:00,594 - INFO - Batch 2300, Loss: 2.5491, LR: 0.000009
2025-10-23 15:42:18,533 - INFO - Batch 2400, Loss: 2.2756, LR: 0.000009
2025-10-23 15:42:34,395 - INFO - Batch 2500, Loss: 2.3628, LR: 0.000009
2025-10-23 15:42:53,839 - INFO - Batch 2600, Loss: 2.0137, LR: 0.000009
2025-10-23 15:43:08,447 - INFO - Batch 2700, Loss: 2.3393, LR: 0.000008
2025-10-23 15:43:25,791 - INFO - Batch 2800, Loss: 2.4237, LR: 0.000008
2025-10-23 15:43:44,390 - INFO - Batch 2900, Loss: 2.1327, LR: 0.000008
2025-10-23 15:44:02,967 - INFO - Batch 3000, Loss: 2.2463, LR: 0.000008
2025-10-23 15:44:23,788 - INFO - Batch 3100, Loss: 2.1166, LR: 0.000008
2025-10-23 15:44:47,109 - INFO - Batch 3200, Loss: 2.3716, LR: 0.000008
2025-10-23 15:45:08,148 - INFO - Batch 3300, Loss: 1.9909, LR: 0.000008
2025-10-23 15:45:25,658 - INFO - Batch 3400, Loss: 2.2490, LR: 0.000008
2025-10-23 15:45:44,755 - INFO - Batch 3500, Loss: 2.4428, LR: 0.000008
2025-10-23 15:46:05,436 - INFO - Batch 3600, Loss: 2.2303, LR: 0.000008
2025-10-23 15:46:23,928 - INFO - Batch 3700, Loss: 2.3110, LR: 0.000008
2025-10-23 15:46:43,657 - INFO - Batch 3800, Loss: 2.0843, LR: 0.000008
2025-10-23 15:47:01,990 - INFO - Batch 3900, Loss: 2.4491, LR: 0.000008
2025-10-23 15:47:20,332 - INFO - Batch 4000, Loss: 2.1758, LR: 0.000008
2025-10-23 15:47:40,565 - INFO - Batch 4100, Loss: 2.2038, LR: 0.000008
2025-10-23 15:48:00,745 - INFO - Batch 4200, Loss: 2.2509, LR: 0.000008
2025-10-23 15:48:21,877 - INFO - Batch 4300, Loss: 2.3705, LR: 0.000008
2025-10-23 15:48:41,079 - INFO - Batch 4400, Loss: 2.0391, LR: 0.000008
2025-10-23 15:49:01,700 - INFO - Batch 4500, Loss: 2.2121, LR: 0.000008
2025-10-23 15:49:19,118 - INFO - Batch 4600, Loss: 2.1507, LR: 0.000008
2025-10-23 15:49:43,065 - INFO - Batch 4700, Loss: 2.6248, LR: 0.000008
2025-10-23 15:50:01,308 - INFO - Batch 4800, Loss: 2.6734, LR: 0.000008
2025-10-23 15:50:21,070 - INFO - Batch 4900, Loss: 2.2583, LR: 0.000007
2025-10-23 15:50:41,946 - INFO - Batch 5000, Loss: 2.2591, LR: 0.000007
2025-10-23 15:51:04,386 - INFO - Batch 5100, Loss: 2.3612, LR: 0.000007
2025-10-23 15:51:24,305 - INFO - Batch 5200, Loss: 2.3770, LR: 0.000007
2025-10-23 15:51:43,502 - INFO - Batch 5300, Loss: 2.4912, LR: 0.000007
2025-10-23 15:52:02,387 - INFO - Batch 5400, Loss: 2.0870, LR: 0.000007
2025-10-23 15:52:24,884 - INFO - Batch 5500, Loss: 2.0422, LR: 0.000007
2025-10-23 15:52:43,691 - INFO - Batch 5600, Loss: 2.2185, LR: 0.000007
2025-10-23 15:53:02,626 - INFO - Batch 5700, Loss: 2.0200, LR: 0.000007
2025-10-23 15:53:22,574 - INFO - Batch 5800, Loss: 2.4310, LR: 0.000007
2025-10-23 15:53:41,624 - INFO - Batch 5900, Loss: 2.1810, LR: 0.000007
2025-10-23 15:53:59,538 - INFO - Batch 6000, Loss: 2.1991, LR: 0.000007
2025-10-23 15:54:20,030 - INFO - Batch 6100, Loss: 2.2635, LR: 0.000007
2025-10-23 15:54:39,628 - INFO - Batch 6200, Loss: 2.4058, LR: 0.000007
2025-10-23 15:54:56,655 - INFO - Batch 6300, Loss: 2.0934, LR: 0.000007
2025-10-23 15:55:17,219 - INFO - Batch 6400, Loss: 2.2972, LR: 0.000007
2025-10-23 15:55:26,231 - INFO - Epoch 25/30: Train Loss: 2.2332, Val Loss: 2.1697, LR: 0.000007
2025-10-23 15:55:26,632 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 15:55:26,831 - INFO - Batch 0, Loss: 2.1354, LR: 0.000007
2025-10-23 15:55:45,116 - INFO - Batch 100, Loss: 2.3723, LR: 0.000007
2025-10-23 15:56:03,767 - INFO - Batch 200, Loss: 2.1780, LR: 0.000007
2025-10-23 15:56:22,365 - INFO - Batch 300, Loss: 2.0040, LR: 0.000007
2025-10-23 15:56:41,381 - INFO - Batch 400, Loss: 2.4415, LR: 0.000007
2025-10-23 15:57:06,758 - INFO - Batch 500, Loss: 2.0703, LR: 0.000007
2025-10-23 15:57:24,630 - INFO - Batch 600, Loss: 2.3914, LR: 0.000007
2025-10-23 15:57:43,906 - INFO - Batch 700, Loss: 2.1890, LR: 0.000007
2025-10-23 15:58:01,760 - INFO - Batch 800, Loss: 2.1156, LR: 0.000007
2025-10-23 15:58:18,990 - INFO - Batch 900, Loss: 2.1038, LR: 0.000006
2025-10-23 15:58:36,760 - INFO - Batch 1000, Loss: 2.1943, LR: 0.000006
2025-10-23 15:58:56,693 - INFO - Batch 1100, Loss: 2.4780, LR: 0.000006
2025-10-23 15:59:15,517 - INFO - Batch 1200, Loss: 2.1068, LR: 0.000006
2025-10-23 15:59:39,578 - INFO - Batch 1300, Loss: 2.1439, LR: 0.000006
2025-10-23 16:00:01,406 - INFO - Batch 1400, Loss: 2.4371, LR: 0.000006
2025-10-23 16:00:19,618 - INFO - Batch 1500, Loss: 2.3301, LR: 0.000006
2025-10-23 16:00:38,967 - INFO - Batch 1600, Loss: 2.2411, LR: 0.000006
2025-10-23 16:00:57,715 - INFO - Batch 1700, Loss: 2.2824, LR: 0.000006
2025-10-23 16:01:16,601 - INFO - Batch 1800, Loss: 2.1176, LR: 0.000006
2025-10-23 16:01:38,320 - INFO - Batch 1900, Loss: 2.0551, LR: 0.000006
2025-10-23 16:01:56,038 - INFO - Batch 2000, Loss: 2.3615, LR: 0.000006
2025-10-23 16:02:15,513 - INFO - Batch 2100, Loss: 2.1694, LR: 0.000006
2025-10-23 16:02:34,872 - INFO - Batch 2200, Loss: 2.0840, LR: 0.000006
2025-10-23 16:02:57,583 - INFO - Batch 2300, Loss: 2.2329, LR: 0.000006
2025-10-23 16:03:15,773 - INFO - Batch 2400, Loss: 2.0970, LR: 0.000006
2025-10-23 16:03:35,605 - INFO - Batch 2500, Loss: 2.3782, LR: 0.000006
2025-10-23 16:03:56,559 - INFO - Batch 2600, Loss: 2.2607, LR: 0.000006
2025-10-23 16:04:20,918 - INFO - Batch 2700, Loss: 2.0110, LR: 0.000006
2025-10-23 16:04:42,182 - INFO - Batch 2800, Loss: 2.2255, LR: 0.000006
2025-10-23 16:05:02,804 - INFO - Batch 2900, Loss: 2.3765, LR: 0.000006
2025-10-23 16:05:21,931 - INFO - Batch 3000, Loss: 2.0320, LR: 0.000006
2025-10-23 16:05:41,718 - INFO - Batch 3100, Loss: 2.3344, LR: 0.000006
2025-10-23 16:06:01,847 - INFO - Batch 3200, Loss: 2.3541, LR: 0.000006
2025-10-23 16:06:22,529 - INFO - Batch 3300, Loss: 2.2240, LR: 0.000006
2025-10-23 16:06:40,362 - INFO - Batch 3400, Loss: 2.0494, LR: 0.000005
2025-10-23 16:06:59,092 - INFO - Batch 3500, Loss: 2.3096, LR: 0.000005
2025-10-23 16:07:18,237 - INFO - Batch 3600, Loss: 2.3106, LR: 0.000005
2025-10-23 16:07:37,142 - INFO - Batch 3700, Loss: 2.3518, LR: 0.000005
2025-10-23 16:07:58,130 - INFO - Batch 3800, Loss: 2.2705, LR: 0.000005
2025-10-23 16:08:17,870 - INFO - Batch 3900, Loss: 2.0406, LR: 0.000005
2025-10-23 16:08:37,198 - INFO - Batch 4000, Loss: 2.2195, LR: 0.000005
2025-10-23 16:08:56,212 - INFO - Batch 4100, Loss: 2.2115, LR: 0.000005
2025-10-23 16:09:18,457 - INFO - Batch 4200, Loss: 2.2057, LR: 0.000005
2025-10-23 16:09:38,816 - INFO - Batch 4300, Loss: 2.3909, LR: 0.000005
2025-10-23 16:09:58,061 - INFO - Batch 4400, Loss: 2.3214, LR: 0.000005
2025-10-23 16:10:16,532 - INFO - Batch 4500, Loss: 2.0913, LR: 0.000005
2025-10-23 16:10:36,515 - INFO - Batch 4600, Loss: 2.1644, LR: 0.000005
2025-10-23 16:10:56,302 - INFO - Batch 4700, Loss: 1.9547, LR: 0.000005
2025-10-23 16:11:16,305 - INFO - Batch 4800, Loss: 2.3193, LR: 0.000005
2025-10-23 16:11:41,934 - INFO - Batch 4900, Loss: 2.2477, LR: 0.000005
2025-10-23 16:11:59,976 - INFO - Batch 5000, Loss: 2.2922, LR: 0.000005
2025-10-23 16:12:17,746 - INFO - Batch 5100, Loss: 2.1081, LR: 0.000005
2025-10-23 16:12:37,193 - INFO - Batch 5200, Loss: 2.1856, LR: 0.000005
2025-10-23 16:12:57,626 - INFO - Batch 5300, Loss: 2.4582, LR: 0.000005
2025-10-23 16:13:18,490 - INFO - Batch 5400, Loss: 2.3823, LR: 0.000005
2025-10-23 16:13:38,543 - INFO - Batch 5500, Loss: 2.3136, LR: 0.000005
2025-10-23 16:13:59,235 - INFO - Batch 5600, Loss: 2.0424, LR: 0.000005
2025-10-23 16:14:20,147 - INFO - Batch 5700, Loss: 2.1139, LR: 0.000005
2025-10-23 16:14:44,336 - INFO - Batch 5800, Loss: 2.2581, LR: 0.000005
2025-10-23 16:15:04,014 - INFO - Batch 5900, Loss: 2.1905, LR: 0.000005
2025-10-23 16:15:22,302 - INFO - Batch 6000, Loss: 2.2839, LR: 0.000005
2025-10-23 16:15:41,749 - INFO - Batch 6100, Loss: 1.8755, LR: 0.000005
2025-10-23 16:16:04,077 - INFO - Batch 6200, Loss: 2.1714, LR: 0.000004
2025-10-23 16:16:24,158 - INFO - Batch 6300, Loss: 2.1244, LR: 0.000004
2025-10-23 16:16:42,632 - INFO - Batch 6400, Loss: 2.4092, LR: 0.000004
2025-10-23 16:16:50,981 - INFO - Epoch 26/30: Train Loss: 2.2257, Val Loss: 2.1671, LR: 0.000004
2025-10-23 16:16:51,394 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 16:16:51,580 - INFO - Batch 0, Loss: 1.9545, LR: 0.000004
2025-10-23 16:17:11,224 - INFO - Batch 100, Loss: 2.2390, LR: 0.000004
2025-10-23 16:17:30,113 - INFO - Batch 200, Loss: 2.2309, LR: 0.000004
2025-10-23 16:17:49,455 - INFO - Batch 300, Loss: 2.0217, LR: 0.000004
2025-10-23 16:18:09,458 - INFO - Batch 400, Loss: 2.4582, LR: 0.000004
2025-10-23 16:18:27,074 - INFO - Batch 500, Loss: 2.3498, LR: 0.000004
2025-10-23 16:18:47,617 - INFO - Batch 600, Loss: 2.2010, LR: 0.000004
2025-10-23 16:19:09,388 - INFO - Batch 700, Loss: 2.0925, LR: 0.000004
2025-10-23 16:19:33,303 - INFO - Batch 800, Loss: 2.2969, LR: 0.000004
2025-10-23 16:19:53,471 - INFO - Batch 900, Loss: 2.2023, LR: 0.000004
2025-10-23 16:20:11,736 - INFO - Batch 1000, Loss: 2.1559, LR: 0.000004
2025-10-23 16:20:29,395 - INFO - Batch 1100, Loss: 2.1422, LR: 0.000004
2025-10-23 16:20:52,852 - INFO - Batch 1200, Loss: 1.8935, LR: 0.000004
2025-10-23 16:21:10,883 - INFO - Batch 1300, Loss: 2.3415, LR: 0.000004
2025-10-23 16:21:29,064 - INFO - Batch 1400, Loss: 2.1044, LR: 0.000004
2025-10-23 16:21:48,719 - INFO - Batch 1500, Loss: 2.2792, LR: 0.000004
2025-10-23 16:22:08,224 - INFO - Batch 1600, Loss: 2.3554, LR: 0.000004
2025-10-23 16:22:26,041 - INFO - Batch 1700, Loss: 2.4612, LR: 0.000004
2025-10-23 16:22:49,231 - INFO - Batch 1800, Loss: 2.1875, LR: 0.000004
2025-10-23 16:23:05,758 - INFO - Batch 1900, Loss: 2.4586, LR: 0.000004
2025-10-23 16:23:25,202 - INFO - Batch 2000, Loss: 2.0844, LR: 0.000004
2025-10-23 16:23:45,488 - INFO - Batch 2100, Loss: 2.1862, LR: 0.000004
2025-10-23 16:24:06,211 - INFO - Batch 2200, Loss: 2.2221, LR: 0.000004
2025-10-23 16:24:26,642 - INFO - Batch 2300, Loss: 2.2878, LR: 0.000004
2025-10-23 16:24:45,928 - INFO - Batch 2400, Loss: 2.2565, LR: 0.000004
2025-10-23 16:25:07,122 - INFO - Batch 2500, Loss: 2.4442, LR: 0.000004
2025-10-23 16:25:27,756 - INFO - Batch 2600, Loss: 1.9448, LR: 0.000004
2025-10-23 16:25:48,309 - INFO - Batch 2700, Loss: 2.1007, LR: 0.000004
2025-10-23 16:26:07,867 - INFO - Batch 2800, Loss: 2.2685, LR: 0.000004
2025-10-23 16:26:27,628 - INFO - Batch 2900, Loss: 2.0541, LR: 0.000003
2025-10-23 16:26:51,985 - INFO - Batch 3000, Loss: 2.4404, LR: 0.000003
2025-10-23 16:27:09,380 - INFO - Batch 3100, Loss: 2.5614, LR: 0.000003
2025-10-23 16:27:28,383 - INFO - Batch 3200, Loss: 2.2814, LR: 0.000003
2025-10-23 16:27:48,390 - INFO - Batch 3300, Loss: 1.9535, LR: 0.000003
2025-10-23 16:28:06,909 - INFO - Batch 3400, Loss: 2.1780, LR: 0.000003
2025-10-23 16:28:25,292 - INFO - Batch 3500, Loss: 2.2169, LR: 0.000003
2025-10-23 16:28:49,908 - INFO - Batch 3600, Loss: 2.3506, LR: 0.000003
2025-10-23 16:29:10,433 - INFO - Batch 3700, Loss: 2.1536, LR: 0.000003
2025-10-23 16:29:29,569 - INFO - Batch 3800, Loss: 2.4333, LR: 0.000003
2025-10-23 16:29:47,602 - INFO - Batch 3900, Loss: 2.4595, LR: 0.000003
2025-10-23 16:30:04,836 - INFO - Batch 4000, Loss: 2.3248, LR: 0.000003
2025-10-23 16:30:25,221 - INFO - Batch 4100, Loss: 2.3834, LR: 0.000003
2025-10-23 16:30:43,827 - INFO - Batch 4200, Loss: 2.2812, LR: 0.000003
2025-10-23 16:31:03,641 - INFO - Batch 4300, Loss: 2.2313, LR: 0.000003
2025-10-23 16:31:21,342 - INFO - Batch 4400, Loss: 2.2081, LR: 0.000003
2025-10-23 16:31:39,598 - INFO - Batch 4500, Loss: 2.1977, LR: 0.000003
2025-10-23 16:32:01,570 - INFO - Batch 4600, Loss: 2.2104, LR: 0.000003
2025-10-23 16:32:21,298 - INFO - Batch 4700, Loss: 2.2342, LR: 0.000003
2025-10-23 16:32:39,319 - INFO - Batch 4800, Loss: 2.3355, LR: 0.000003
2025-10-23 16:32:59,232 - INFO - Batch 4900, Loss: 2.0979, LR: 0.000003
2025-10-23 16:33:18,348 - INFO - Batch 5000, Loss: 2.0182, LR: 0.000003
2025-10-23 16:33:34,016 - INFO - Batch 5100, Loss: 1.9479, LR: 0.000003
2025-10-23 16:33:53,338 - INFO - Batch 5200, Loss: 2.2757, LR: 0.000003
2025-10-23 16:34:13,948 - INFO - Batch 5300, Loss: 2.4067, LR: 0.000003
2025-10-23 16:34:33,090 - INFO - Batch 5400, Loss: 2.0576, LR: 0.000003
2025-10-23 16:34:54,278 - INFO - Batch 5500, Loss: 2.0983, LR: 0.000003
2025-10-23 16:35:13,888 - INFO - Batch 5600, Loss: 2.3888, LR: 0.000003
2025-10-23 16:35:35,458 - INFO - Batch 5700, Loss: 2.2011, LR: 0.000003
2025-10-23 16:35:55,639 - INFO - Batch 5800, Loss: 2.2740, LR: 0.000003
2025-10-23 16:36:12,631 - INFO - Batch 5900, Loss: 2.3904, LR: 0.000003
2025-10-23 16:36:31,591 - INFO - Batch 6000, Loss: 2.0984, LR: 0.000003
2025-10-23 16:36:50,403 - INFO - Batch 6100, Loss: 2.1793, LR: 0.000003
2025-10-23 16:37:12,048 - INFO - Batch 6200, Loss: 2.3165, LR: 0.000003
2025-10-23 16:37:31,605 - INFO - Batch 6300, Loss: 2.3655, LR: 0.000003
2025-10-23 16:37:52,170 - INFO - Batch 6400, Loss: 2.2237, LR: 0.000003
2025-10-23 16:38:00,811 - INFO - Epoch 27/30: Train Loss: 2.2184, Val Loss: 2.1634, LR: 0.000002
2025-10-23 16:38:01,503 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 16:38:01,713 - INFO - Batch 0, Loss: 1.9803, LR: 0.000002
2025-10-23 16:38:21,288 - INFO - Batch 100, Loss: 2.3707, LR: 0.000002
2025-10-23 16:38:38,805 - INFO - Batch 200, Loss: 2.2778, LR: 0.000002
2025-10-23 16:38:53,723 - INFO - Batch 300, Loss: 2.2991, LR: 0.000002
2025-10-23 16:39:16,963 - INFO - Batch 400, Loss: 2.2113, LR: 0.000002
2025-10-23 16:39:38,356 - INFO - Batch 500, Loss: 2.2772, LR: 0.000002
2025-10-23 16:39:57,543 - INFO - Batch 600, Loss: 2.2075, LR: 0.000002
2025-10-23 16:40:18,359 - INFO - Batch 700, Loss: 2.1530, LR: 0.000002
2025-10-23 16:40:35,310 - INFO - Batch 800, Loss: 2.2731, LR: 0.000002
2025-10-23 16:40:52,371 - INFO - Batch 900, Loss: 2.2524, LR: 0.000002
2025-10-23 16:41:15,858 - INFO - Batch 1000, Loss: 2.3594, LR: 0.000002
2025-10-23 16:41:35,102 - INFO - Batch 1100, Loss: 2.2785, LR: 0.000002
2025-10-23 16:41:56,624 - INFO - Batch 1200, Loss: 2.2618, LR: 0.000002
2025-10-23 16:42:20,047 - INFO - Batch 1300, Loss: 2.4178, LR: 0.000002
2025-10-23 16:42:40,106 - INFO - Batch 1400, Loss: 2.0301, LR: 0.000002
2025-10-23 16:42:57,421 - INFO - Batch 1500, Loss: 2.2214, LR: 0.000002
2025-10-23 16:43:17,815 - INFO - Batch 1600, Loss: 2.0559, LR: 0.000002
2025-10-23 16:43:34,198 - INFO - Batch 1700, Loss: 2.2113, LR: 0.000002
2025-10-23 16:43:54,869 - INFO - Batch 1800, Loss: 2.1912, LR: 0.000002
2025-10-23 16:44:15,006 - INFO - Batch 1900, Loss: 1.9052, LR: 0.000002
2025-10-23 16:44:34,213 - INFO - Batch 2000, Loss: 2.1640, LR: 0.000002
2025-10-23 16:44:53,453 - INFO - Batch 2100, Loss: 2.0441, LR: 0.000002
2025-10-23 16:45:12,470 - INFO - Batch 2200, Loss: 2.3162, LR: 0.000002
2025-10-23 16:45:29,899 - INFO - Batch 2300, Loss: 2.3491, LR: 0.000002
2025-10-23 16:45:48,655 - INFO - Batch 2400, Loss: 2.2822, LR: 0.000002
2025-10-23 16:46:07,362 - INFO - Batch 2500, Loss: 2.2245, LR: 0.000002
2025-10-23 16:46:25,298 - INFO - Batch 2600, Loss: 1.9051, LR: 0.000002
2025-10-23 16:46:43,596 - INFO - Batch 2700, Loss: 2.0678, LR: 0.000002
2025-10-23 16:47:03,049 - INFO - Batch 2800, Loss: 2.2566, LR: 0.000002
2025-10-23 16:47:22,947 - INFO - Batch 2900, Loss: 2.0331, LR: 0.000002
2025-10-23 16:47:40,568 - INFO - Batch 3000, Loss: 2.2552, LR: 0.000002
2025-10-23 16:47:58,353 - INFO - Batch 3100, Loss: 2.0473, LR: 0.000002
2025-10-23 16:48:18,196 - INFO - Batch 3200, Loss: 1.9501, LR: 0.000002
2025-10-23 16:48:37,730 - INFO - Batch 3300, Loss: 2.1857, LR: 0.000002
2025-10-23 16:48:56,681 - INFO - Batch 3400, Loss: 2.1026, LR: 0.000002
2025-10-23 16:49:15,725 - INFO - Batch 3500, Loss: 2.0338, LR: 0.000002
2025-10-23 16:49:36,077 - INFO - Batch 3600, Loss: 2.4184, LR: 0.000002
2025-10-23 16:49:52,177 - INFO - Batch 3700, Loss: 2.1402, LR: 0.000002
2025-10-23 16:50:10,764 - INFO - Batch 3800, Loss: 2.1253, LR: 0.000002
2025-10-23 16:50:32,544 - INFO - Batch 3900, Loss: 2.4470, LR: 0.000002
2025-10-23 16:50:50,523 - INFO - Batch 4000, Loss: 2.2636, LR: 0.000002
2025-10-23 16:51:16,782 - INFO - Batch 4100, Loss: 2.1411, LR: 0.000002
2025-10-23 16:51:37,216 - INFO - Batch 4200, Loss: 1.9901, LR: 0.000002
2025-10-23 16:51:56,823 - INFO - Batch 4300, Loss: 2.2107, LR: 0.000002
2025-10-23 16:52:18,826 - INFO - Batch 4400, Loss: 1.9085, LR: 0.000001
2025-10-23 16:52:40,814 - INFO - Batch 4500, Loss: 1.9753, LR: 0.000001
2025-10-23 16:53:01,762 - INFO - Batch 4600, Loss: 2.0256, LR: 0.000001
2025-10-23 16:53:21,268 - INFO - Batch 4700, Loss: 2.2783, LR: 0.000001
2025-10-23 16:53:40,528 - INFO - Batch 4800, Loss: 2.1347, LR: 0.000001
2025-10-23 16:53:59,676 - INFO - Batch 4900, Loss: 2.2734, LR: 0.000001
2025-10-23 16:54:20,353 - INFO - Batch 5000, Loss: 2.5125, LR: 0.000001
2025-10-23 16:54:39,107 - INFO - Batch 5100, Loss: 2.4639, LR: 0.000001
2025-10-23 16:54:59,189 - INFO - Batch 5200, Loss: 2.5904, LR: 0.000001
2025-10-23 16:55:22,393 - INFO - Batch 5300, Loss: 2.4545, LR: 0.000001
2025-10-23 16:55:40,820 - INFO - Batch 5400, Loss: 2.0789, LR: 0.000001
2025-10-23 16:55:59,831 - INFO - Batch 5500, Loss: 2.0880, LR: 0.000001
2025-10-23 16:56:19,217 - INFO - Batch 5600, Loss: 2.3178, LR: 0.000001
2025-10-23 16:56:38,982 - INFO - Batch 5700, Loss: 2.2186, LR: 0.000001
2025-10-23 16:56:59,376 - INFO - Batch 5800, Loss: 2.0591, LR: 0.000001
2025-10-23 16:57:22,450 - INFO - Batch 5900, Loss: 2.0239, LR: 0.000001
2025-10-23 16:57:40,416 - INFO - Batch 6000, Loss: 2.2072, LR: 0.000001
2025-10-23 16:58:00,412 - INFO - Batch 6100, Loss: 2.2741, LR: 0.000001
2025-10-23 16:58:21,610 - INFO - Batch 6200, Loss: 2.2778, LR: 0.000001
2025-10-23 16:58:43,256 - INFO - Batch 6300, Loss: 2.1374, LR: 0.000001
2025-10-23 16:59:01,014 - INFO - Batch 6400, Loss: 2.4333, LR: 0.000001
2025-10-23 16:59:09,600 - INFO - Epoch 28/30: Train Loss: 2.2130, Val Loss: 2.1642, LR: 0.000001
2025-10-23 16:59:09,802 - INFO - Batch 0, Loss: 2.0148, LR: 0.000001
2025-10-23 16:59:29,084 - INFO - Batch 100, Loss: 2.2497, LR: 0.000001
2025-10-23 16:59:46,799 - INFO - Batch 200, Loss: 2.1299, LR: 0.000001
2025-10-23 17:00:08,459 - INFO - Batch 300, Loss: 1.9890, LR: 0.000001
2025-10-23 17:00:26,883 - INFO - Batch 400, Loss: 2.3071, LR: 0.000001
2025-10-23 17:00:46,143 - INFO - Batch 500, Loss: 2.1444, LR: 0.000001
2025-10-23 17:01:03,987 - INFO - Batch 600, Loss: 2.1081, LR: 0.000001
2025-10-23 17:01:27,257 - INFO - Batch 700, Loss: 2.1875, LR: 0.000001
2025-10-23 17:01:44,187 - INFO - Batch 800, Loss: 2.1089, LR: 0.000001
2025-10-23 17:02:00,124 - INFO - Batch 900, Loss: 2.1042, LR: 0.000001
2025-10-23 17:02:18,495 - INFO - Batch 1000, Loss: 2.3143, LR: 0.000001
2025-10-23 17:02:36,636 - INFO - Batch 1100, Loss: 2.1716, LR: 0.000001
2025-10-23 17:02:55,554 - INFO - Batch 1200, Loss: 2.2114, LR: 0.000001
2025-10-23 17:03:13,083 - INFO - Batch 1300, Loss: 2.3273, LR: 0.000001
2025-10-23 17:03:33,829 - INFO - Batch 1400, Loss: 2.3464, LR: 0.000001
2025-10-23 17:03:53,410 - INFO - Batch 1500, Loss: 2.0855, LR: 0.000001
2025-10-23 17:04:12,185 - INFO - Batch 1600, Loss: 2.0435, LR: 0.000001
2025-10-23 17:04:31,622 - INFO - Batch 1700, Loss: 2.3206, LR: 0.000001
2025-10-23 17:04:49,672 - INFO - Batch 1800, Loss: 2.1123, LR: 0.000001
2025-10-23 17:05:08,343 - INFO - Batch 1900, Loss: 2.4419, LR: 0.000001
2025-10-23 17:05:28,450 - INFO - Batch 2000, Loss: 2.1632, LR: 0.000001
2025-10-23 17:05:50,530 - INFO - Batch 2100, Loss: 2.2019, LR: 0.000001
2025-10-23 17:06:09,886 - INFO - Batch 2200, Loss: 2.1904, LR: 0.000001
2025-10-23 17:06:30,417 - INFO - Batch 2300, Loss: 2.3840, LR: 0.000001
2025-10-23 17:06:49,460 - INFO - Batch 2400, Loss: 1.9092, LR: 0.000001
2025-10-23 17:07:07,211 - INFO - Batch 2500, Loss: 2.2924, LR: 0.000001
2025-10-23 17:07:26,446 - INFO - Batch 2600, Loss: 2.1447, LR: 0.000001
2025-10-23 17:07:43,394 - INFO - Batch 2700, Loss: 2.1244, LR: 0.000001
2025-10-23 17:08:01,671 - INFO - Batch 2800, Loss: 1.9951, LR: 0.000001
2025-10-23 17:08:21,642 - INFO - Batch 2900, Loss: 2.0616, LR: 0.000001
2025-10-23 17:08:40,888 - INFO - Batch 3000, Loss: 2.3638, LR: 0.000001
2025-10-23 17:08:59,591 - INFO - Batch 3100, Loss: 2.1951, LR: 0.000001
2025-10-23 17:09:22,158 - INFO - Batch 3200, Loss: 2.0785, LR: 0.000001
2025-10-23 17:09:40,248 - INFO - Batch 3300, Loss: 2.0811, LR: 0.000001
2025-10-23 17:10:00,561 - INFO - Batch 3400, Loss: 1.9740, LR: 0.000001
2025-10-23 17:10:17,914 - INFO - Batch 3500, Loss: 2.1302, LR: 0.000001
2025-10-23 17:10:38,710 - INFO - Batch 3600, Loss: 2.1153, LR: 0.000001
2025-10-23 17:10:56,884 - INFO - Batch 3700, Loss: 1.9315, LR: 0.000001
2025-10-23 17:11:10,445 - INFO - Batch 3800, Loss: 2.3089, LR: 0.000001
2025-10-23 17:11:30,067 - INFO - Batch 3900, Loss: 2.1525, LR: 0.000001
2025-10-23 17:11:48,866 - INFO - Batch 4000, Loss: 2.1665, LR: 0.000001
2025-10-23 17:12:07,652 - INFO - Batch 4100, Loss: 2.3723, LR: 0.000001
2025-10-23 17:12:26,217 - INFO - Batch 4200, Loss: 2.2687, LR: 0.000001
2025-10-23 17:12:43,834 - INFO - Batch 4300, Loss: 2.1956, LR: 0.000000
2025-10-23 17:13:04,006 - INFO - Batch 4400, Loss: 2.1680, LR: 0.000000
2025-10-23 17:13:22,585 - INFO - Batch 4500, Loss: 2.4202, LR: 0.000000
2025-10-23 17:13:44,032 - INFO - Batch 4600, Loss: 2.1420, LR: 0.000000
2025-10-23 17:14:04,318 - INFO - Batch 4700, Loss: 2.1155, LR: 0.000000
2025-10-23 17:14:21,769 - INFO - Batch 4800, Loss: 2.2452, LR: 0.000000
2025-10-23 17:14:40,718 - INFO - Batch 4900, Loss: 2.1071, LR: 0.000000
2025-10-23 17:14:59,467 - INFO - Batch 5000, Loss: 2.2303, LR: 0.000000
2025-10-23 17:15:19,412 - INFO - Batch 5100, Loss: 2.3497, LR: 0.000000
2025-10-23 17:15:41,040 - INFO - Batch 5200, Loss: 2.0294, LR: 0.000000
2025-10-23 17:15:55,565 - INFO - Batch 5300, Loss: 2.1357, LR: 0.000000
2025-10-23 17:16:15,200 - INFO - Batch 5400, Loss: 2.3191, LR: 0.000000
2025-10-23 17:16:32,688 - INFO - Batch 5500, Loss: 2.3258, LR: 0.000000
2025-10-23 17:16:58,672 - INFO - Batch 5600, Loss: 1.9850, LR: 0.000000
2025-10-23 17:17:17,260 - INFO - Batch 5700, Loss: 2.1660, LR: 0.000000
2025-10-23 17:17:39,149 - INFO - Batch 5800, Loss: 2.3013, LR: 0.000000
2025-10-23 17:17:57,604 - INFO - Batch 5900, Loss: 2.2354, LR: 0.000000
2025-10-23 17:18:15,005 - INFO - Batch 6000, Loss: 2.0556, LR: 0.000000
2025-10-23 17:18:35,903 - INFO - Batch 6100, Loss: 2.2025, LR: 0.000000
2025-10-23 17:18:56,110 - INFO - Batch 6200, Loss: 2.1564, LR: 0.000000
2025-10-23 17:19:11,314 - INFO - Batch 6300, Loss: 2.0969, LR: 0.000000
2025-10-23 17:19:30,695 - INFO - Batch 6400, Loss: 2.3099, LR: 0.000000
2025-10-23 17:19:40,234 - INFO - Epoch 29/30: Train Loss: 2.2089, Val Loss: 2.1641, LR: 0.000000
2025-10-23 17:19:40,426 - INFO - Batch 0, Loss: 2.1781, LR: 0.000000
2025-10-23 17:19:58,137 - INFO - Batch 100, Loss: 2.0110, LR: 0.000000
2025-10-23 17:20:15,234 - INFO - Batch 200, Loss: 2.4531, LR: 0.000000
2025-10-23 17:20:34,981 - INFO - Batch 300, Loss: 2.3045, LR: 0.000000
2025-10-23 17:20:54,973 - INFO - Batch 400, Loss: 2.4556, LR: 0.000000
2025-10-23 17:21:19,439 - INFO - Batch 500, Loss: 2.1685, LR: 0.000000
2025-10-23 17:21:40,678 - INFO - Batch 600, Loss: 2.4077, LR: 0.000000
2025-10-23 17:21:58,222 - INFO - Batch 700, Loss: 2.0389, LR: 0.000000
2025-10-23 17:22:17,875 - INFO - Batch 800, Loss: 2.1970, LR: 0.000000
2025-10-23 17:22:35,151 - INFO - Batch 900, Loss: 2.2970, LR: 0.000000
2025-10-23 17:22:54,902 - INFO - Batch 1000, Loss: 2.1695, LR: 0.000000
2025-10-23 17:23:13,406 - INFO - Batch 1100, Loss: 2.1556, LR: 0.000000
2025-10-23 17:23:33,711 - INFO - Batch 1200, Loss: 2.1666, LR: 0.000000
2025-10-23 17:23:53,306 - INFO - Batch 1300, Loss: 2.3926, LR: 0.000000
2025-10-23 17:24:14,831 - INFO - Batch 1400, Loss: 2.1111, LR: 0.000000
2025-10-23 17:24:33,362 - INFO - Batch 1500, Loss: 2.1606, LR: 0.000000
2025-10-23 17:24:52,651 - INFO - Batch 1600, Loss: 2.1258, LR: 0.000000
2025-10-23 17:25:13,175 - INFO - Batch 1700, Loss: 2.1675, LR: 0.000000
2025-10-23 17:25:30,381 - INFO - Batch 1800, Loss: 2.1459, LR: 0.000000
2025-10-23 17:25:47,895 - INFO - Batch 1900, Loss: 2.1137, LR: 0.000000
2025-10-23 17:26:07,961 - INFO - Batch 2000, Loss: 2.2537, LR: 0.000000
2025-10-23 17:26:27,492 - INFO - Batch 2100, Loss: 2.2980, LR: 0.000000
2025-10-23 17:26:44,601 - INFO - Batch 2200, Loss: 1.9838, LR: 0.000000
2025-10-23 17:27:02,841 - INFO - Batch 2300, Loss: 1.9797, LR: 0.000000
2025-10-23 17:27:25,452 - INFO - Batch 2400, Loss: 2.0880, LR: 0.000000
2025-10-23 17:27:44,404 - INFO - Batch 2500, Loss: 2.2499, LR: 0.000000
2025-10-23 17:28:06,474 - INFO - Batch 2600, Loss: 1.9069, LR: 0.000000
2025-10-23 17:28:25,738 - INFO - Batch 2700, Loss: 2.0378, LR: 0.000000
2025-10-23 17:28:44,474 - INFO - Batch 2800, Loss: 2.3486, LR: 0.000000
2025-10-23 17:29:02,822 - INFO - Batch 2900, Loss: 2.2579, LR: 0.000000
2025-10-23 17:29:23,062 - INFO - Batch 3000, Loss: 2.4520, LR: 0.000000
2025-10-23 17:29:47,899 - INFO - Batch 3100, Loss: 1.9484, LR: 0.000000
2025-10-23 17:30:08,164 - INFO - Batch 3200, Loss: 2.2124, LR: 0.000000
2025-10-23 17:30:26,518 - INFO - Batch 3300, Loss: 2.0186, LR: 0.000000
2025-10-23 17:30:42,582 - INFO - Batch 3400, Loss: 2.1839, LR: 0.000000
2025-10-23 17:30:55,199 - INFO - Batch 3500, Loss: 2.1755, LR: 0.000000
2025-10-23 17:31:08,876 - INFO - Batch 3600, Loss: 2.4194, LR: 0.000000
2025-10-23 17:31:26,703 - INFO - Batch 3700, Loss: 2.0952, LR: 0.000000
2025-10-23 17:31:49,122 - INFO - Batch 3800, Loss: 2.2523, LR: 0.000000
2025-10-23 17:32:11,671 - INFO - Batch 3900, Loss: 2.2270, LR: 0.000000
2025-10-23 17:32:36,939 - INFO - Batch 4000, Loss: 2.0895, LR: 0.000000
2025-10-23 17:32:54,581 - INFO - Batch 4100, Loss: 2.1481, LR: 0.000000
2025-10-23 17:33:14,757 - INFO - Batch 4200, Loss: 2.0394, LR: 0.000000
2025-10-23 17:33:33,565 - INFO - Batch 4300, Loss: 2.2901, LR: 0.000000
2025-10-23 17:33:50,595 - INFO - Batch 4400, Loss: 2.5216, LR: 0.000000
2025-10-23 17:34:08,709 - INFO - Batch 4500, Loss: 2.2764, LR: 0.000000
2025-10-23 17:34:24,810 - INFO - Batch 4600, Loss: 2.3334, LR: 0.000000
2025-10-23 17:34:44,124 - INFO - Batch 4700, Loss: 2.0840, LR: 0.000000
2025-10-23 17:35:00,879 - INFO - Batch 4800, Loss: 2.1169, LR: 0.000000
2025-10-23 17:35:21,027 - INFO - Batch 4900, Loss: 2.1717, LR: 0.000000
2025-10-23 17:35:38,630 - INFO - Batch 5000, Loss: 2.2709, LR: 0.000000
2025-10-23 17:35:55,037 - INFO - Batch 5100, Loss: 2.2205, LR: 0.000000
2025-10-23 17:36:14,004 - INFO - Batch 5200, Loss: 2.2265, LR: 0.000000
2025-10-23 17:36:32,450 - INFO - Batch 5300, Loss: 2.0624, LR: 0.000000
2025-10-23 17:36:53,603 - INFO - Batch 5400, Loss: 2.2639, LR: 0.000000
2025-10-23 17:37:13,052 - INFO - Batch 5500, Loss: 2.1202, LR: 0.000000
2025-10-23 17:37:33,363 - INFO - Batch 5600, Loss: 2.0804, LR: 0.000000
2025-10-23 17:37:57,095 - INFO - Batch 5700, Loss: 2.2146, LR: 0.000000
2025-10-23 17:38:18,197 - INFO - Batch 5800, Loss: 2.2250, LR: 0.000000
2025-10-23 17:38:37,991 - INFO - Batch 5900, Loss: 2.2070, LR: 0.000000
2025-10-23 17:38:58,830 - INFO - Batch 6000, Loss: 2.3069, LR: 0.000000
2025-10-23 17:39:17,368 - INFO - Batch 6100, Loss: 2.1270, LR: 0.000000
2025-10-23 17:39:36,056 - INFO - Batch 6200, Loss: 2.2850, LR: 0.000000
2025-10-23 17:39:52,057 - INFO - Batch 6300, Loss: 2.4622, LR: 0.000000
2025-10-23 17:40:12,248 - INFO - Batch 6400, Loss: 2.0483, LR: 0.000000
2025-10-23 17:40:21,246 - INFO - Epoch 30/30: Train Loss: 2.2070, Val Loss: 2.1634, LR: 0.000000
2025-10-23 17:40:21,556 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 17:40:22,285 - INFO - 模型已保存到: ./checkpoints/checkpoint_epoch_30.pth
2025-10-23 17:40:22,526 - INFO - 模型已保存到: ./checkpoints/final_model.pth
2025-10-23 17:40:23,736 - INFO - 训练完成!
