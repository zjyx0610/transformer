2025-10-21 14:37:02,400 - INFO - 使用设备: cpu
2025-10-21 14:37:02,400 - INFO - 加载数据...
2025-10-21 14:37:02,400 - INFO - 加载训练数据...
2025-10-21 14:37:06,884 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-10-21 14:37:06,885 - INFO - 训练数据结构:
2025-10-21 14:37:06,885 - INFO - 数据结构分析:
2025-10-21 14:37:06,885 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:37:06,885 - INFO - 
英语相关语言对:
2025-10-21 14:37:06,885 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:37:06,885 - INFO -   en->de: 206112 个样本
2025-10-21 14:37:06,885 - INFO - 
德语相关语言对:
2025-10-21 14:37:06,885 - INFO - 德语->其他语言: ['en']
2025-10-21 14:37:06,885 - INFO -   de->en: 206112 个样本
2025-10-21 14:37:06,885 - INFO - 提取 en->de 的翻译对
2025-10-21 14:37:06,886 - INFO - 找到 en->de: 206112 个样本
2025-10-21 14:37:06,942 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 14:37:06,942 - INFO - 加载验证数据...
2025-10-21 14:37:06,990 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-10-21 14:37:06,990 - INFO - 提取 en->de 的翻译对
2025-10-21 14:37:06,990 - INFO - 找到 en->de: 888 个样本
2025-10-21 14:37:06,991 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 14:37:06,991 - INFO - 加载测试数据...
2025-10-21 14:37:07,095 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-10-21 14:37:07,095 - INFO - 提取 en->de 的翻译对
2025-10-21 14:37:07,095 - INFO - 找到 en->de: 8079 个样本
2025-10-21 14:37:07,098 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 14:37:07,099 - INFO - 开始构建分词器...
2025-10-21 14:37:07,158 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 14:37:08,553 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 14:37:11,311 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 14:37:11,356 - INFO - 数据加载器创建完成
2025-10-21 14:37:11,356 - INFO - 训练集: 206112 个样本
2025-10-21 14:37:11,356 - INFO - 验证集: 888 个样本
2025-10-21 14:37:11,356 - INFO - 测试集: 8079 个样本
2025-10-21 14:37:12,355 - INFO - 源语言词汇表大小: 5000
2025-10-21 14:37:12,356 - INFO - 目标语言词汇表大小: 5000
2025-10-21 14:37:12,356 - INFO - 创建模型...
2025-10-21 14:37:12,490 - INFO - 总参数数: 15,041,416
2025-10-21 14:37:12,490 - INFO - 可训练参数数: 15,041,416
2025-10-21 14:37:12,492 - INFO - 开始训练...
2025-10-21 14:37:29,876 - INFO - Batch 0, Loss: 8.7017, LR: 0.000000
2025-10-21 14:40:03,862 - INFO - Batch 10, Loss: 8.7065, LR: 0.000000
2025-10-21 14:42:34,415 - INFO - Batch 20, Loss: 8.6847, LR: 0.000000
2025-10-21 14:45:04,825 - INFO - Batch 30, Loss: 8.6975, LR: 0.000000
2025-10-21 14:46:32,651 - INFO - 使用设备: cpu
2025-10-21 14:46:32,651 - INFO - 加载数据...
2025-10-21 14:46:32,651 - INFO - 加载训练数据...
2025-10-21 14:46:54,988 - INFO - 使用设备: cpu
2025-10-21 14:46:54,988 - INFO - 加载数据...
2025-10-21 14:46:54,988 - INFO - 加载训练数据...
2025-10-21 14:46:59,420 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-10-21 14:46:59,420 - INFO - 训练数据结构:
2025-10-21 14:46:59,420 - INFO - 数据结构分析:
2025-10-21 14:46:59,420 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:46:59,420 - INFO - 
英语相关语言对:
2025-10-21 14:46:59,420 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:46:59,421 - INFO -   en->de: 206112 个样本
2025-10-21 14:46:59,421 - INFO - 
德语相关语言对:
2025-10-21 14:46:59,421 - INFO - 德语->其他语言: ['en']
2025-10-21 14:46:59,421 - INFO -   de->en: 206112 个样本
2025-10-21 14:46:59,421 - INFO - 提取 en->de 的翻译对
2025-10-21 14:46:59,421 - INFO - 找到 en->de: 206112 个样本
2025-10-21 14:46:59,476 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 14:46:59,476 - INFO - 加载验证数据...
2025-10-21 14:46:59,521 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-10-21 14:46:59,522 - INFO - 提取 en->de 的翻译对
2025-10-21 14:46:59,522 - INFO - 找到 en->de: 888 个样本
2025-10-21 14:46:59,522 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 14:46:59,522 - INFO - 加载测试数据...
2025-10-21 14:46:59,624 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-10-21 14:46:59,624 - INFO - 提取 en->de 的翻译对
2025-10-21 14:46:59,624 - INFO - 找到 en->de: 8079 个样本
2025-10-21 14:46:59,627 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 14:46:59,627 - INFO - 开始构建分词器...
2025-10-21 14:46:59,683 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 14:47:00,959 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 14:47:03,041 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 14:47:03,079 - INFO - 数据加载器创建完成
2025-10-21 14:47:03,080 - INFO - 训练集: 206112 个样本
2025-10-21 14:47:03,080 - INFO - 验证集: 888 个样本
2025-10-21 14:47:03,080 - INFO - 测试集: 8079 个样本
2025-10-21 14:47:04,061 - INFO - 源语言词汇表大小: 5000
2025-10-21 14:47:04,061 - INFO - 目标语言词汇表大小: 5000
2025-10-21 14:47:04,061 - INFO - 创建模型...
2025-10-21 14:47:04,201 - INFO - 总参数数: 15,041,416
2025-10-21 14:47:04,201 - INFO - 可训练参数数: 15,041,416
2025-10-21 14:47:04,202 - INFO - 开始训练...
2025-10-21 14:47:19,969 - INFO - Batch 0, Loss: 8.7017, LR: 0.000000
2025-10-21 14:49:52,055 - INFO - Batch 10, Loss: 8.7059, LR: 0.000000
2025-10-21 14:52:26,063 - INFO - 使用设备: cpu
2025-10-21 14:52:26,063 - INFO - 加载数据...
2025-10-21 14:52:26,063 - INFO - 加载训练数据...
2025-10-21 14:52:39,786 - INFO - 使用设备: cpu
2025-10-21 14:52:39,786 - INFO - 加载数据...
2025-10-21 14:52:39,786 - INFO - 加载训练数据...
2025-10-21 14:52:44,228 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-10-21 14:52:44,228 - INFO - 训练数据结构:
2025-10-21 14:52:44,228 - INFO - 数据结构分析:
2025-10-21 14:52:44,228 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:52:44,228 - INFO - 
英语相关语言对:
2025-10-21 14:52:44,228 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:52:44,229 - INFO -   en->de: 206112 个样本
2025-10-21 14:52:44,229 - INFO - 
德语相关语言对:
2025-10-21 14:52:44,229 - INFO - 德语->其他语言: ['en']
2025-10-21 14:52:44,229 - INFO -   de->en: 206112 个样本
2025-10-21 14:52:44,229 - INFO - 提取 en->de 的翻译对
2025-10-21 14:52:44,229 - INFO - 找到 en->de: 206112 个样本
2025-10-21 14:52:44,285 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 14:52:44,286 - INFO - 加载验证数据...
2025-10-21 14:52:44,331 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-10-21 14:52:44,331 - INFO - 提取 en->de 的翻译对
2025-10-21 14:52:44,331 - INFO - 找到 en->de: 888 个样本
2025-10-21 14:52:44,331 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 14:52:44,331 - INFO - 加载测试数据...
2025-10-21 14:52:44,435 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-10-21 14:52:44,435 - INFO - 提取 en->de 的翻译对
2025-10-21 14:52:44,435 - INFO - 找到 en->de: 8079 个样本
2025-10-21 14:52:44,438 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 14:52:44,438 - INFO - 开始构建分词器...
2025-10-21 14:52:44,497 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 14:52:56,941 - INFO - 使用设备: cpu
2025-10-21 14:52:56,942 - INFO - 加载数据...
2025-10-21 14:52:56,942 - INFO - 加载训练数据...
2025-10-21 14:53:01,437 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-10-21 14:53:01,437 - INFO - 训练数据结构:
2025-10-21 14:53:01,437 - INFO - 数据结构分析:
2025-10-21 14:53:01,437 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:53:01,437 - INFO - 
英语相关语言对:
2025-10-21 14:53:01,437 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:53:01,437 - INFO -   en->de: 206112 个样本
2025-10-21 14:53:01,437 - INFO - 
德语相关语言对:
2025-10-21 14:53:01,438 - INFO - 德语->其他语言: ['en']
2025-10-21 14:53:01,438 - INFO -   de->en: 206112 个样本
2025-10-21 14:53:01,438 - INFO - 提取 en->de 的翻译对
2025-10-21 14:53:01,438 - INFO - 找到 en->de: 206112 个样本
2025-10-21 14:53:01,492 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 14:53:01,493 - INFO - 加载验证数据...
2025-10-21 14:53:01,538 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-10-21 14:53:01,538 - INFO - 提取 en->de 的翻译对
2025-10-21 14:53:01,538 - INFO - 找到 en->de: 888 个样本
2025-10-21 14:53:01,538 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 14:53:01,538 - INFO - 加载测试数据...
2025-10-21 14:53:01,641 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-10-21 14:53:01,641 - INFO - 提取 en->de 的翻译对
2025-10-21 14:53:01,641 - INFO - 找到 en->de: 8079 个样本
2025-10-21 14:53:01,644 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 14:53:01,644 - INFO - 开始构建分词器...
2025-10-21 14:53:01,689 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 14:53:02,897 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 14:53:05,005 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 14:53:05,047 - INFO - 数据加载器创建完成
2025-10-21 14:53:05,047 - INFO - 训练集: 206112 个样本
2025-10-21 14:53:05,047 - INFO - 验证集: 888 个样本
2025-10-21 14:53:05,047 - INFO - 测试集: 8079 个样本
2025-10-21 14:53:06,047 - INFO - 源语言词汇表大小: 5000
2025-10-21 14:53:06,047 - INFO - 目标语言词汇表大小: 5000
2025-10-21 14:53:06,047 - INFO - 创建模型...
2025-10-21 14:53:06,181 - INFO - 总参数数: 15,041,416
2025-10-21 14:53:06,183 - INFO - 可训练参数数: 15,041,416
2025-10-21 14:53:06,184 - INFO - 开始训练...
2025-10-21 14:53:19,798 - INFO - Batch 0, Loss: 8.6371, LR: 0.000000
2025-10-21 14:55:51,825 - INFO - Batch 10, Loss: 8.6392, LR: 0.000000
2025-10-21 14:57:23,788 - INFO - 使用设备: cpu
2025-10-21 14:57:23,788 - INFO - 加载数据...
2025-10-21 14:57:23,788 - INFO - 加载训练数据...
2025-10-21 14:57:28,211 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-10-21 14:57:28,211 - INFO - 训练数据结构:
2025-10-21 14:57:28,212 - INFO - 数据结构分析:
2025-10-21 14:57:28,212 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:57:28,212 - INFO - 
英语相关语言对:
2025-10-21 14:57:28,212 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 14:57:28,212 - INFO -   en->de: 206112 个样本
2025-10-21 14:57:28,212 - INFO - 
德语相关语言对:
2025-10-21 14:57:28,212 - INFO - 德语->其他语言: ['en']
2025-10-21 14:57:28,212 - INFO -   de->en: 206112 个样本
2025-10-21 14:57:28,212 - INFO - 提取 en->de 的翻译对
2025-10-21 14:57:28,212 - INFO - 找到 en->de: 206112 个样本
2025-10-21 14:57:28,269 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 14:57:28,269 - INFO - 加载验证数据...
2025-10-21 14:57:28,314 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-10-21 14:57:28,314 - INFO - 提取 en->de 的翻译对
2025-10-21 14:57:28,314 - INFO - 找到 en->de: 888 个样本
2025-10-21 14:57:28,315 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 14:57:28,315 - INFO - 加载测试数据...
2025-10-21 14:57:28,417 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-10-21 14:57:28,417 - INFO - 提取 en->de 的翻译对
2025-10-21 14:57:28,417 - INFO - 找到 en->de: 8079 个样本
2025-10-21 14:57:28,420 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 14:57:28,420 - INFO - 开始构建分词器...
2025-10-21 14:57:28,475 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 14:57:29,707 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 14:57:31,740 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 14:57:31,778 - INFO - 数据加载器创建完成
2025-10-21 14:57:31,778 - INFO - 训练集: 206112 个样本
2025-10-21 14:57:31,778 - INFO - 验证集: 888 个样本
2025-10-21 14:57:31,778 - INFO - 测试集: 8079 个样本
2025-10-21 14:57:32,759 - INFO - 源语言词汇表大小: 5000
2025-10-21 14:57:32,760 - INFO - 目标语言词汇表大小: 5000
2025-10-21 14:57:32,760 - INFO - 创建模型...
2025-10-21 14:57:32,898 - INFO - 总参数数: 15,041,416
2025-10-21 14:57:32,899 - INFO - 可训练参数数: 15,041,416
2025-10-21 14:57:32,900 - INFO - 开始训练...
2025-10-21 14:57:49,176 - INFO - Batch 0, Loss: 8.5819, LR: 0.000000
2025-10-21 15:12:03,336 - INFO - 使用设备: cpu
2025-10-21 15:12:03,336 - INFO - 加载数据...
2025-10-21 15:12:03,336 - INFO - 加载训练数据...
2025-10-21 15:12:07,808 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-10-21 15:12:07,809 - INFO - 训练数据结构:
2025-10-21 15:12:07,809 - INFO - 数据结构分析:
2025-10-21 15:12:07,809 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 15:12:07,809 - INFO - 
英语相关语言对:
2025-10-21 15:12:07,809 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 15:12:07,809 - INFO -   en->de: 206112 个样本
2025-10-21 15:12:07,809 - INFO - 
德语相关语言对:
2025-10-21 15:12:07,809 - INFO - 德语->其他语言: ['en']
2025-10-21 15:12:07,809 - INFO -   de->en: 206112 个样本
2025-10-21 15:12:07,809 - INFO - 提取 en->de 的翻译对
2025-10-21 15:12:07,809 - INFO - 找到 en->de: 206112 个样本
2025-10-21 15:12:07,865 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 15:12:07,865 - INFO - 加载验证数据...
2025-10-21 15:12:07,909 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-10-21 15:12:07,910 - INFO - 提取 en->de 的翻译对
2025-10-21 15:12:07,910 - INFO - 找到 en->de: 888 个样本
2025-10-21 15:12:07,910 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 15:12:07,910 - INFO - 加载测试数据...
2025-10-21 15:12:08,011 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-10-21 15:12:08,011 - INFO - 提取 en->de 的翻译对
2025-10-21 15:12:08,011 - INFO - 找到 en->de: 8079 个样本
2025-10-21 15:12:08,014 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 15:12:08,015 - INFO - 开始构建分词器...
2025-10-21 15:12:08,067 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 15:12:09,330 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 15:12:11,376 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 15:12:11,415 - INFO - 数据加载器创建完成
2025-10-21 15:12:11,415 - INFO - 训练集: 206112 个样本
2025-10-21 15:12:11,416 - INFO - 验证集: 888 个样本
2025-10-21 15:12:11,416 - INFO - 测试集: 8079 个样本
2025-10-21 15:12:12,398 - INFO - 源语言词汇表大小: 5000
2025-10-21 15:12:12,398 - INFO - 目标语言词汇表大小: 5000
2025-10-21 15:12:12,398 - INFO - 创建模型...
2025-10-21 15:12:12,555 - INFO - 总参数数: 15,041,416
2025-10-21 15:12:12,555 - INFO - 可训练参数数: 15,041,416
2025-10-21 15:12:12,556 - INFO - 开始训练...
2025-10-21 15:12:26,925 - INFO - Batch 0, Loss: 8.5819, LR: 0.000000
2025-10-21 15:12:51,169 - INFO - 使用设备: cpu
2025-10-21 15:12:51,169 - INFO - 加载数据...
2025-10-21 15:12:51,169 - INFO - 加载训练数据...
2025-10-21 15:12:55,644 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-10-21 15:12:55,644 - INFO - 训练数据结构:
2025-10-21 15:12:55,645 - INFO - 数据结构分析:
2025-10-21 15:12:55,645 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 15:12:55,645 - INFO - 
英语相关语言对:
2025-10-21 15:12:55,645 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 15:12:55,645 - INFO -   en->de: 206112 个样本
2025-10-21 15:12:55,645 - INFO - 
德语相关语言对:
2025-10-21 15:12:55,645 - INFO - 德语->其他语言: ['en']
2025-10-21 15:12:55,645 - INFO -   de->en: 206112 个样本
2025-10-21 15:12:55,645 - INFO - 提取 en->de 的翻译对
2025-10-21 15:12:55,645 - INFO - 找到 en->de: 206112 个样本
2025-10-21 15:12:55,700 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 15:12:55,700 - INFO - 加载验证数据...
2025-10-21 15:12:55,747 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-10-21 15:12:55,747 - INFO - 提取 en->de 的翻译对
2025-10-21 15:12:55,747 - INFO - 找到 en->de: 888 个样本
2025-10-21 15:12:55,748 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 15:12:55,748 - INFO - 加载测试数据...
2025-10-21 15:12:55,849 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-10-21 15:12:55,850 - INFO - 提取 en->de 的翻译对
2025-10-21 15:12:55,850 - INFO - 找到 en->de: 8079 个样本
2025-10-21 15:12:55,853 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 15:12:55,853 - INFO - 开始构建分词器...
2025-10-21 15:12:55,909 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 15:12:57,230 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 15:12:59,386 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 15:12:59,426 - INFO - 数据加载器创建完成
2025-10-21 15:12:59,426 - INFO - 训练集: 206112 个样本
2025-10-21 15:12:59,426 - INFO - 验证集: 888 个样本
2025-10-21 15:12:59,426 - INFO - 测试集: 8079 个样本
2025-10-21 15:13:00,429 - INFO - 源语言词汇表大小: 5000
2025-10-21 15:13:00,429 - INFO - 目标语言词汇表大小: 5000
2025-10-21 15:13:00,429 - INFO - 创建模型...
2025-10-21 15:13:00,565 - INFO - 总参数数: 15,041,416
2025-10-21 15:13:00,565 - INFO - 可训练参数数: 15,041,416
2025-10-21 15:13:00,566 - INFO - 开始训练...
2025-10-21 15:13:14,024 - INFO - Batch 0, Loss: 8.6371, LR: 0.000000
2025-10-21 15:15:49,109 - INFO - Batch 10, Loss: 8.6391, LR: 0.000000
2025-10-21 15:18:18,935 - INFO - Batch 20, Loss: 8.6970, LR: 0.000000
2025-10-21 15:20:53,190 - INFO - Batch 30, Loss: 8.6510, LR: 0.000000
2025-10-21 15:21:29,070 - INFO - 使用设备: cpu
2025-10-21 15:21:29,070 - INFO - 加载数据...
2025-10-21 15:21:29,070 - INFO - 加载训练数据...
2025-10-21 15:21:33,607 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-10-21 15:21:33,607 - INFO - 训练数据结构:
2025-10-21 15:21:33,607 - INFO - 数据结构分析:
2025-10-21 15:21:33,607 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 15:21:33,607 - INFO - 
英语相关语言对:
2025-10-21 15:21:33,607 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 15:21:33,607 - INFO -   en->de: 206112 个样本
2025-10-21 15:21:33,607 - INFO - 
德语相关语言对:
2025-10-21 15:21:33,607 - INFO - 德语->其他语言: ['en']
2025-10-21 15:21:33,608 - INFO -   de->en: 206112 个样本
2025-10-21 15:21:33,608 - INFO - 提取 en->de 的翻译对
2025-10-21 15:21:33,608 - INFO - 找到 en->de: 206112 个样本
2025-10-21 15:21:33,662 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 15:21:33,662 - INFO - 加载验证数据...
2025-10-21 15:21:33,707 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-10-21 15:21:33,707 - INFO - 提取 en->de 的翻译对
2025-10-21 15:21:33,707 - INFO - 找到 en->de: 888 个样本
2025-10-21 15:21:33,707 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 15:21:33,708 - INFO - 加载测试数据...
2025-10-21 15:21:33,809 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-10-21 15:21:33,809 - INFO - 提取 en->de 的翻译对
2025-10-21 15:21:33,809 - INFO - 找到 en->de: 8079 个样本
2025-10-21 15:21:33,811 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 15:21:33,811 - INFO - 开始构建分词器...
2025-10-21 15:21:33,854 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 15:21:35,125 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 15:21:37,142 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 15:21:37,184 - INFO - 数据加载器创建完成
2025-10-21 15:21:37,185 - INFO - 训练集: 206112 个样本
2025-10-21 15:21:37,185 - INFO - 验证集: 888 个样本
2025-10-21 15:21:37,185 - INFO - 测试集: 8079 个样本
2025-10-21 15:21:38,152 - INFO - 源语言词汇表大小: 5000
2025-10-21 15:21:38,152 - INFO - 目标语言词汇表大小: 5000
2025-10-21 15:21:38,152 - INFO - 创建模型...
2025-10-21 15:21:38,293 - INFO - 总参数数: 15,041,416
2025-10-21 15:21:38,293 - INFO - 可训练参数数: 15,041,416
2025-10-21 15:21:38,294 - INFO - 开始训练...
2025-10-21 15:21:52,438 - INFO - Batch 0, Loss: 8.6371, LR: 0.000000
2025-10-21 15:24:20,417 - INFO - Batch 10, Loss: 8.6391, LR: 0.000000
2025-10-21 15:26:52,175 - INFO - Batch 20, Loss: 8.6970, LR: 0.000000
2025-10-21 15:29:21,277 - INFO - Batch 30, Loss: 8.6510, LR: 0.000000
2025-10-21 15:31:52,619 - INFO - Batch 40, Loss: 8.6571, LR: 0.000000
2025-10-21 15:34:22,456 - INFO - Batch 50, Loss: 8.6260, LR: 0.000000
2025-10-21 15:36:55,862 - INFO - Batch 60, Loss: 8.6569, LR: 0.000000
2025-10-21 15:57:02,494 - INFO - 使用设备: cuda:2
2025-10-21 15:57:02,494 - INFO - 加载数据...
2025-10-21 15:57:02,494 - ERROR - 训练数据文件不存在: data/IWSLT2017/iwslt2017_train.pkl
2025-10-21 16:00:33,788 - INFO - 使用设备: cuda:2
2025-10-21 16:00:33,789 - INFO - 加载数据...
2025-10-21 16:00:33,789 - INFO - 加载训练数据...
2025-10-21 16:00:37,637 - INFO - 成功加载 data/IWSLT2017/iwslt2017_train.pkl
2025-10-21 16:00:37,637 - INFO - 训练数据结构:
2025-10-21 16:00:37,637 - INFO - 数据结构分析:
2025-10-21 16:00:37,637 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 16:00:37,637 - INFO - 
英语相关语言对:
2025-10-21 16:00:37,637 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 16:00:37,637 - INFO -   en->de: 206112 个样本
2025-10-21 16:00:37,637 - INFO - 
德语相关语言对:
2025-10-21 16:00:37,637 - INFO - 德语->其他语言: ['en']
2025-10-21 16:00:37,637 - INFO -   de->en: 206112 个样本
2025-10-21 16:00:37,637 - INFO - 提取 en->de 的翻译对
2025-10-21 16:00:37,637 - INFO - 找到 en->de: 206112 个样本
2025-10-21 16:00:37,672 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 16:00:37,672 - INFO - 加载验证数据...
2025-10-21 16:00:37,703 - INFO - 成功加载 data/IWSLT2017/iwslt2017_validation.pkl
2025-10-21 16:00:37,703 - INFO - 提取 en->de 的翻译对
2025-10-21 16:00:37,703 - INFO - 找到 en->de: 888 个样本
2025-10-21 16:00:37,703 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 16:00:37,704 - INFO - 加载测试数据...
2025-10-21 16:00:37,788 - INFO - 成功加载 data/IWSLT2017/iwslt2017_test.pkl
2025-10-21 16:00:37,788 - INFO - 提取 en->de 的翻译对
2025-10-21 16:00:37,788 - INFO - 找到 en->de: 8079 个样本
2025-10-21 16:00:37,789 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 16:00:37,790 - INFO - 开始构建分词器...
2025-10-21 16:00:37,821 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 16:00:39,361 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 16:00:42,367 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 16:00:42,428 - INFO - 数据加载器创建完成
2025-10-21 16:00:42,428 - INFO - 训练集: 206112 个样本
2025-10-21 16:00:42,428 - INFO - 验证集: 888 个样本
2025-10-21 16:00:42,428 - INFO - 测试集: 8079 个样本
2025-10-21 16:00:43,619 - INFO - 源语言词汇表大小: 5000
2025-10-21 16:00:43,619 - INFO - 目标语言词汇表大小: 5000
2025-10-21 16:00:43,619 - INFO - 创建模型...
2025-10-21 16:00:50,311 - INFO - 总参数数: 15,041,416
2025-10-21 16:00:50,311 - INFO - 可训练参数数: 15,041,416
2025-10-21 16:00:50,311 - INFO - 开始训练...
2025-10-21 16:00:54,527 - INFO - Batch 0, Loss: 8.5653, LR: 0.000000
2025-10-21 16:00:55,942 - INFO - Batch 10, Loss: 8.5961, LR: 0.000000
2025-10-21 16:00:57,338 - INFO - Batch 20, Loss: 8.5806, LR: 0.000000
2025-10-21 16:00:58,737 - INFO - Batch 30, Loss: 8.5876, LR: 0.000000
2025-10-21 16:01:00,138 - INFO - Batch 40, Loss: 8.5597, LR: 0.000000
2025-10-21 16:01:01,535 - INFO - Batch 50, Loss: 8.5972, LR: 0.000000
2025-10-21 16:01:02,937 - INFO - Batch 60, Loss: 8.5880, LR: 0.000000
2025-10-21 16:01:04,342 - INFO - Batch 70, Loss: 8.5887, LR: 0.000000
2025-10-21 16:01:05,739 - INFO - Batch 80, Loss: 8.5496, LR: 0.000000
2025-10-21 16:01:07,148 - INFO - Batch 90, Loss: 8.5734, LR: 0.000000
2025-10-21 16:01:08,557 - INFO - Batch 100, Loss: 8.5599, LR: 0.000000
2025-10-21 16:01:09,966 - INFO - Batch 110, Loss: 8.5412, LR: 0.000000
2025-10-21 16:01:11,387 - INFO - Batch 120, Loss: 8.5475, LR: 0.000000
2025-10-21 16:01:12,816 - INFO - Batch 130, Loss: 8.5697, LR: 0.000000
2025-10-21 16:01:14,234 - INFO - Batch 140, Loss: 8.5613, LR: 0.000000
2025-10-21 16:01:15,650 - INFO - Batch 150, Loss: 8.5590, LR: 0.000000
2025-10-21 16:01:17,063 - INFO - Batch 160, Loss: 8.5402, LR: 0.000000
2025-10-21 16:01:18,483 - INFO - Batch 170, Loss: 8.5522, LR: 0.000000
2025-10-21 16:01:19,910 - INFO - Batch 180, Loss: 8.5425, LR: 0.000000
2025-10-21 16:01:21,331 - INFO - Batch 190, Loss: 8.5609, LR: 0.000000
2025-10-21 16:01:22,745 - INFO - Batch 200, Loss: 8.5410, LR: 0.000000
2025-10-21 16:01:24,154 - INFO - Batch 210, Loss: 8.5431, LR: 0.000000
2025-10-21 16:01:25,570 - INFO - Batch 220, Loss: 8.5584, LR: 0.000000
2025-10-21 16:01:26,978 - INFO - Batch 230, Loss: 8.5104, LR: 0.000000
2025-10-21 16:01:28,400 - INFO - Batch 240, Loss: 8.5453, LR: 0.000000
2025-10-21 16:01:29,820 - INFO - Batch 250, Loss: 8.5379, LR: 0.000000
2025-10-21 16:01:31,236 - INFO - Batch 260, Loss: 8.5080, LR: 0.000000
2025-10-21 16:01:32,659 - INFO - Batch 270, Loss: 8.5229, LR: 0.000000
2025-10-21 16:01:34,086 - INFO - Batch 280, Loss: 8.5202, LR: 0.000000
2025-10-21 16:01:35,499 - INFO - Batch 290, Loss: 8.5185, LR: 0.000000
2025-10-21 16:01:36,905 - INFO - Batch 300, Loss: 8.4848, LR: 0.000000
2025-10-21 16:01:38,312 - INFO - Batch 310, Loss: 8.4824, LR: 0.000000
2025-10-21 16:01:39,720 - INFO - Batch 320, Loss: 8.4727, LR: 0.000000
2025-10-21 16:01:41,137 - INFO - Batch 330, Loss: 8.4820, LR: 0.000000
2025-10-21 16:01:42,549 - INFO - Batch 340, Loss: 8.4707, LR: 0.000000
2025-10-21 16:01:43,967 - INFO - Batch 350, Loss: 8.4901, LR: 0.000000
2025-10-21 16:01:45,385 - INFO - Batch 360, Loss: 8.4400, LR: 0.000000
2025-10-21 16:01:46,789 - INFO - Batch 370, Loss: 8.4666, LR: 0.000000
2025-10-21 16:01:48,207 - INFO - Batch 380, Loss: 8.4519, LR: 0.000000
2025-10-21 16:01:49,625 - INFO - Batch 390, Loss: 8.4799, LR: 0.000000
2025-10-21 16:01:51,039 - INFO - Batch 400, Loss: 8.4457, LR: 0.000000
2025-10-21 16:01:52,458 - INFO - Batch 410, Loss: 8.4442, LR: 0.000000
2025-10-21 16:01:53,865 - INFO - Batch 420, Loss: 8.4208, LR: 0.000000
2025-10-21 16:01:55,282 - INFO - Batch 430, Loss: 8.4269, LR: 0.000000
2025-10-21 16:01:56,700 - INFO - Batch 440, Loss: 8.4237, LR: 0.000000
2025-10-21 16:02:39,172 - INFO - 使用设备: cuda:2
2025-10-21 16:02:39,172 - INFO - 加载数据...
2025-10-21 16:02:39,172 - INFO - 加载训练数据...
2025-10-21 16:02:42,996 - INFO - 成功加载 data/IWSLT2017/iwslt2017_train.pkl
2025-10-21 16:02:42,997 - INFO - 训练数据结构:
2025-10-21 16:02:42,997 - INFO - 数据结构分析:
2025-10-21 16:02:42,997 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 16:02:42,997 - INFO - 
英语相关语言对:
2025-10-21 16:02:42,997 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 16:02:42,997 - INFO -   en->de: 206112 个样本
2025-10-21 16:02:42,997 - INFO - 
德语相关语言对:
2025-10-21 16:02:42,997 - INFO - 德语->其他语言: ['en']
2025-10-21 16:02:42,997 - INFO -   de->en: 206112 个样本
2025-10-21 16:02:42,997 - INFO - 提取 en->de 的翻译对
2025-10-21 16:02:42,997 - INFO - 找到 en->de: 206112 个样本
2025-10-21 16:02:43,032 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 16:02:43,032 - INFO - 加载验证数据...
2025-10-21 16:02:43,063 - INFO - 成功加载 data/IWSLT2017/iwslt2017_validation.pkl
2025-10-21 16:02:43,063 - INFO - 提取 en->de 的翻译对
2025-10-21 16:02:43,063 - INFO - 找到 en->de: 888 个样本
2025-10-21 16:02:43,064 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 16:02:43,064 - INFO - 加载测试数据...
2025-10-21 16:02:43,149 - INFO - 成功加载 data/IWSLT2017/iwslt2017_test.pkl
2025-10-21 16:02:43,149 - INFO - 提取 en->de 的翻译对
2025-10-21 16:02:43,149 - INFO - 找到 en->de: 8079 个样本
2025-10-21 16:02:43,151 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 16:02:43,151 - INFO - 开始构建分词器...
2025-10-21 16:02:43,183 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 16:02:44,709 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 16:02:47,667 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 16:02:47,729 - INFO - 数据加载器创建完成
2025-10-21 16:02:47,729 - INFO - 训练集: 206112 个样本
2025-10-21 16:02:47,729 - INFO - 验证集: 888 个样本
2025-10-21 16:02:47,729 - INFO - 测试集: 8079 个样本
2025-10-21 16:02:48,978 - INFO - 源语言词汇表大小: 5000
2025-10-21 16:02:48,978 - INFO - 目标语言词汇表大小: 5000
2025-10-21 16:02:48,978 - INFO - 创建模型...
2025-10-21 16:02:50,394 - INFO - 总参数数: 15,041,416
2025-10-21 16:02:50,394 - INFO - 可训练参数数: 15,041,416
2025-10-21 16:02:50,394 - INFO - 开始训练...
2025-10-21 16:02:51,446 - INFO - Batch 0, Loss: 8.5653, LR: 0.000000
2025-10-21 16:03:05,469 - INFO - Batch 100, Loss: 8.5599, LR: 0.000000
2025-10-21 16:03:19,531 - INFO - Batch 200, Loss: 8.5410, LR: 0.000000
2025-10-21 16:03:33,547 - INFO - Batch 300, Loss: 8.4848, LR: 0.000000
2025-10-21 16:03:47,889 - INFO - Batch 400, Loss: 8.4457, LR: 0.000000
2025-10-21 16:04:02,275 - INFO - Batch 500, Loss: 8.4150, LR: 0.000000
2025-10-21 16:04:16,706 - INFO - Batch 600, Loss: 8.3398, LR: 0.000000
2025-10-21 16:04:31,124 - INFO - Batch 700, Loss: 8.2754, LR: 0.000000
2025-10-21 16:04:45,553 - INFO - Batch 800, Loss: 8.1447, LR: 0.000000
2025-10-21 16:04:59,887 - INFO - Batch 900, Loss: 8.1485, LR: 0.000000
2025-10-21 16:05:14,085 - INFO - Batch 1000, Loss: 8.1125, LR: 0.000000
2025-10-21 16:05:28,193 - INFO - Batch 1100, Loss: 8.0379, LR: 0.000000
2025-10-21 16:05:42,512 - INFO - Batch 1200, Loss: 7.9482, LR: 0.000000
2025-10-21 16:05:56,677 - INFO - Batch 1300, Loss: 7.9286, LR: 0.000001
2025-10-21 16:06:10,810 - INFO - Batch 1400, Loss: 7.9143, LR: 0.000001
2025-10-21 16:06:24,991 - INFO - Batch 1500, Loss: 7.8296, LR: 0.000001
2025-10-21 16:06:39,295 - INFO - Batch 1600, Loss: 7.8348, LR: 0.000001
2025-10-21 16:06:53,613 - INFO - Batch 1700, Loss: 7.7973, LR: 0.000001
2025-10-21 16:07:07,928 - INFO - Batch 1800, Loss: 7.7022, LR: 0.000001
2025-10-21 16:07:22,401 - INFO - Batch 1900, Loss: 7.7370, LR: 0.000001
2025-10-21 16:07:36,576 - INFO - Batch 2000, Loss: 7.6842, LR: 0.000001
2025-10-21 16:07:50,756 - INFO - Batch 2100, Loss: 7.6726, LR: 0.000001
2025-10-21 16:08:04,910 - INFO - Batch 2200, Loss: 7.6625, LR: 0.000001
2025-10-21 16:08:19,108 - INFO - Batch 2300, Loss: 7.5754, LR: 0.000001
2025-10-21 16:08:33,249 - INFO - Batch 2400, Loss: 7.6186, LR: 0.000001
2025-10-21 16:08:47,374 - INFO - Batch 2500, Loss: 7.5088, LR: 0.000001
2025-10-21 16:09:01,547 - INFO - Batch 2600, Loss: 7.4623, LR: 0.000001
2025-10-21 16:09:15,763 - INFO - Batch 2700, Loss: 7.4077, LR: 0.000001
2025-10-21 16:09:29,919 - INFO - Batch 2800, Loss: 7.3588, LR: 0.000001
2025-10-21 16:09:44,101 - INFO - Batch 2900, Loss: 7.3770, LR: 0.000001
2025-10-21 16:09:58,260 - INFO - Batch 3000, Loss: 7.4309, LR: 0.000001
2025-10-21 16:10:12,598 - INFO - Batch 3100, Loss: 7.3456, LR: 0.000001
2025-10-21 16:10:26,996 - INFO - Batch 3200, Loss: 7.3509, LR: 0.000001
2025-10-21 16:10:41,413 - INFO - Batch 3300, Loss: 7.2978, LR: 0.000001
2025-10-21 16:10:55,843 - INFO - Batch 3400, Loss: 7.2237, LR: 0.000001
2025-10-21 16:11:10,247 - INFO - Batch 3500, Loss: 7.2454, LR: 0.000001
2025-10-21 16:11:24,658 - INFO - Batch 3600, Loss: 7.1670, LR: 0.000001
2025-10-21 16:11:38,837 - INFO - Batch 3700, Loss: 7.1690, LR: 0.000001
2025-10-21 16:11:53,064 - INFO - Batch 3800, Loss: 7.0596, LR: 0.000002
2025-10-21 16:12:07,265 - INFO - Batch 3900, Loss: 7.0600, LR: 0.000002
2025-10-21 16:12:21,547 - INFO - Batch 4000, Loss: 7.0660, LR: 0.000002
2025-10-21 16:12:35,772 - INFO - Batch 4100, Loss: 7.0123, LR: 0.000002
2025-10-21 16:12:49,980 - INFO - Batch 4200, Loss: 6.9384, LR: 0.000002
2025-10-21 16:13:04,203 - INFO - Batch 4300, Loss: 6.9608, LR: 0.000002
2025-10-21 16:13:18,388 - INFO - Batch 4400, Loss: 6.9568, LR: 0.000002
2025-10-21 16:13:32,595 - INFO - Batch 4500, Loss: 6.8820, LR: 0.000001
2025-10-21 16:13:46,820 - INFO - Batch 4600, Loss: 6.9345, LR: 0.000001
2025-10-21 16:14:01,115 - INFO - Batch 4700, Loss: 6.9031, LR: 0.000001
2025-10-21 16:14:15,272 - INFO - Batch 4800, Loss: 6.9266, LR: 0.000001
2025-10-21 16:14:29,437 - INFO - Batch 4900, Loss: 6.8593, LR: 0.000001
2025-10-21 16:14:43,563 - INFO - Batch 5000, Loss: 6.8580, LR: 0.000001
2025-10-21 16:14:57,736 - INFO - Batch 5100, Loss: 6.8272, LR: 0.000001
2025-10-21 16:15:11,911 - INFO - Batch 5200, Loss: 6.8554, LR: 0.000001
2025-10-21 16:15:26,124 - INFO - Batch 5300, Loss: 6.6353, LR: 0.000001
2025-10-21 16:15:40,332 - INFO - Batch 5400, Loss: 6.6798, LR: 0.000001
2025-10-21 16:15:54,506 - INFO - Batch 5500, Loss: 6.6584, LR: 0.000001
2025-10-21 16:16:08,787 - INFO - Batch 5600, Loss: 6.7312, LR: 0.000001
2025-10-21 16:16:23,152 - INFO - Batch 5700, Loss: 6.5569, LR: 0.000001
2025-10-21 16:16:37,613 - INFO - Batch 5800, Loss: 6.6550, LR: 0.000001
2025-10-21 16:16:52,110 - INFO - Batch 5900, Loss: 6.7759, LR: 0.000001
2025-10-21 16:17:06,628 - INFO - Batch 6000, Loss: 6.7583, LR: 0.000001
2025-10-21 16:17:21,067 - INFO - Batch 6100, Loss: 6.6846, LR: 0.000001
2025-10-21 16:17:35,482 - INFO - Batch 6200, Loss: 6.8186, LR: 0.000001
2025-10-21 16:17:49,745 - INFO - Batch 6300, Loss: 6.6060, LR: 0.000001
2025-10-21 16:18:04,209 - INFO - Batch 6400, Loss: 6.6739, LR: 0.000001
2025-10-21 16:18:11,440 - INFO - Epoch 1/50: Train Loss: 7.4002, Val Loss: 6.6818, LR: 0.000001
2025-10-21 16:18:11,655 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 16:18:11,805 - INFO - Batch 0, Loss: 6.6887, LR: 0.000001
2025-10-21 16:18:26,102 - INFO - Batch 100, Loss: 6.5279, LR: 0.000001
2025-10-21 16:18:40,431 - INFO - Batch 200, Loss: 6.6431, LR: 0.000001
2025-10-21 16:19:35,202 - INFO - 使用设备: cuda:2
2025-10-21 16:19:35,202 - INFO - 加载数据...
2025-10-21 16:19:35,202 - INFO - 加载训练数据...
2025-10-21 16:19:38,971 - INFO - 成功加载 data/IWSLT2017/iwslt2017_train.pkl
2025-10-21 16:19:38,972 - INFO - 训练数据结构:
2025-10-21 16:19:38,972 - INFO - 数据结构分析:
2025-10-21 16:19:38,972 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 16:19:38,972 - INFO - 
英语相关语言对:
2025-10-21 16:19:38,972 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 16:19:38,972 - INFO -   en->de: 206112 个样本
2025-10-21 16:19:38,972 - INFO - 
德语相关语言对:
2025-10-21 16:19:38,972 - INFO - 德语->其他语言: ['en']
2025-10-21 16:19:38,972 - INFO -   de->en: 206112 个样本
2025-10-21 16:19:38,972 - INFO - 提取 en->de 的翻译对
2025-10-21 16:19:38,972 - INFO - 找到 en->de: 206112 个样本
2025-10-21 16:19:39,006 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 16:19:39,006 - INFO - 加载验证数据...
2025-10-21 16:19:39,037 - INFO - 成功加载 data/IWSLT2017/iwslt2017_validation.pkl
2025-10-21 16:19:39,037 - INFO - 提取 en->de 的翻译对
2025-10-21 16:19:39,037 - INFO - 找到 en->de: 888 个样本
2025-10-21 16:19:39,037 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 16:19:39,037 - INFO - 加载测试数据...
2025-10-21 16:19:39,120 - INFO - 成功加载 data/IWSLT2017/iwslt2017_test.pkl
2025-10-21 16:19:39,120 - INFO - 提取 en->de 的翻译对
2025-10-21 16:19:39,120 - INFO - 找到 en->de: 8079 个样本
2025-10-21 16:19:39,121 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 16:19:39,122 - INFO - 开始构建分词器...
2025-10-21 16:19:39,152 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 16:19:40,733 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 16:19:43,643 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 16:19:43,703 - INFO - 数据加载器创建完成
2025-10-21 16:19:43,703 - INFO - 训练集: 206112 个样本
2025-10-21 16:19:43,703 - INFO - 验证集: 888 个样本
2025-10-21 16:19:43,703 - INFO - 测试集: 8079 个样本
2025-10-21 16:19:44,948 - INFO - 源语言词汇表大小: 5000
2025-10-21 16:19:44,948 - INFO - 目标语言词汇表大小: 5000
2025-10-21 16:19:44,948 - INFO - 创建模型...
2025-10-21 16:19:46,423 - INFO - 总参数数: 15,041,416
2025-10-21 16:19:46,423 - INFO - 可训练参数数: 15,041,416
2025-10-21 16:19:46,423 - INFO - 开始训练...
2025-10-21 16:19:47,468 - INFO - Batch 0, Loss: 8.5653, LR: 0.000000
2025-10-21 16:20:01,626 - INFO - Batch 100, Loss: 8.5455, LR: 0.000000
2025-10-21 16:20:15,652 - INFO - Batch 200, Loss: 8.4772, LR: 0.000000
2025-10-21 16:20:29,652 - INFO - Batch 300, Loss: 8.3519, LR: 0.000000
2025-10-21 16:20:43,726 - INFO - Batch 400, Loss: 8.2600, LR: 0.000000
2025-10-21 16:20:57,841 - INFO - Batch 500, Loss: 8.2270, LR: 0.000001
2025-10-21 16:21:11,942 - INFO - Batch 600, Loss: 8.1041, LR: 0.000001
2025-10-21 16:21:26,038 - INFO - Batch 700, Loss: 8.0128, LR: 0.000001
2025-10-21 16:21:40,138 - INFO - Batch 800, Loss: 7.8412, LR: 0.000001
2025-10-21 16:21:54,256 - INFO - Batch 900, Loss: 7.8436, LR: 0.000001
2025-10-21 16:22:08,423 - INFO - Batch 1000, Loss: 7.8315, LR: 0.000001
2025-10-21 16:22:22,638 - INFO - Batch 1100, Loss: 7.7403, LR: 0.000001
2025-10-21 16:22:36,899 - INFO - Batch 1200, Loss: 7.6123, LR: 0.000001
2025-10-21 16:22:51,351 - INFO - Batch 1300, Loss: 7.6173, LR: 0.000001
2025-10-21 16:23:05,826 - INFO - Batch 1400, Loss: 7.6083, LR: 0.000002
2025-10-21 16:23:20,297 - INFO - Batch 1500, Loss: 7.4879, LR: 0.000002
2025-10-21 16:23:34,746 - INFO - Batch 1600, Loss: 7.4973, LR: 0.000002
2025-10-21 16:23:49,212 - INFO - Batch 1700, Loss: 7.4184, LR: 0.000002
2025-10-21 16:24:03,673 - INFO - Batch 1800, Loss: 7.2996, LR: 0.000002
2025-10-21 16:24:18,098 - INFO - Batch 1900, Loss: 7.3345, LR: 0.000002
2025-10-21 16:24:32,255 - INFO - Batch 2000, Loss: 7.2510, LR: 0.000002
2025-10-21 16:24:46,397 - INFO - Batch 2100, Loss: 7.2298, LR: 0.000002
2025-10-21 16:25:00,536 - INFO - Batch 2200, Loss: 7.1929, LR: 0.000002
2025-10-21 16:25:14,863 - INFO - Batch 2300, Loss: 7.0742, LR: 0.000002
2025-10-21 16:25:29,351 - INFO - Batch 2400, Loss: 7.1279, LR: 0.000002
2025-10-21 16:25:43,860 - INFO - Batch 2500, Loss: 6.9861, LR: 0.000002
2025-10-21 16:25:58,381 - INFO - Batch 2600, Loss: 6.9533, LR: 0.000002
2025-10-21 16:26:12,868 - INFO - Batch 2700, Loss: 6.8772, LR: 0.000002
2025-10-21 16:26:27,364 - INFO - Batch 2800, Loss: 6.8050, LR: 0.000002
2025-10-21 16:26:41,932 - INFO - Batch 2900, Loss: 6.8707, LR: 0.000002
2025-10-21 16:26:56,355 - INFO - Batch 3000, Loss: 6.9306, LR: 0.000002
2025-10-21 16:27:10,704 - INFO - Batch 3100, Loss: 6.8642, LR: 0.000002
2025-10-21 16:27:25,056 - INFO - Batch 3200, Loss: 6.8504, LR: 0.000002
2025-10-21 16:27:39,314 - INFO - Batch 3300, Loss: 6.8481, LR: 0.000002
2025-10-21 16:27:53,587 - INFO - Batch 3400, Loss: 6.7665, LR: 0.000002
2025-10-21 16:28:07,894 - INFO - Batch 3500, Loss: 6.7917, LR: 0.000002
2025-10-21 16:28:22,241 - INFO - Batch 3600, Loss: 6.7168, LR: 0.000002
2025-10-21 16:28:36,641 - INFO - Batch 3700, Loss: 6.7769, LR: 0.000002
2025-10-21 16:28:51,026 - INFO - Batch 3800, Loss: 6.6804, LR: 0.000002
2025-10-21 16:29:05,496 - INFO - Batch 3900, Loss: 6.6982, LR: 0.000002
2025-10-21 16:29:20,004 - INFO - Batch 4000, Loss: 6.7126, LR: 0.000002
2025-10-21 16:29:34,481 - INFO - Batch 4100, Loss: 6.7085, LR: 0.000002
2025-10-21 16:29:48,898 - INFO - Batch 4200, Loss: 6.6280, LR: 0.000002
2025-10-21 16:30:03,201 - INFO - Batch 4300, Loss: 6.6646, LR: 0.000002
2025-10-21 16:30:17,513 - INFO - Batch 4400, Loss: 6.6888, LR: 0.000002
2025-10-21 16:30:32,004 - INFO - Batch 4500, Loss: 6.6101, LR: 0.000001
2025-10-21 16:30:46,518 - INFO - Batch 4600, Loss: 6.7046, LR: 0.000001
2025-10-21 16:31:01,006 - INFO - Batch 4700, Loss: 6.6668, LR: 0.000001
2025-10-21 16:31:15,377 - INFO - Batch 4800, Loss: 6.7059, LR: 0.000001
2025-10-21 16:31:29,649 - INFO - Batch 4900, Loss: 6.6320, LR: 0.000001
2025-10-21 16:31:43,948 - INFO - Batch 5000, Loss: 6.6502, LR: 0.000001
2025-10-21 16:31:58,223 - INFO - Batch 5100, Loss: 6.6041, LR: 0.000001
2025-10-21 16:32:12,666 - INFO - Batch 5200, Loss: 6.6694, LR: 0.000001
2025-10-21 16:32:27,083 - INFO - Batch 5300, Loss: 6.4293, LR: 0.000001
2025-10-21 16:32:41,358 - INFO - Batch 5400, Loss: 6.4750, LR: 0.000001
2025-10-21 16:32:55,646 - INFO - Batch 5500, Loss: 6.4721, LR: 0.000001
2025-10-21 16:33:09,941 - INFO - Batch 5600, Loss: 6.5564, LR: 0.000001
2025-10-21 16:33:24,189 - INFO - Batch 5700, Loss: 6.3807, LR: 0.000001
2025-10-21 16:33:38,653 - INFO - Batch 5800, Loss: 6.4959, LR: 0.000001
2025-10-21 16:33:53,203 - INFO - Batch 5900, Loss: 6.6450, LR: 0.000001
2025-10-21 16:34:07,789 - INFO - Batch 6000, Loss: 6.6200, LR: 0.000001
2025-10-21 16:34:22,214 - INFO - Batch 6100, Loss: 6.5380, LR: 0.000001
2025-10-21 16:34:36,429 - INFO - Batch 6200, Loss: 6.7182, LR: 0.000001
2025-10-21 16:34:50,678 - INFO - Batch 6300, Loss: 6.4851, LR: 0.000001
2025-10-21 16:35:05,052 - INFO - Batch 6400, Loss: 6.5502, LR: 0.000001
2025-10-21 16:35:12,154 - INFO - Epoch 1/30: Train Loss: 7.0981, Val Loss: 6.5863, LR: 0.000001
2025-10-21 16:35:12,389 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 16:35:12,541 - INFO - Batch 0, Loss: 6.5839, LR: 0.000001
2025-10-21 16:35:26,837 - INFO - Batch 100, Loss: 6.4070, LR: 0.000001
2025-10-21 16:35:41,132 - INFO - Batch 200, Loss: 6.5416, LR: 0.000001
2025-10-21 16:35:55,576 - INFO - Batch 300, Loss: 6.5554, LR: 0.000001
2025-10-21 16:36:10,096 - INFO - Batch 400, Loss: 6.3904, LR: 0.000001
2025-10-21 16:36:24,620 - INFO - Batch 500, Loss: 6.6869, LR: 0.000001
2025-10-21 16:36:39,177 - INFO - Batch 600, Loss: 6.6761, LR: 0.000001
2025-10-21 16:36:53,555 - INFO - Batch 700, Loss: 6.6978, LR: 0.000001
2025-10-21 16:37:07,758 - INFO - Batch 800, Loss: 6.4733, LR: 0.000001
2025-10-21 16:37:22,166 - INFO - Batch 900, Loss: 6.5495, LR: 0.000001
2025-10-21 16:37:36,787 - INFO - Batch 1000, Loss: 6.6203, LR: 0.000001
2025-10-21 16:37:51,395 - INFO - Batch 1100, Loss: 6.5960, LR: 0.000001
2025-10-21 16:38:05,763 - INFO - Batch 1200, Loss: 6.5132, LR: 0.000001
2025-10-21 16:38:20,097 - INFO - Batch 1300, Loss: 6.5874, LR: 0.000001
2025-10-21 16:38:34,608 - INFO - Batch 1400, Loss: 6.6150, LR: 0.000001
2025-10-21 16:38:49,139 - INFO - Batch 1500, Loss: 6.4699, LR: 0.000001
2025-10-21 16:39:03,642 - INFO - Batch 1600, Loss: 6.4474, LR: 0.000001
2025-10-21 16:39:17,899 - INFO - Batch 1700, Loss: 6.3946, LR: 0.000001
2025-10-21 16:39:32,224 - INFO - Batch 1800, Loss: 6.6054, LR: 0.000001
2025-10-21 16:39:46,525 - INFO - Batch 1900, Loss: 6.4271, LR: 0.000001
2025-10-21 16:40:00,802 - INFO - Batch 2000, Loss: 6.5695, LR: 0.000001
2025-10-21 16:40:15,049 - INFO - Batch 2100, Loss: 6.3460, LR: 0.000001
2025-10-21 16:40:29,279 - INFO - Batch 2200, Loss: 6.3146, LR: 0.000001
2025-10-21 16:40:43,494 - INFO - Batch 2300, Loss: 6.4041, LR: 0.000001
2025-10-21 16:40:57,780 - INFO - Batch 2400, Loss: 6.5510, LR: 0.000001
2025-10-21 16:41:12,063 - INFO - Batch 2500, Loss: 6.4519, LR: 0.000001
2025-10-21 16:41:26,298 - INFO - Batch 2600, Loss: 6.4568, LR: 0.000001
2025-10-21 16:41:40,549 - INFO - Batch 2700, Loss: 6.4872, LR: 0.000001
2025-10-21 16:41:54,779 - INFO - Batch 2800, Loss: 6.5377, LR: 0.000001
2025-10-21 16:42:08,997 - INFO - Batch 2900, Loss: 6.3872, LR: 0.000001
2025-10-21 16:42:23,364 - INFO - Batch 3000, Loss: 6.5109, LR: 0.000001
2025-10-21 16:42:37,620 - INFO - Batch 3100, Loss: 6.5086, LR: 0.000001
2025-10-21 16:42:51,916 - INFO - Batch 3200, Loss: 6.4533, LR: 0.000001
2025-10-21 16:43:06,100 - INFO - Batch 3300, Loss: 6.5555, LR: 0.000001
2025-10-21 16:43:20,269 - INFO - Batch 3400, Loss: 6.4297, LR: 0.000001
2025-10-21 16:43:34,462 - INFO - Batch 3500, Loss: 6.4601, LR: 0.000001
2025-10-21 16:43:48,661 - INFO - Batch 3600, Loss: 6.4369, LR: 0.000001
2025-10-21 16:44:02,800 - INFO - Batch 3700, Loss: 6.5598, LR: 0.000001
2025-10-21 16:44:16,974 - INFO - Batch 3800, Loss: 6.3272, LR: 0.000001
2025-10-21 16:44:31,114 - INFO - Batch 3900, Loss: 6.5491, LR: 0.000001
2025-10-21 16:44:45,312 - INFO - Batch 4000, Loss: 6.5153, LR: 0.000001
2025-10-21 16:44:59,467 - INFO - Batch 4100, Loss: 6.4393, LR: 0.000001
2025-10-21 16:45:13,643 - INFO - Batch 4200, Loss: 6.5249, LR: 0.000001
2025-10-21 16:45:27,841 - INFO - Batch 4300, Loss: 6.6072, LR: 0.000001
2025-10-21 16:45:42,097 - INFO - Batch 4400, Loss: 6.2055, LR: 0.000001
2025-10-21 16:45:56,335 - INFO - Batch 4500, Loss: 6.2977, LR: 0.000001
2025-10-21 16:46:10,591 - INFO - Batch 4600, Loss: 6.5033, LR: 0.000001
2025-10-21 16:46:24,732 - INFO - Batch 4700, Loss: 6.4862, LR: 0.000001
2025-10-21 16:46:38,914 - INFO - Batch 4800, Loss: 6.4565, LR: 0.000001
2025-10-21 16:46:53,106 - INFO - Batch 4900, Loss: 6.4129, LR: 0.000001
2025-10-21 16:47:07,254 - INFO - Batch 5000, Loss: 6.3331, LR: 0.000001
2025-10-21 16:47:21,474 - INFO - Batch 5100, Loss: 6.3932, LR: 0.000001
2025-10-21 16:47:35,675 - INFO - Batch 5200, Loss: 6.6383, LR: 0.000001
2025-10-21 16:47:49,825 - INFO - Batch 5300, Loss: 6.3909, LR: 0.000001
2025-10-21 16:48:03,981 - INFO - Batch 5400, Loss: 6.4226, LR: 0.000001
2025-10-21 16:48:18,170 - INFO - Batch 5500, Loss: 6.4387, LR: 0.000001
2025-10-21 16:48:32,386 - INFO - Batch 5600, Loss: 6.3821, LR: 0.000001
2025-10-21 16:48:46,559 - INFO - Batch 5700, Loss: 6.3859, LR: 0.000001
2025-10-21 16:49:00,765 - INFO - Batch 5800, Loss: 6.5582, LR: 0.000001
2025-10-21 16:49:14,937 - INFO - Batch 5900, Loss: 6.2731, LR: 0.000001
2025-10-21 16:49:29,126 - INFO - Batch 6000, Loss: 6.4519, LR: 0.000001
2025-10-21 16:49:43,455 - INFO - Batch 6100, Loss: 6.4913, LR: 0.000001
2025-10-21 16:49:57,746 - INFO - Batch 6200, Loss: 6.3701, LR: 0.000001
2025-10-21 16:50:12,052 - INFO - Batch 6300, Loss: 6.5367, LR: 0.000001
2025-10-21 16:50:26,271 - INFO - Batch 6400, Loss: 6.3452, LR: 0.000001
2025-10-21 16:50:33,370 - INFO - Epoch 2/30: Train Loss: 6.4868, Val Loss: 6.4564, LR: 0.000001
2025-10-21 16:50:33,624 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 16:50:33,777 - INFO - Batch 0, Loss: 6.3175, LR: 0.000001
2025-10-21 16:50:48,049 - INFO - Batch 100, Loss: 6.4579, LR: 0.000001
2025-10-21 16:51:02,284 - INFO - Batch 200, Loss: 6.4697, LR: 0.000001
2025-10-21 16:51:16,579 - INFO - Batch 300, Loss: 6.5109, LR: 0.000001
2025-10-21 16:51:30,781 - INFO - Batch 400, Loss: 6.5351, LR: 0.000001
2025-10-21 16:51:45,156 - INFO - Batch 500, Loss: 6.4228, LR: 0.000001
2025-10-21 16:51:59,583 - INFO - Batch 600, Loss: 6.3347, LR: 0.000001
2025-10-21 16:52:14,043 - INFO - Batch 700, Loss: 6.3559, LR: 0.000001
2025-10-21 16:52:28,463 - INFO - Batch 800, Loss: 6.4543, LR: 0.000001
2025-10-21 16:52:42,949 - INFO - Batch 900, Loss: 6.3803, LR: 0.000001
2025-10-21 16:52:57,416 - INFO - Batch 1000, Loss: 6.3836, LR: 0.000001
2025-10-21 16:53:11,815 - INFO - Batch 1100, Loss: 6.5475, LR: 0.000001
2025-10-21 16:53:26,198 - INFO - Batch 1200, Loss: 6.4480, LR: 0.000001
2025-10-21 16:53:40,404 - INFO - Batch 1300, Loss: 6.3584, LR: 0.000001
2025-10-21 16:53:54,600 - INFO - Batch 1400, Loss: 6.4676, LR: 0.000001
2025-10-21 16:54:08,840 - INFO - Batch 1500, Loss: 6.4728, LR: 0.000001
2025-10-21 16:54:23,080 - INFO - Batch 1600, Loss: 6.5427, LR: 0.000001
2025-10-21 16:54:37,289 - INFO - Batch 1700, Loss: 6.3386, LR: 0.000001
2025-10-21 16:54:51,528 - INFO - Batch 1800, Loss: 6.3085, LR: 0.000001
2025-10-21 16:55:05,763 - INFO - Batch 1900, Loss: 6.2444, LR: 0.000001
2025-10-21 16:55:19,977 - INFO - Batch 2000, Loss: 6.3841, LR: 0.000001
2025-10-21 16:55:34,199 - INFO - Batch 2100, Loss: 6.4608, LR: 0.000001
2025-10-21 16:55:48,408 - INFO - Batch 2200, Loss: 6.4087, LR: 0.000001
2025-10-21 16:56:02,660 - INFO - Batch 2300, Loss: 6.4350, LR: 0.000001
2025-10-21 16:56:16,912 - INFO - Batch 2400, Loss: 6.4079, LR: 0.000001
2025-10-21 16:56:31,112 - INFO - Batch 2500, Loss: 6.4209, LR: 0.000001
2025-10-21 16:56:45,310 - INFO - Batch 2600, Loss: 6.3392, LR: 0.000001
2025-10-21 16:56:59,548 - INFO - Batch 2700, Loss: 6.3628, LR: 0.000001
2025-10-21 16:57:13,792 - INFO - Batch 2800, Loss: 6.3103, LR: 0.000001
2025-10-21 16:57:28,116 - INFO - Batch 2900, Loss: 6.5988, LR: 0.000001
2025-10-21 16:57:42,358 - INFO - Batch 3000, Loss: 6.3103, LR: 0.000001
2025-10-21 16:57:56,627 - INFO - Batch 3100, Loss: 6.4230, LR: 0.000001
2025-10-21 16:58:10,921 - INFO - Batch 3200, Loss: 6.3960, LR: 0.000001
2025-10-21 16:58:25,121 - INFO - Batch 3300, Loss: 6.3632, LR: 0.000001
2025-10-21 16:58:39,329 - INFO - Batch 3400, Loss: 6.3215, LR: 0.000001
2025-10-21 16:58:53,562 - INFO - Batch 3500, Loss: 6.4417, LR: 0.000001
2025-10-21 16:59:07,781 - INFO - Batch 3600, Loss: 6.3381, LR: 0.000001
2025-10-21 16:59:22,006 - INFO - Batch 3700, Loss: 6.2826, LR: 0.000001
2025-10-21 16:59:36,317 - INFO - Batch 3800, Loss: 6.3624, LR: 0.000001
2025-10-21 16:59:50,563 - INFO - Batch 3900, Loss: 6.3841, LR: 0.000001
2025-10-21 17:00:04,836 - INFO - Batch 4000, Loss: 6.3880, LR: 0.000001
2025-10-21 17:00:19,044 - INFO - Batch 4100, Loss: 6.4986, LR: 0.000001
2025-10-21 17:00:33,233 - INFO - Batch 4200, Loss: 6.3356, LR: 0.000001
2025-10-21 17:00:47,446 - INFO - Batch 4300, Loss: 6.4797, LR: 0.000001
2025-10-21 17:01:01,582 - INFO - Batch 4400, Loss: 6.4795, LR: 0.000001
2025-10-21 17:01:15,791 - INFO - Batch 4500, Loss: 6.4838, LR: 0.000001
2025-10-21 17:01:29,948 - INFO - Batch 4600, Loss: 6.3444, LR: 0.000001
2025-10-21 17:01:44,174 - INFO - Batch 4700, Loss: 6.3865, LR: 0.000001
2025-10-21 17:01:58,444 - INFO - Batch 4800, Loss: 6.3742, LR: 0.000001
2025-10-21 17:02:12,708 - INFO - Batch 4900, Loss: 6.3881, LR: 0.000001
2025-10-21 17:02:27,098 - INFO - Batch 5000, Loss: 6.2278, LR: 0.000001
2025-10-21 17:02:41,395 - INFO - Batch 5100, Loss: 6.4776, LR: 0.000001
2025-10-21 17:02:55,669 - INFO - Batch 5200, Loss: 6.2496, LR: 0.000001
2025-10-21 17:03:09,967 - INFO - Batch 5300, Loss: 6.3066, LR: 0.000001
2025-10-21 17:03:24,245 - INFO - Batch 5400, Loss: 6.4378, LR: 0.000001
2025-10-21 17:03:38,541 - INFO - Batch 5500, Loss: 6.4534, LR: 0.000001
2025-10-21 17:03:52,730 - INFO - Batch 5600, Loss: 6.4314, LR: 0.000001
2025-10-21 17:04:06,854 - INFO - Batch 5700, Loss: 6.3628, LR: 0.000001
2025-10-21 17:04:20,976 - INFO - Batch 5800, Loss: 6.3070, LR: 0.000001
2025-10-21 17:04:35,145 - INFO - Batch 5900, Loss: 6.4393, LR: 0.000001
2025-10-21 17:04:49,304 - INFO - Batch 6000, Loss: 6.2454, LR: 0.000001
2025-10-21 17:05:03,461 - INFO - Batch 6100, Loss: 6.4613, LR: 0.000001
2025-10-21 17:05:17,599 - INFO - Batch 6200, Loss: 6.1836, LR: 0.000001
2025-10-21 17:05:31,818 - INFO - Batch 6300, Loss: 6.4297, LR: 0.000001
2025-10-21 17:05:45,976 - INFO - Batch 6400, Loss: 6.2422, LR: 0.000001
2025-10-21 17:05:53,052 - INFO - Epoch 3/30: Train Loss: 6.3831, Val Loss: 6.3625, LR: 0.000001
2025-10-21 17:05:53,289 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 17:05:53,440 - INFO - Batch 0, Loss: 6.2683, LR: 0.000001
2025-10-21 17:06:07,713 - INFO - Batch 100, Loss: 6.1755, LR: 0.000001
2025-10-21 17:06:21,997 - INFO - Batch 200, Loss: 6.4216, LR: 0.000001
2025-10-21 17:06:36,283 - INFO - Batch 300, Loss: 6.3093, LR: 0.000001
2025-10-21 17:06:50,546 - INFO - Batch 400, Loss: 6.4515, LR: 0.000001
2025-10-21 17:07:04,782 - INFO - Batch 500, Loss: 6.3592, LR: 0.000001
2025-10-21 17:07:19,126 - INFO - Batch 600, Loss: 6.3666, LR: 0.000001
2025-10-21 17:07:33,398 - INFO - Batch 700, Loss: 6.2800, LR: 0.000001
2025-10-21 17:07:47,620 - INFO - Batch 800, Loss: 6.3896, LR: 0.000001
2025-10-21 17:08:01,889 - INFO - Batch 900, Loss: 6.4353, LR: 0.000001
2025-10-21 17:08:16,158 - INFO - Batch 1000, Loss: 6.3376, LR: 0.000001
2025-10-21 17:08:30,453 - INFO - Batch 1100, Loss: 6.5066, LR: 0.000001
2025-10-21 17:08:44,712 - INFO - Batch 1200, Loss: 6.2612, LR: 0.000001
2025-10-21 17:08:59,012 - INFO - Batch 1300, Loss: 6.2738, LR: 0.000001
2025-10-21 17:09:13,285 - INFO - Batch 1400, Loss: 6.3879, LR: 0.000001
2025-10-21 17:09:27,553 - INFO - Batch 1500, Loss: 6.2178, LR: 0.000001
2025-10-21 17:09:41,855 - INFO - Batch 1600, Loss: 6.3613, LR: 0.000001
2025-10-21 17:09:56,204 - INFO - Batch 1700, Loss: 6.2314, LR: 0.000001
2025-10-21 17:10:10,457 - INFO - Batch 1800, Loss: 6.3002, LR: 0.000001
2025-10-21 17:10:24,765 - INFO - Batch 1900, Loss: 6.3946, LR: 0.000001
2025-10-21 17:10:39,056 - INFO - Batch 2000, Loss: 6.2827, LR: 0.000001
2025-10-21 17:10:53,377 - INFO - Batch 2100, Loss: 6.3651, LR: 0.000001
2025-10-21 17:11:07,709 - INFO - Batch 2200, Loss: 6.4508, LR: 0.000001
2025-10-21 17:11:22,004 - INFO - Batch 2300, Loss: 6.1260, LR: 0.000001
2025-10-21 17:11:36,295 - INFO - Batch 2400, Loss: 6.2414, LR: 0.000001
2025-10-21 17:11:50,633 - INFO - Batch 2500, Loss: 6.2778, LR: 0.000001
2025-10-21 17:12:04,914 - INFO - Batch 2600, Loss: 6.2200, LR: 0.000001
2025-10-21 17:12:19,266 - INFO - Batch 2700, Loss: 6.3497, LR: 0.000001
2025-10-21 17:12:33,598 - INFO - Batch 2800, Loss: 6.3057, LR: 0.000001
2025-10-21 17:12:47,920 - INFO - Batch 2900, Loss: 6.3263, LR: 0.000001
2025-10-21 17:13:02,252 - INFO - Batch 3000, Loss: 6.1833, LR: 0.000001
2025-10-21 17:13:16,661 - INFO - Batch 3100, Loss: 6.3319, LR: 0.000001
2025-10-21 17:13:31,091 - INFO - Batch 3200, Loss: 6.1631, LR: 0.000001
2025-10-21 17:13:45,472 - INFO - Batch 3300, Loss: 6.3196, LR: 0.000001
2025-10-21 17:13:59,854 - INFO - Batch 3400, Loss: 6.4423, LR: 0.000001
2025-10-21 17:14:14,299 - INFO - Batch 3500, Loss: 6.2477, LR: 0.000001
2025-10-21 17:14:28,857 - INFO - Batch 3600, Loss: 6.1372, LR: 0.000001
2025-10-21 17:14:43,326 - INFO - Batch 3700, Loss: 6.1834, LR: 0.000001
2025-10-21 17:14:57,814 - INFO - Batch 3800, Loss: 6.3925, LR: 0.000001
2025-10-21 17:15:12,189 - INFO - Batch 3900, Loss: 6.3045, LR: 0.000001
2025-10-21 17:15:26,509 - INFO - Batch 4000, Loss: 6.2838, LR: 0.000001
2025-10-21 17:15:40,804 - INFO - Batch 4100, Loss: 6.3708, LR: 0.000001
2025-10-21 17:15:55,088 - INFO - Batch 4200, Loss: 6.1503, LR: 0.000001
2025-10-21 17:16:09,388 - INFO - Batch 4300, Loss: 6.3858, LR: 0.000001
2025-10-21 17:16:23,656 - INFO - Batch 4400, Loss: 6.3419, LR: 0.000001
2025-10-21 17:16:38,241 - INFO - Batch 4500, Loss: 6.2230, LR: 0.000001
2025-10-21 17:16:52,612 - INFO - Batch 4600, Loss: 6.1182, LR: 0.000001
2025-10-21 17:17:07,201 - INFO - Batch 4700, Loss: 6.1875, LR: 0.000001
2025-10-21 17:17:21,667 - INFO - Batch 4800, Loss: 6.2821, LR: 0.000001
2025-10-21 17:17:35,776 - INFO - Batch 4900, Loss: 6.3062, LR: 0.000001
2025-10-21 17:17:49,922 - INFO - Batch 5000, Loss: 6.3040, LR: 0.000001
2025-10-21 17:18:04,048 - INFO - Batch 5100, Loss: 6.2825, LR: 0.000001
2025-10-21 17:18:18,174 - INFO - Batch 5200, Loss: 6.1424, LR: 0.000001
2025-10-21 17:18:32,313 - INFO - Batch 5300, Loss: 6.0400, LR: 0.000001
2025-10-21 17:18:46,428 - INFO - Batch 5400, Loss: 6.2668, LR: 0.000001
2025-10-21 17:19:00,539 - INFO - Batch 5500, Loss: 6.2950, LR: 0.000001
2025-10-21 17:19:14,657 - INFO - Batch 5600, Loss: 6.2513, LR: 0.000001
2025-10-21 17:19:28,791 - INFO - Batch 5700, Loss: 6.2987, LR: 0.000001
2025-10-21 17:19:42,938 - INFO - Batch 5800, Loss: 6.1875, LR: 0.000001
2025-10-21 17:19:57,060 - INFO - Batch 5900, Loss: 6.2403, LR: 0.000001
2025-10-21 17:20:11,195 - INFO - Batch 6000, Loss: 6.2316, LR: 0.000001
2025-10-21 17:20:25,289 - INFO - Batch 6100, Loss: 6.1778, LR: 0.000001
2025-10-21 17:20:39,418 - INFO - Batch 6200, Loss: 6.3553, LR: 0.000001
2025-10-21 17:20:53,562 - INFO - Batch 6300, Loss: 6.3026, LR: 0.000001
2025-10-21 17:21:07,686 - INFO - Batch 6400, Loss: 6.2488, LR: 0.000001
2025-10-21 17:21:14,713 - INFO - Epoch 4/30: Train Loss: 6.3028, Val Loss: 6.2867, LR: 0.000001
2025-10-21 17:21:14,946 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 17:21:15,096 - INFO - Batch 0, Loss: 6.2126, LR: 0.000001
2025-10-21 17:21:29,357 - INFO - Batch 100, Loss: 6.2791, LR: 0.000001
2025-10-21 17:21:43,558 - INFO - Batch 200, Loss: 6.3312, LR: 0.000001
2025-10-21 17:21:57,793 - INFO - Batch 300, Loss: 6.3631, LR: 0.000001
2025-10-21 17:22:12,066 - INFO - Batch 400, Loss: 6.2069, LR: 0.000001
2025-10-21 17:22:26,374 - INFO - Batch 500, Loss: 6.1694, LR: 0.000001
2025-10-21 17:22:40,737 - INFO - Batch 600, Loss: 6.2111, LR: 0.000001
2025-10-21 17:22:50,237 - INFO - 使用设备: cuda:2
2025-10-21 17:22:50,238 - INFO - 加载数据...
2025-10-21 17:22:50,238 - INFO - 加载训练数据...
2025-10-21 17:22:54,037 - INFO - 成功加载 data/IWSLT2017/iwslt2017_train.pkl
2025-10-21 17:22:54,038 - INFO - 训练数据结构:
2025-10-21 17:22:54,038 - INFO - 数据结构分析:
2025-10-21 17:22:54,038 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 17:22:54,038 - INFO - 
英语相关语言对:
2025-10-21 17:22:54,038 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 17:22:54,038 - INFO -   en->de: 206112 个样本
2025-10-21 17:22:54,038 - INFO - 
德语相关语言对:
2025-10-21 17:22:54,038 - INFO - 德语->其他语言: ['en']
2025-10-21 17:22:54,038 - INFO -   de->en: 206112 个样本
2025-10-21 17:22:54,038 - INFO - 提取 en->de 的翻译对
2025-10-21 17:22:54,038 - INFO - 找到 en->de: 206112 个样本
2025-10-21 17:22:54,072 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 17:22:54,072 - INFO - 加载验证数据...
2025-10-21 17:22:54,104 - INFO - 成功加载 data/IWSLT2017/iwslt2017_validation.pkl
2025-10-21 17:22:54,104 - INFO - 提取 en->de 的翻译对
2025-10-21 17:22:54,104 - INFO - 找到 en->de: 888 个样本
2025-10-21 17:22:54,104 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 17:22:54,104 - INFO - 加载测试数据...
2025-10-21 17:22:54,188 - INFO - 成功加载 data/IWSLT2017/iwslt2017_test.pkl
2025-10-21 17:22:54,188 - INFO - 提取 en->de 的翻译对
2025-10-21 17:22:54,188 - INFO - 找到 en->de: 8079 个样本
2025-10-21 17:22:54,190 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 17:22:54,190 - INFO - 开始构建分词器...
2025-10-21 17:22:54,221 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 17:22:55,161 - INFO - Batch 700, Loss: 6.1258, LR: 0.000001
2025-10-21 17:22:55,846 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 17:22:58,918 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 17:22:58,979 - INFO - 数据加载器创建完成
2025-10-21 17:22:58,979 - INFO - 训练集: 206112 个样本
2025-10-21 17:22:58,979 - INFO - 验证集: 888 个样本
2025-10-21 17:22:58,979 - INFO - 测试集: 8079 个样本
2025-10-21 17:23:00,668 - INFO - 源语言词汇表大小: 5000
2025-10-21 17:23:00,668 - INFO - 目标语言词汇表大小: 5000
2025-10-21 17:23:00,669 - INFO - 创建模型...
2025-10-21 17:23:03,205 - INFO - 总参数数: 15,041,416
2025-10-21 17:23:03,205 - INFO - 可训练参数数: 15,041,416
2025-10-21 17:23:03,206 - INFO - 开始训练...
2025-10-21 17:23:05,589 - INFO - Batch 0, Loss: 8.5653, LR: 0.000000
2025-10-21 17:23:14,310 - INFO - Batch 800, Loss: 6.1909, LR: 0.000001
2025-10-21 17:23:24,674 - INFO - Batch 100, Loss: 8.5455, LR: 0.000000
2025-10-21 17:23:39,062 - INFO - Batch 200, Loss: 8.4772, LR: 0.000000
2025-10-21 17:23:53,421 - INFO - Batch 300, Loss: 8.3519, LR: 0.000000
2025-10-21 18:36:13,009 - INFO - 使用设备: cuda:2
2025-10-21 18:36:13,009 - INFO - 加载数据...
2025-10-21 18:36:13,009 - INFO - 加载训练数据...
2025-10-21 18:36:16,815 - INFO - 成功加载 data/IWSLT2017/iwslt2017_train.pkl
2025-10-21 18:36:16,815 - INFO - 训练数据结构:
2025-10-21 18:36:16,815 - INFO - 数据结构分析:
2025-10-21 18:36:16,815 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 18:36:16,815 - INFO - 
英语相关语言对:
2025-10-21 18:36:16,815 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-21 18:36:16,815 - INFO -   en->de: 206112 个样本
2025-10-21 18:36:16,815 - INFO - 
德语相关语言对:
2025-10-21 18:36:16,815 - INFO - 德语->其他语言: ['en']
2025-10-21 18:36:16,815 - INFO -   de->en: 206112 个样本
2025-10-21 18:36:16,815 - INFO - 提取 en->de 的翻译对
2025-10-21 18:36:16,815 - INFO - 找到 en->de: 206112 个样本
2025-10-21 18:36:16,850 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-21 18:36:16,850 - INFO - 加载验证数据...
2025-10-21 18:36:16,882 - INFO - 成功加载 data/IWSLT2017/iwslt2017_validation.pkl
2025-10-21 18:36:16,882 - INFO - 提取 en->de 的翻译对
2025-10-21 18:36:16,882 - INFO - 找到 en->de: 888 个样本
2025-10-21 18:36:16,882 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-21 18:36:16,882 - INFO - 加载测试数据...
2025-10-21 18:36:16,966 - INFO - 成功加载 data/IWSLT2017/iwslt2017_test.pkl
2025-10-21 18:36:16,966 - INFO - 提取 en->de 的翻译对
2025-10-21 18:36:16,966 - INFO - 找到 en->de: 8079 个样本
2025-10-21 18:36:16,967 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-21 18:36:16,967 - INFO - 开始构建分词器...
2025-10-21 18:36:16,998 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-10-21 18:36:18,583 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-10-21 18:36:21,544 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-21 18:36:21,605 - INFO - 数据加载器创建完成
2025-10-21 18:36:21,605 - INFO - 训练集: 206112 个样本
2025-10-21 18:36:21,605 - INFO - 验证集: 888 个样本
2025-10-21 18:36:21,605 - INFO - 测试集: 8079 个样本
2025-10-21 18:36:22,807 - INFO - 源语言词汇表大小: 5000
2025-10-21 18:36:22,808 - INFO - 目标语言词汇表大小: 5000
2025-10-21 18:36:22,808 - INFO - 创建模型...
2025-10-21 18:36:24,223 - INFO - 总参数数: 15,041,416
2025-10-21 18:36:24,223 - INFO - 可训练参数数: 15,041,416
2025-10-21 18:36:24,224 - INFO - 开始训练...
2025-10-21 18:36:25,269 - INFO - Batch 0, Loss: 8.5653, LR: 0.000000
2025-10-21 18:36:39,192 - INFO - Batch 100, Loss: 8.0509, LR: 0.000005
2025-10-21 18:36:53,217 - INFO - Batch 200, Loss: 7.5752, LR: 0.000010
2025-10-21 18:37:07,205 - INFO - Batch 300, Loss: 7.1791, LR: 0.000015
2025-10-21 18:37:21,282 - INFO - Batch 400, Loss: 6.8169, LR: 0.000020
2025-10-21 18:37:35,401 - INFO - Batch 500, Loss: 6.8743, LR: 0.000025
2025-10-21 18:37:49,510 - INFO - Batch 600, Loss: 6.7779, LR: 0.000030
2025-10-21 18:38:03,615 - INFO - Batch 700, Loss: 6.6596, LR: 0.000035
2025-10-21 18:38:17,680 - INFO - Batch 800, Loss: 6.4064, LR: 0.000040
2025-10-21 18:38:31,792 - INFO - Batch 900, Loss: 6.4369, LR: 0.000045
2025-10-21 18:38:45,949 - INFO - Batch 1000, Loss: 6.5548, LR: 0.000050
2025-10-21 18:39:00,078 - INFO - Batch 1100, Loss: 6.3132, LR: 0.000055
2025-10-21 18:39:14,228 - INFO - Batch 1200, Loss: 6.2779, LR: 0.000060
2025-10-21 18:39:28,337 - INFO - Batch 1300, Loss: 6.2803, LR: 0.000065
2025-10-21 18:39:42,468 - INFO - Batch 1400, Loss: 6.2290, LR: 0.000070
2025-10-21 18:39:56,582 - INFO - Batch 1500, Loss: 5.9483, LR: 0.000075
2025-10-21 18:40:10,705 - INFO - Batch 1600, Loss: 6.1133, LR: 0.000080
2025-10-21 18:40:24,884 - INFO - Batch 1700, Loss: 5.9900, LR: 0.000085
2025-10-21 18:40:39,036 - INFO - Batch 1800, Loss: 5.7866, LR: 0.000090
2025-10-21 18:40:53,201 - INFO - Batch 1900, Loss: 5.9871, LR: 0.000095
2025-10-21 18:41:07,381 - INFO - Batch 2000, Loss: 5.7788, LR: 0.000100
2025-10-21 18:41:21,518 - INFO - Batch 2100, Loss: 5.8143, LR: 0.000100
2025-10-21 18:41:35,693 - INFO - Batch 2200, Loss: 5.6029, LR: 0.000100
2025-10-21 18:41:49,905 - INFO - Batch 2300, Loss: 5.5297, LR: 0.000100
2025-10-21 18:42:04,081 - INFO - Batch 2400, Loss: 5.6362, LR: 0.000100
2025-10-21 18:42:18,292 - INFO - Batch 2500, Loss: 5.4903, LR: 0.000100
2025-10-21 18:42:32,495 - INFO - Batch 2600, Loss: 5.5437, LR: 0.000100
2025-10-21 18:42:46,670 - INFO - Batch 2700, Loss: 5.4360, LR: 0.000100
2025-10-21 18:43:00,843 - INFO - Batch 2800, Loss: 5.2621, LR: 0.000100
2025-10-21 18:43:14,999 - INFO - Batch 2900, Loss: 5.3452, LR: 0.000100
2025-10-21 18:43:29,155 - INFO - Batch 3000, Loss: 5.3715, LR: 0.000100
2025-10-21 18:43:43,252 - INFO - Batch 3100, Loss: 5.5146, LR: 0.000100
2025-10-21 18:43:57,407 - INFO - Batch 3200, Loss: 5.4413, LR: 0.000100
2025-10-21 18:44:11,554 - INFO - Batch 3300, Loss: 5.4802, LR: 0.000100
2025-10-21 18:44:25,676 - INFO - Batch 3400, Loss: 5.2238, LR: 0.000100
2025-10-21 18:44:39,805 - INFO - Batch 3500, Loss: 5.2114, LR: 0.000100
2025-10-21 18:44:53,937 - INFO - Batch 3600, Loss: 5.0777, LR: 0.000100
2025-10-21 18:45:08,091 - INFO - Batch 3700, Loss: 5.3464, LR: 0.000100
2025-10-21 18:45:22,259 - INFO - Batch 3800, Loss: 5.0748, LR: 0.000100
2025-10-21 18:45:36,426 - INFO - Batch 3900, Loss: 5.1715, LR: 0.000100
2025-10-21 18:45:50,533 - INFO - Batch 4000, Loss: 5.1947, LR: 0.000100
2025-10-21 18:46:04,672 - INFO - Batch 4100, Loss: 5.0968, LR: 0.000100
2025-10-21 18:46:18,791 - INFO - Batch 4200, Loss: 5.0386, LR: 0.000100
2025-10-21 18:46:32,944 - INFO - Batch 4300, Loss: 5.1186, LR: 0.000100
2025-10-21 18:46:47,078 - INFO - Batch 4400, Loss: 5.1654, LR: 0.000100
2025-10-21 18:47:01,178 - INFO - Batch 4500, Loss: 5.1408, LR: 0.000100
2025-10-21 18:47:15,301 - INFO - Batch 4600, Loss: 5.1630, LR: 0.000100
2025-10-21 18:47:29,441 - INFO - Batch 4700, Loss: 4.9566, LR: 0.000100
2025-10-21 18:47:43,521 - INFO - Batch 4800, Loss: 5.0587, LR: 0.000100
2025-10-21 18:47:57,597 - INFO - Batch 4900, Loss: 5.0318, LR: 0.000100
2025-10-21 18:48:11,670 - INFO - Batch 5000, Loss: 5.1464, LR: 0.000100
2025-10-21 18:48:25,744 - INFO - Batch 5100, Loss: 4.9704, LR: 0.000100
2025-10-21 18:48:39,776 - INFO - Batch 5200, Loss: 5.1814, LR: 0.000100
2025-10-21 18:48:53,792 - INFO - Batch 5300, Loss: 4.7201, LR: 0.000100
2025-10-21 18:49:07,918 - INFO - Batch 5400, Loss: 4.5806, LR: 0.000100
2025-10-21 18:49:22,316 - INFO - Batch 5500, Loss: 4.7734, LR: 0.000100
2025-10-21 18:49:36,605 - INFO - Batch 5600, Loss: 4.9144, LR: 0.000100
2025-10-21 18:49:50,679 - INFO - Batch 5700, Loss: 4.6365, LR: 0.000100
2025-10-21 18:50:04,793 - INFO - Batch 5800, Loss: 4.8793, LR: 0.000100
2025-10-21 18:50:18,845 - INFO - Batch 5900, Loss: 5.0637, LR: 0.000100
2025-10-21 18:50:32,934 - INFO - Batch 6000, Loss: 4.8647, LR: 0.000100
2025-10-21 18:50:47,036 - INFO - Batch 6100, Loss: 4.9888, LR: 0.000100
2025-10-21 18:51:01,095 - INFO - Batch 6200, Loss: 5.0413, LR: 0.000100
2025-10-21 18:51:15,345 - INFO - Batch 6300, Loss: 4.7921, LR: 0.000100
2025-10-21 18:51:29,747 - INFO - Batch 6400, Loss: 4.8954, LR: 0.000100
2025-10-21 18:51:36,967 - INFO - Epoch 1/30: Train Loss: 5.6219, Val Loss: 4.8213, LR: 0.000100
2025-10-21 18:51:37,354 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 18:51:37,509 - INFO - Batch 0, Loss: 4.7617, LR: 0.000100
2025-10-21 18:51:51,683 - INFO - Batch 100, Loss: 4.4717, LR: 0.000100
2025-10-21 18:52:05,926 - INFO - Batch 200, Loss: 4.6500, LR: 0.000100
2025-10-21 18:52:20,119 - INFO - Batch 300, Loss: 4.7964, LR: 0.000100
2025-10-21 18:52:34,369 - INFO - Batch 400, Loss: 4.5035, LR: 0.000100
2025-10-21 18:52:48,575 - INFO - Batch 500, Loss: 4.8258, LR: 0.000100
2025-10-21 18:53:02,793 - INFO - Batch 600, Loss: 4.8963, LR: 0.000100
2025-10-21 18:53:16,934 - INFO - Batch 700, Loss: 4.8773, LR: 0.000100
2025-10-21 18:53:31,044 - INFO - Batch 800, Loss: 4.6107, LR: 0.000100
2025-10-21 18:53:45,281 - INFO - Batch 900, Loss: 4.7964, LR: 0.000100
2025-10-21 18:53:59,582 - INFO - Batch 1000, Loss: 4.8976, LR: 0.000100
2025-10-21 18:54:13,870 - INFO - Batch 1100, Loss: 4.7680, LR: 0.000100
2025-10-21 18:54:28,070 - INFO - Batch 1200, Loss: 4.6583, LR: 0.000100
2025-10-21 18:54:42,428 - INFO - Batch 1300, Loss: 4.7559, LR: 0.000100
2025-10-21 18:54:56,216 - INFO - Batch 1400, Loss: 4.6659, LR: 0.000100
2025-10-21 18:55:10,355 - INFO - Batch 1500, Loss: 4.4233, LR: 0.000100
2025-10-21 18:55:24,492 - INFO - Batch 1600, Loss: 4.6145, LR: 0.000100
2025-10-21 18:55:38,262 - INFO - Batch 1700, Loss: 4.4037, LR: 0.000100
2025-10-21 18:55:52,176 - INFO - Batch 1800, Loss: 4.6557, LR: 0.000100
2025-10-21 18:56:06,046 - INFO - Batch 1900, Loss: 4.5128, LR: 0.000100
2025-10-21 18:56:20,184 - INFO - Batch 2000, Loss: 4.6318, LR: 0.000100
2025-10-21 18:56:34,295 - INFO - Batch 2100, Loss: 4.3457, LR: 0.000100
2025-10-21 18:56:48,508 - INFO - Batch 2200, Loss: 4.3940, LR: 0.000100
2025-10-21 18:57:02,674 - INFO - Batch 2300, Loss: 4.5513, LR: 0.000100
2025-10-21 18:57:16,903 - INFO - Batch 2400, Loss: 4.7467, LR: 0.000100
2025-10-21 18:57:31,210 - INFO - Batch 2500, Loss: 4.3580, LR: 0.000100
2025-10-21 18:57:45,578 - INFO - Batch 2600, Loss: 4.5245, LR: 0.000100
2025-10-21 18:57:59,840 - INFO - Batch 2700, Loss: 4.5995, LR: 0.000100
2025-10-21 18:58:13,962 - INFO - Batch 2800, Loss: 4.4915, LR: 0.000100
2025-10-21 18:58:28,283 - INFO - Batch 2900, Loss: 4.3384, LR: 0.000100
2025-10-21 18:58:42,253 - INFO - Batch 3000, Loss: 4.5112, LR: 0.000100
2025-10-21 18:58:56,362 - INFO - Batch 3100, Loss: 4.5762, LR: 0.000100
2025-10-21 18:59:10,581 - INFO - Batch 3200, Loss: 4.4811, LR: 0.000100
2025-10-21 18:59:24,758 - INFO - Batch 3300, Loss: 4.5587, LR: 0.000100
2025-10-21 18:59:38,933 - INFO - Batch 3400, Loss: 4.4625, LR: 0.000100
2025-10-21 18:59:53,249 - INFO - Batch 3500, Loss: 4.5318, LR: 0.000100
2025-10-21 19:00:07,634 - INFO - Batch 3600, Loss: 4.4896, LR: 0.000100
2025-10-21 19:00:21,857 - INFO - Batch 3700, Loss: 4.4924, LR: 0.000100
2025-10-21 19:00:36,191 - INFO - Batch 3800, Loss: 4.3166, LR: 0.000100
2025-10-21 19:00:50,467 - INFO - Batch 3900, Loss: 4.3165, LR: 0.000100
2025-10-21 19:01:04,720 - INFO - Batch 4000, Loss: 4.5506, LR: 0.000100
2025-10-21 19:01:19,001 - INFO - Batch 4100, Loss: 4.3345, LR: 0.000100
2025-10-21 19:01:33,310 - INFO - Batch 4200, Loss: 4.3632, LR: 0.000099
2025-10-21 19:01:47,443 - INFO - Batch 4300, Loss: 4.5377, LR: 0.000099
2025-10-21 19:02:01,581 - INFO - Batch 4400, Loss: 4.0486, LR: 0.000099
2025-10-21 19:02:15,746 - INFO - Batch 4500, Loss: 4.0846, LR: 0.000099
2025-10-21 19:02:30,210 - INFO - Batch 4600, Loss: 4.4516, LR: 0.000099
2025-10-21 19:02:44,587 - INFO - Batch 4700, Loss: 4.5330, LR: 0.000099
2025-10-21 19:02:58,869 - INFO - Batch 4800, Loss: 4.3386, LR: 0.000099
2025-10-21 19:03:13,105 - INFO - Batch 4900, Loss: 4.4036, LR: 0.000099
2025-10-21 19:03:27,478 - INFO - Batch 5000, Loss: 4.1006, LR: 0.000099
2025-10-21 19:03:41,711 - INFO - Batch 5100, Loss: 4.1783, LR: 0.000099
2025-10-21 19:03:55,921 - INFO - Batch 5200, Loss: 4.5671, LR: 0.000099
2025-10-21 19:04:10,077 - INFO - Batch 5300, Loss: 4.1878, LR: 0.000099
2025-10-21 19:04:24,251 - INFO - Batch 5400, Loss: 4.4491, LR: 0.000099
2025-10-21 19:04:38,717 - INFO - Batch 5500, Loss: 4.3341, LR: 0.000099
2025-10-21 19:04:52,921 - INFO - Batch 5600, Loss: 4.2817, LR: 0.000099
2025-10-21 19:05:07,189 - INFO - Batch 5700, Loss: 4.1884, LR: 0.000099
2025-10-21 19:05:21,384 - INFO - Batch 5800, Loss: 4.4921, LR: 0.000099
2025-10-21 19:05:35,597 - INFO - Batch 5900, Loss: 3.9791, LR: 0.000099
2025-10-21 19:05:49,793 - INFO - Batch 6000, Loss: 4.2923, LR: 0.000099
2025-10-21 19:06:03,986 - INFO - Batch 6100, Loss: 4.3514, LR: 0.000099
2025-10-21 19:06:18,116 - INFO - Batch 6200, Loss: 4.1622, LR: 0.000099
2025-10-21 19:06:32,428 - INFO - Batch 6300, Loss: 4.2309, LR: 0.000099
2025-10-21 19:06:46,825 - INFO - Batch 6400, Loss: 4.0482, LR: 0.000099
2025-10-21 19:06:53,991 - INFO - Epoch 2/30: Train Loss: 4.4856, Val Loss: 4.2386, LR: 0.000099
2025-10-21 19:06:54,224 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 19:06:54,374 - INFO - Batch 0, Loss: 4.1681, LR: 0.000099
2025-10-21 19:07:08,642 - INFO - Batch 100, Loss: 4.2673, LR: 0.000099
2025-10-21 19:07:23,062 - INFO - Batch 200, Loss: 4.2057, LR: 0.000099
2025-10-21 19:07:37,561 - INFO - Batch 300, Loss: 4.2258, LR: 0.000099
2025-10-21 19:07:52,001 - INFO - Batch 400, Loss: 4.3188, LR: 0.000099
2025-10-21 19:08:06,323 - INFO - Batch 500, Loss: 4.1604, LR: 0.000099
2025-10-21 19:08:20,608 - INFO - Batch 600, Loss: 3.9922, LR: 0.000099
2025-10-21 19:08:34,860 - INFO - Batch 700, Loss: 4.2545, LR: 0.000099
2025-10-21 19:08:49,178 - INFO - Batch 800, Loss: 4.3196, LR: 0.000099
2025-10-21 19:09:03,526 - INFO - Batch 900, Loss: 4.1161, LR: 0.000099
2025-10-21 19:09:17,870 - INFO - Batch 1000, Loss: 4.1285, LR: 0.000099
2025-10-21 19:09:32,150 - INFO - Batch 1100, Loss: 4.2057, LR: 0.000099
2025-10-21 19:09:46,302 - INFO - Batch 1200, Loss: 4.1752, LR: 0.000099
2025-10-21 19:10:00,485 - INFO - Batch 1300, Loss: 4.0862, LR: 0.000099
2025-10-21 19:10:14,593 - INFO - Batch 1400, Loss: 4.2029, LR: 0.000099
2025-10-21 19:10:28,809 - INFO - Batch 1500, Loss: 4.2132, LR: 0.000099
2025-10-21 19:10:42,926 - INFO - Batch 1600, Loss: 4.2583, LR: 0.000099
2025-10-21 19:10:57,064 - INFO - Batch 1700, Loss: 4.0669, LR: 0.000099
2025-10-21 19:11:11,213 - INFO - Batch 1800, Loss: 4.0720, LR: 0.000099
2025-10-21 19:11:25,442 - INFO - Batch 1900, Loss: 3.8762, LR: 0.000099
2025-10-21 19:11:39,657 - INFO - Batch 2000, Loss: 4.2261, LR: 0.000099
2025-10-21 19:11:53,773 - INFO - Batch 2100, Loss: 4.1079, LR: 0.000099
2025-10-21 19:12:07,934 - INFO - Batch 2200, Loss: 4.0455, LR: 0.000099
2025-10-21 19:12:22,151 - INFO - Batch 2300, Loss: 4.1468, LR: 0.000099
2025-10-21 19:12:36,296 - INFO - Batch 2400, Loss: 4.1202, LR: 0.000099
2025-10-21 19:12:50,417 - INFO - Batch 2500, Loss: 4.1362, LR: 0.000099
2025-10-21 19:13:04,562 - INFO - Batch 2600, Loss: 4.0099, LR: 0.000099
2025-10-21 19:13:18,704 - INFO - Batch 2700, Loss: 4.1735, LR: 0.000099
2025-10-21 19:13:32,813 - INFO - Batch 2800, Loss: 3.9862, LR: 0.000099
2025-10-21 19:13:46,922 - INFO - Batch 2900, Loss: 4.3983, LR: 0.000099
2025-10-21 19:14:01,115 - INFO - Batch 3000, Loss: 4.0137, LR: 0.000099
2025-10-21 19:14:15,257 - INFO - Batch 3100, Loss: 4.1778, LR: 0.000099
2025-10-21 19:14:29,387 - INFO - Batch 3200, Loss: 4.0389, LR: 0.000099
2025-10-21 19:14:43,516 - INFO - Batch 3300, Loss: 3.9237, LR: 0.000099
2025-10-21 19:14:57,682 - INFO - Batch 3400, Loss: 3.9572, LR: 0.000099
2025-10-21 19:15:11,848 - INFO - Batch 3500, Loss: 4.1787, LR: 0.000099
2025-10-21 19:15:26,000 - INFO - Batch 3600, Loss: 4.0316, LR: 0.000099
2025-10-21 19:15:40,155 - INFO - Batch 3700, Loss: 3.7849, LR: 0.000099
2025-10-21 19:15:54,309 - INFO - Batch 3800, Loss: 4.0106, LR: 0.000099
2025-10-21 19:16:08,481 - INFO - Batch 3900, Loss: 4.0967, LR: 0.000099
2025-10-21 19:16:22,604 - INFO - Batch 4000, Loss: 4.1052, LR: 0.000099
2025-10-21 19:16:36,765 - INFO - Batch 4100, Loss: 4.0030, LR: 0.000098
2025-10-21 19:16:50,904 - INFO - Batch 4200, Loss: 4.0637, LR: 0.000098
2025-10-21 19:17:04,992 - INFO - Batch 4300, Loss: 4.2735, LR: 0.000098
2025-10-21 19:17:19,227 - INFO - Batch 4400, Loss: 4.1178, LR: 0.000098
2025-10-21 19:17:33,398 - INFO - Batch 4500, Loss: 4.0686, LR: 0.000098
2025-10-21 19:17:47,529 - INFO - Batch 4600, Loss: 3.9750, LR: 0.000098
2025-10-21 19:18:01,661 - INFO - Batch 4700, Loss: 4.0441, LR: 0.000098
2025-10-21 19:18:15,807 - INFO - Batch 4800, Loss: 3.8161, LR: 0.000098
2025-10-21 19:18:29,949 - INFO - Batch 4900, Loss: 3.9793, LR: 0.000098
2025-10-21 19:18:44,045 - INFO - Batch 5000, Loss: 3.8175, LR: 0.000098
2025-10-21 19:18:58,133 - INFO - Batch 5100, Loss: 4.1378, LR: 0.000098
2025-10-21 19:19:12,285 - INFO - Batch 5200, Loss: 3.7709, LR: 0.000098
2025-10-21 19:19:26,433 - INFO - Batch 5300, Loss: 3.7396, LR: 0.000098
2025-10-21 19:19:40,552 - INFO - Batch 5400, Loss: 3.9998, LR: 0.000098
2025-10-21 19:19:54,705 - INFO - Batch 5500, Loss: 4.0098, LR: 0.000098
2025-10-21 19:20:08,838 - INFO - Batch 5600, Loss: 4.1138, LR: 0.000098
2025-10-21 19:20:22,997 - INFO - Batch 5700, Loss: 3.7808, LR: 0.000098
2025-10-21 19:20:37,121 - INFO - Batch 5800, Loss: 3.8539, LR: 0.000098
2025-10-21 19:20:51,257 - INFO - Batch 5900, Loss: 3.9711, LR: 0.000098
2025-10-21 19:21:05,435 - INFO - Batch 6000, Loss: 3.5632, LR: 0.000098
2025-10-21 19:21:19,632 - INFO - Batch 6100, Loss: 3.8966, LR: 0.000098
2025-10-21 19:21:33,840 - INFO - Batch 6200, Loss: 3.7466, LR: 0.000098
2025-10-21 19:21:48,142 - INFO - Batch 6300, Loss: 3.8633, LR: 0.000098
2025-10-21 19:22:02,396 - INFO - Batch 6400, Loss: 3.7257, LR: 0.000098
2025-10-21 19:22:09,478 - INFO - Epoch 3/30: Train Loss: 4.0332, Val Loss: 3.7365, LR: 0.000098
2025-10-21 19:22:09,706 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 19:22:09,857 - INFO - Batch 0, Loss: 3.7215, LR: 0.000098
2025-10-21 19:22:24,192 - INFO - Batch 100, Loss: 3.4918, LR: 0.000098
2025-10-21 19:22:38,416 - INFO - Batch 200, Loss: 3.8992, LR: 0.000098
2025-10-21 19:22:52,661 - INFO - Batch 300, Loss: 3.7637, LR: 0.000098
2025-10-21 19:23:06,901 - INFO - Batch 400, Loss: 3.8901, LR: 0.000098
2025-10-21 19:23:21,158 - INFO - Batch 500, Loss: 3.6866, LR: 0.000098
2025-10-21 19:23:35,428 - INFO - Batch 600, Loss: 3.6637, LR: 0.000098
2025-10-21 19:23:49,636 - INFO - Batch 700, Loss: 3.5265, LR: 0.000098
2025-10-21 19:24:03,811 - INFO - Batch 800, Loss: 3.6224, LR: 0.000098
2025-10-21 19:24:18,031 - INFO - Batch 900, Loss: 3.7345, LR: 0.000098
2025-10-21 19:24:32,244 - INFO - Batch 1000, Loss: 3.7166, LR: 0.000098
2025-10-21 19:24:46,445 - INFO - Batch 1100, Loss: 3.8755, LR: 0.000098
2025-10-21 19:25:00,751 - INFO - Batch 1200, Loss: 3.6963, LR: 0.000098
2025-10-21 19:25:15,111 - INFO - Batch 1300, Loss: 3.5130, LR: 0.000098
2025-10-21 19:25:29,510 - INFO - Batch 1400, Loss: 3.6950, LR: 0.000098
2025-10-21 19:25:43,917 - INFO - Batch 1500, Loss: 3.4098, LR: 0.000098
2025-10-21 19:25:58,288 - INFO - Batch 1600, Loss: 3.7580, LR: 0.000098
2025-10-21 19:26:12,671 - INFO - Batch 1700, Loss: 3.6540, LR: 0.000098
2025-10-21 19:26:27,057 - INFO - Batch 1800, Loss: 3.5856, LR: 0.000098
2025-10-21 19:26:41,416 - INFO - Batch 1900, Loss: 3.6562, LR: 0.000098
2025-10-21 19:26:55,783 - INFO - Batch 2000, Loss: 3.5400, LR: 0.000098
2025-10-21 19:27:10,152 - INFO - Batch 2100, Loss: 3.4631, LR: 0.000097
2025-10-21 19:27:24,562 - INFO - Batch 2200, Loss: 3.6985, LR: 0.000097
2025-10-21 19:27:38,851 - INFO - Batch 2300, Loss: 3.1730, LR: 0.000097
2025-10-21 19:27:53,354 - INFO - Batch 2400, Loss: 3.4588, LR: 0.000097
2025-10-21 19:28:07,903 - INFO - Batch 2500, Loss: 3.3419, LR: 0.000097
2025-10-21 19:28:22,438 - INFO - Batch 2600, Loss: 3.3539, LR: 0.000097
2025-10-21 19:28:36,991 - INFO - Batch 2700, Loss: 3.7322, LR: 0.000097
2025-10-21 19:28:51,560 - INFO - Batch 2800, Loss: 3.6470, LR: 0.000097
2025-10-21 19:29:06,004 - INFO - Batch 2900, Loss: 3.4670, LR: 0.000097
2025-10-21 19:29:20,219 - INFO - Batch 3000, Loss: 3.3351, LR: 0.000097
2025-10-21 19:29:34,428 - INFO - Batch 3100, Loss: 3.4465, LR: 0.000097
2025-10-21 19:29:48,660 - INFO - Batch 3200, Loss: 3.5178, LR: 0.000097
2025-10-21 19:30:02,859 - INFO - Batch 3300, Loss: 3.4333, LR: 0.000097
2025-10-21 19:30:17,073 - INFO - Batch 3400, Loss: 3.6974, LR: 0.000097
2025-10-21 19:30:31,296 - INFO - Batch 3500, Loss: 3.5117, LR: 0.000097
2025-10-21 19:30:45,542 - INFO - Batch 3600, Loss: 3.4103, LR: 0.000097
2025-10-21 19:30:59,760 - INFO - Batch 3700, Loss: 3.4060, LR: 0.000097
2025-10-21 19:31:13,994 - INFO - Batch 3800, Loss: 3.3980, LR: 0.000097
2025-10-21 19:31:28,269 - INFO - Batch 3900, Loss: 3.4200, LR: 0.000097
2025-10-21 19:31:42,488 - INFO - Batch 4000, Loss: 3.4795, LR: 0.000097
2025-10-21 19:31:56,709 - INFO - Batch 4100, Loss: 3.6174, LR: 0.000097
2025-10-21 19:32:10,928 - INFO - Batch 4200, Loss: 3.4867, LR: 0.000097
2025-10-21 19:32:25,232 - INFO - Batch 4300, Loss: 3.6590, LR: 0.000097
2025-10-21 19:32:39,457 - INFO - Batch 4400, Loss: 3.3352, LR: 0.000097
2025-10-21 19:32:53,650 - INFO - Batch 4500, Loss: 3.2020, LR: 0.000097
2025-10-21 19:33:07,869 - INFO - Batch 4600, Loss: 3.3917, LR: 0.000097
2025-10-21 19:33:22,123 - INFO - Batch 4700, Loss: 3.5120, LR: 0.000097
2025-10-21 19:33:36,444 - INFO - Batch 4800, Loss: 3.2223, LR: 0.000097
2025-10-21 19:33:50,712 - INFO - Batch 4900, Loss: 3.4076, LR: 0.000097
2025-10-21 19:34:04,987 - INFO - Batch 5000, Loss: 3.3740, LR: 0.000097
2025-10-21 19:34:19,240 - INFO - Batch 5100, Loss: 3.3444, LR: 0.000097
2025-10-21 19:34:33,483 - INFO - Batch 5200, Loss: 3.2038, LR: 0.000097
2025-10-21 19:34:47,731 - INFO - Batch 5300, Loss: 3.2269, LR: 0.000097
2025-10-21 19:35:01,974 - INFO - Batch 5400, Loss: 3.4030, LR: 0.000097
2025-10-21 19:35:16,189 - INFO - Batch 5500, Loss: 3.3494, LR: 0.000097
2025-10-21 19:35:30,419 - INFO - Batch 5600, Loss: 3.2772, LR: 0.000096
2025-10-21 19:35:44,632 - INFO - Batch 5700, Loss: 3.4160, LR: 0.000096
2025-10-21 19:35:58,884 - INFO - Batch 5800, Loss: 3.4184, LR: 0.000096
2025-10-21 19:36:13,161 - INFO - Batch 5900, Loss: 3.4961, LR: 0.000096
2025-10-21 19:36:27,453 - INFO - Batch 6000, Loss: 3.1542, LR: 0.000096
2025-10-21 19:36:41,701 - INFO - Batch 6100, Loss: 3.2350, LR: 0.000096
2025-10-21 19:36:55,909 - INFO - Batch 6200, Loss: 3.4422, LR: 0.000096
2025-10-21 19:37:10,122 - INFO - Batch 6300, Loss: 3.3566, LR: 0.000096
2025-10-21 19:37:24,402 - INFO - Batch 6400, Loss: 3.1759, LR: 0.000096
2025-10-21 19:37:31,501 - INFO - Epoch 4/30: Train Loss: 3.5359, Val Loss: 3.3035, LR: 0.000096
2025-10-21 19:37:31,736 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 19:37:31,888 - INFO - Batch 0, Loss: 3.1603, LR: 0.000096
2025-10-21 19:37:46,137 - INFO - Batch 100, Loss: 3.3051, LR: 0.000096
2025-10-21 19:38:00,242 - INFO - Batch 200, Loss: 3.1824, LR: 0.000096
2025-10-21 19:38:14,191 - INFO - Batch 300, Loss: 3.2544, LR: 0.000096
2025-10-21 19:38:28,140 - INFO - Batch 400, Loss: 3.4330, LR: 0.000096
2025-10-21 19:38:42,094 - INFO - Batch 500, Loss: 3.0944, LR: 0.000096
2025-10-21 19:38:56,059 - INFO - Batch 600, Loss: 3.1687, LR: 0.000096
2025-10-21 19:39:10,015 - INFO - Batch 700, Loss: 3.1674, LR: 0.000096
2025-10-21 19:39:23,978 - INFO - Batch 800, Loss: 3.2712, LR: 0.000096
2025-10-21 19:39:37,956 - INFO - Batch 900, Loss: 3.2742, LR: 0.000096
2025-10-21 19:39:51,902 - INFO - Batch 1000, Loss: 3.1654, LR: 0.000096
2025-10-21 19:40:06,054 - INFO - Batch 1100, Loss: 3.3572, LR: 0.000096
2025-10-21 19:40:20,277 - INFO - Batch 1200, Loss: 3.0771, LR: 0.000096
2025-10-21 19:40:34,530 - INFO - Batch 1300, Loss: 3.0054, LR: 0.000096
2025-10-21 19:40:48,728 - INFO - Batch 1400, Loss: 3.4269, LR: 0.000096
2025-10-21 19:41:02,954 - INFO - Batch 1500, Loss: 3.1896, LR: 0.000096
2025-10-21 19:41:17,156 - INFO - Batch 1600, Loss: 3.3852, LR: 0.000096
2025-10-21 19:41:31,355 - INFO - Batch 1700, Loss: 3.2164, LR: 0.000096
2025-10-21 19:41:45,859 - INFO - Batch 1800, Loss: 3.1604, LR: 0.000096
2025-10-21 19:42:00,389 - INFO - Batch 1900, Loss: 3.3030, LR: 0.000096
2025-10-21 19:42:14,793 - INFO - Batch 2000, Loss: 3.4310, LR: 0.000096
2025-10-21 19:42:29,239 - INFO - Batch 2100, Loss: 3.1105, LR: 0.000096
2025-10-21 19:42:43,637 - INFO - Batch 2200, Loss: 3.2411, LR: 0.000096
2025-10-21 19:42:58,034 - INFO - Batch 2300, Loss: 3.4755, LR: 0.000095
2025-10-21 19:43:12,399 - INFO - Batch 2400, Loss: 3.0756, LR: 0.000095
2025-10-21 19:43:26,850 - INFO - Batch 2500, Loss: 3.4010, LR: 0.000095
2025-10-21 19:43:41,483 - INFO - Batch 2600, Loss: 3.1426, LR: 0.000095
2025-10-21 19:43:56,113 - INFO - Batch 2700, Loss: 3.4239, LR: 0.000095
2025-10-21 19:44:10,570 - INFO - Batch 2800, Loss: 3.1609, LR: 0.000095
2025-10-21 19:44:25,012 - INFO - Batch 2900, Loss: 3.2029, LR: 0.000095
2025-10-21 19:44:39,314 - INFO - Batch 3000, Loss: 3.2346, LR: 0.000095
2025-10-21 19:44:53,664 - INFO - Batch 3100, Loss: 3.3511, LR: 0.000095
2025-10-21 19:45:08,038 - INFO - Batch 3200, Loss: 3.1967, LR: 0.000095
2025-10-21 19:45:22,387 - INFO - Batch 3300, Loss: 3.2555, LR: 0.000095
2025-10-21 19:45:36,773 - INFO - Batch 3400, Loss: 3.3258, LR: 0.000095
2025-10-21 19:45:51,102 - INFO - Batch 3500, Loss: 3.5776, LR: 0.000095
2025-10-21 19:46:05,546 - INFO - Batch 3600, Loss: 3.1209, LR: 0.000095
2025-10-21 19:46:19,892 - INFO - Batch 3700, Loss: 3.2080, LR: 0.000095
2025-10-21 19:46:34,247 - INFO - Batch 3800, Loss: 3.1207, LR: 0.000095
2025-10-21 19:46:48,605 - INFO - Batch 3900, Loss: 2.9626, LR: 0.000095
2025-10-21 19:47:03,140 - INFO - Batch 4000, Loss: 3.2568, LR: 0.000095
2025-10-21 19:47:17,641 - INFO - Batch 4100, Loss: 3.2367, LR: 0.000095
2025-10-21 19:47:32,198 - INFO - Batch 4200, Loss: 3.0790, LR: 0.000095
2025-10-21 19:47:46,692 - INFO - Batch 4300, Loss: 3.1479, LR: 0.000095
2025-10-21 19:48:01,024 - INFO - Batch 4400, Loss: 3.0657, LR: 0.000095
2025-10-21 19:48:15,414 - INFO - Batch 4500, Loss: 2.9536, LR: 0.000095
2025-10-21 19:48:30,036 - INFO - Batch 4600, Loss: 3.0992, LR: 0.000095
2025-10-21 19:48:44,570 - INFO - Batch 4700, Loss: 3.0985, LR: 0.000095
2025-10-21 19:48:58,904 - INFO - Batch 4800, Loss: 3.1715, LR: 0.000095
2025-10-21 19:49:13,194 - INFO - Batch 4900, Loss: 3.2544, LR: 0.000095
2025-10-21 19:49:27,515 - INFO - Batch 5000, Loss: 3.1959, LR: 0.000095
2025-10-21 19:49:42,054 - INFO - Batch 5100, Loss: 2.9820, LR: 0.000094
2025-10-21 19:49:56,346 - INFO - Batch 5200, Loss: 3.0854, LR: 0.000094
2025-10-21 19:50:10,589 - INFO - Batch 5300, Loss: 2.9062, LR: 0.000094
2025-10-21 19:50:24,855 - INFO - Batch 5400, Loss: 3.1238, LR: 0.000094
2025-10-21 19:50:39,096 - INFO - Batch 5500, Loss: 3.1143, LR: 0.000094
2025-10-21 19:50:53,335 - INFO - Batch 5600, Loss: 3.0904, LR: 0.000094
2025-10-21 19:51:07,608 - INFO - Batch 5700, Loss: 3.0460, LR: 0.000094
2025-10-21 19:51:21,859 - INFO - Batch 5800, Loss: 3.0454, LR: 0.000094
2025-10-21 19:51:36,114 - INFO - Batch 5900, Loss: 3.0675, LR: 0.000094
2025-10-21 19:51:50,397 - INFO - Batch 6000, Loss: 3.0467, LR: 0.000094
2025-10-21 19:52:04,675 - INFO - Batch 6100, Loss: 3.1974, LR: 0.000094
2025-10-21 19:52:19,069 - INFO - Batch 6200, Loss: 3.2227, LR: 0.000094
2025-10-21 19:52:33,377 - INFO - Batch 6300, Loss: 2.9490, LR: 0.000094
2025-10-21 19:52:47,667 - INFO - Batch 6400, Loss: 2.8820, LR: 0.000094
2025-10-21 19:52:54,794 - INFO - Epoch 5/30: Train Loss: 3.1888, Val Loss: 2.9690, LR: 0.000094
2025-10-21 19:52:55,047 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 19:52:55,197 - INFO - Batch 0, Loss: 2.8579, LR: 0.000094
2025-10-21 19:53:09,461 - INFO - Batch 100, Loss: 3.0869, LR: 0.000094
2025-10-21 19:53:23,701 - INFO - Batch 200, Loss: 3.1424, LR: 0.000094
2025-10-21 19:53:37,949 - INFO - Batch 300, Loss: 3.0744, LR: 0.000094
2025-10-21 19:53:52,339 - INFO - Batch 400, Loss: 2.9490, LR: 0.000094
2025-10-21 19:54:06,910 - INFO - Batch 500, Loss: 2.9715, LR: 0.000094
2025-10-21 19:54:21,400 - INFO - Batch 600, Loss: 2.7058, LR: 0.000094
2025-10-21 19:54:35,649 - INFO - Batch 700, Loss: 2.8606, LR: 0.000094
2025-10-21 19:54:49,975 - INFO - Batch 800, Loss: 3.0882, LR: 0.000094
2025-10-21 19:55:04,284 - INFO - Batch 900, Loss: 3.0478, LR: 0.000094
2025-10-21 19:55:18,600 - INFO - Batch 1000, Loss: 2.8198, LR: 0.000094
2025-10-21 19:55:32,931 - INFO - Batch 1100, Loss: 3.0073, LR: 0.000094
2025-10-21 19:55:47,221 - INFO - Batch 1200, Loss: 2.9644, LR: 0.000093
2025-10-21 19:56:01,513 - INFO - Batch 1300, Loss: 3.1355, LR: 0.000093
2025-10-21 19:56:15,822 - INFO - Batch 1400, Loss: 2.9376, LR: 0.000093
2025-10-21 19:56:30,175 - INFO - Batch 1500, Loss: 3.0083, LR: 0.000093
2025-10-21 19:56:44,508 - INFO - Batch 1600, Loss: 2.9488, LR: 0.000093
2025-10-21 19:56:58,836 - INFO - Batch 1700, Loss: 2.7307, LR: 0.000093
2025-10-21 19:57:13,141 - INFO - Batch 1800, Loss: 3.0287, LR: 0.000093
2025-10-21 19:57:27,617 - INFO - Batch 1900, Loss: 2.8213, LR: 0.000093
2025-10-21 19:57:42,090 - INFO - Batch 2000, Loss: 3.1415, LR: 0.000093
2025-10-21 19:57:56,593 - INFO - Batch 2100, Loss: 2.8838, LR: 0.000093
2025-10-21 19:58:11,086 - INFO - Batch 2200, Loss: 2.6759, LR: 0.000093
2025-10-21 19:58:25,581 - INFO - Batch 2300, Loss: 2.9732, LR: 0.000093
2025-10-21 19:58:40,100 - INFO - Batch 2400, Loss: 3.0081, LR: 0.000093
2025-10-21 19:58:54,552 - INFO - Batch 2500, Loss: 3.0185, LR: 0.000093
2025-10-21 19:59:09,047 - INFO - Batch 2600, Loss: 2.8753, LR: 0.000093
2025-10-21 19:59:23,538 - INFO - Batch 2700, Loss: 2.9698, LR: 0.000093
2025-10-21 19:59:38,084 - INFO - Batch 2800, Loss: 2.9671, LR: 0.000093
2025-10-21 19:59:52,422 - INFO - Batch 2900, Loss: 2.8594, LR: 0.000093
2025-10-21 20:00:06,860 - INFO - Batch 3000, Loss: 3.1065, LR: 0.000093
2025-10-21 20:00:21,200 - INFO - Batch 3100, Loss: 3.0737, LR: 0.000093
2025-10-21 20:00:35,524 - INFO - Batch 3200, Loss: 3.1389, LR: 0.000093
2025-10-21 20:00:49,890 - INFO - Batch 3300, Loss: 2.5291, LR: 0.000093
2025-10-21 20:01:04,228 - INFO - Batch 3400, Loss: 2.6592, LR: 0.000093
2025-10-21 20:01:18,556 - INFO - Batch 3500, Loss: 2.6804, LR: 0.000093
2025-10-21 20:01:32,874 - INFO - Batch 3600, Loss: 3.1663, LR: 0.000092
2025-10-21 20:01:47,176 - INFO - Batch 3700, Loss: 2.6594, LR: 0.000092
2025-10-21 20:02:01,565 - INFO - Batch 3800, Loss: 2.8611, LR: 0.000092
2025-10-21 20:02:16,057 - INFO - Batch 3900, Loss: 3.2908, LR: 0.000092
2025-10-21 20:02:30,506 - INFO - Batch 4000, Loss: 3.1050, LR: 0.000092
2025-10-21 20:02:44,899 - INFO - Batch 4100, Loss: 3.0294, LR: 0.000092
2025-10-21 20:02:59,356 - INFO - Batch 4200, Loss: 2.8263, LR: 0.000092
2025-10-21 20:03:13,940 - INFO - Batch 4300, Loss: 3.0003, LR: 0.000092
2025-10-21 20:03:28,312 - INFO - Batch 4400, Loss: 2.8211, LR: 0.000092
2025-10-21 20:03:42,875 - INFO - Batch 4500, Loss: 2.9155, LR: 0.000092
2025-10-21 20:03:57,190 - INFO - Batch 4600, Loss: 3.0354, LR: 0.000092
2025-10-21 20:04:11,538 - INFO - Batch 4700, Loss: 2.8689, LR: 0.000092
2025-10-21 20:04:26,135 - INFO - Batch 4800, Loss: 3.0813, LR: 0.000092
2025-10-21 20:04:40,475 - INFO - Batch 4900, Loss: 2.9811, LR: 0.000092
2025-10-21 20:04:54,775 - INFO - Batch 5000, Loss: 2.9402, LR: 0.000092
2025-10-21 20:05:09,076 - INFO - Batch 5100, Loss: 2.8669, LR: 0.000092
2025-10-21 20:05:23,388 - INFO - Batch 5200, Loss: 2.7181, LR: 0.000092
2025-10-21 20:05:37,704 - INFO - Batch 5300, Loss: 2.8100, LR: 0.000092
2025-10-21 20:05:52,006 - INFO - Batch 5400, Loss: 3.0896, LR: 0.000092
2025-10-21 20:06:06,335 - INFO - Batch 5500, Loss: 2.9582, LR: 0.000092
2025-10-21 20:06:20,696 - INFO - Batch 5600, Loss: 2.6160, LR: 0.000092
2025-10-21 20:06:35,195 - INFO - Batch 5700, Loss: 2.8666, LR: 0.000092
2025-10-21 20:06:49,662 - INFO - Batch 5800, Loss: 2.8004, LR: 0.000092
2025-10-21 20:07:03,994 - INFO - Batch 5900, Loss: 2.7429, LR: 0.000091
2025-10-21 20:07:18,301 - INFO - Batch 6000, Loss: 3.0092, LR: 0.000091
2025-10-21 20:07:32,647 - INFO - Batch 6100, Loss: 2.9554, LR: 0.000091
2025-10-21 20:07:46,970 - INFO - Batch 6200, Loss: 2.8024, LR: 0.000091
2025-10-21 20:08:01,310 - INFO - Batch 6300, Loss: 2.6265, LR: 0.000091
2025-10-21 20:08:15,880 - INFO - Batch 6400, Loss: 2.6101, LR: 0.000091
2025-10-21 20:08:23,134 - INFO - Epoch 6/30: Train Loss: 2.9070, Val Loss: 2.7184, LR: 0.000091
2025-10-21 20:08:23,379 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 20:08:23,530 - INFO - Batch 0, Loss: 2.5769, LR: 0.000091
2025-10-21 20:08:38,106 - INFO - Batch 100, Loss: 2.8048, LR: 0.000091
2025-10-21 20:08:52,656 - INFO - Batch 200, Loss: 2.5750, LR: 0.000091
2025-10-21 20:09:07,230 - INFO - Batch 300, Loss: 2.2115, LR: 0.000091
2025-10-21 20:09:21,657 - INFO - Batch 400, Loss: 2.9020, LR: 0.000091
2025-10-21 20:09:36,002 - INFO - Batch 500, Loss: 2.8986, LR: 0.000091
2025-10-21 20:09:50,280 - INFO - Batch 600, Loss: 2.6083, LR: 0.000091
2025-10-21 20:10:04,573 - INFO - Batch 700, Loss: 2.5944, LR: 0.000091
2025-10-21 20:10:18,820 - INFO - Batch 800, Loss: 2.5621, LR: 0.000091
2025-10-21 20:10:33,022 - INFO - Batch 900, Loss: 2.5921, LR: 0.000091
2025-10-21 20:10:47,192 - INFO - Batch 1000, Loss: 2.3839, LR: 0.000091
2025-10-21 20:11:01,340 - INFO - Batch 1100, Loss: 3.0494, LR: 0.000091
2025-10-21 20:11:15,591 - INFO - Batch 1200, Loss: 2.5680, LR: 0.000091
2025-10-21 20:11:29,765 - INFO - Batch 1300, Loss: 2.7091, LR: 0.000091
2025-10-21 20:11:43,943 - INFO - Batch 1400, Loss: 2.6372, LR: 0.000091
2025-10-21 20:11:58,105 - INFO - Batch 1500, Loss: 2.5233, LR: 0.000090
2025-10-21 20:12:12,266 - INFO - Batch 1600, Loss: 2.5872, LR: 0.000090
2025-10-21 20:12:26,420 - INFO - Batch 1700, Loss: 2.7415, LR: 0.000090
2025-10-21 20:12:40,684 - INFO - Batch 1800, Loss: 2.7777, LR: 0.000090
2025-10-21 20:12:54,909 - INFO - Batch 1900, Loss: 2.6076, LR: 0.000090
2025-10-21 20:13:09,133 - INFO - Batch 2000, Loss: 2.8637, LR: 0.000090
2025-10-21 20:13:23,342 - INFO - Batch 2100, Loss: 2.6383, LR: 0.000090
2025-10-21 20:13:37,640 - INFO - Batch 2200, Loss: 2.7465, LR: 0.000090
2025-10-21 20:13:51,850 - INFO - Batch 2300, Loss: 3.0418, LR: 0.000090
2025-10-21 20:14:06,044 - INFO - Batch 2400, Loss: 2.7847, LR: 0.000090
2025-10-21 20:14:20,176 - INFO - Batch 2500, Loss: 3.0988, LR: 0.000090
2025-10-21 20:14:34,280 - INFO - Batch 2600, Loss: 2.5574, LR: 0.000090
2025-10-21 20:14:48,418 - INFO - Batch 2700, Loss: 2.7813, LR: 0.000090
2025-10-21 20:15:02,566 - INFO - Batch 2800, Loss: 2.7809, LR: 0.000090
2025-10-21 20:15:16,722 - INFO - Batch 2900, Loss: 2.6391, LR: 0.000090
2025-10-21 20:15:30,849 - INFO - Batch 3000, Loss: 3.0598, LR: 0.000090
2025-10-21 20:15:45,021 - INFO - Batch 3100, Loss: 2.9739, LR: 0.000090
2025-10-21 20:15:59,194 - INFO - Batch 3200, Loss: 2.6218, LR: 0.000090
2025-10-21 20:16:13,423 - INFO - Batch 3300, Loss: 2.9171, LR: 0.000090
2025-10-21 20:16:27,718 - INFO - Batch 3400, Loss: 2.6109, LR: 0.000090
2025-10-21 20:16:41,957 - INFO - Batch 3500, Loss: 2.5500, LR: 0.000090
2025-10-21 20:16:56,197 - INFO - Batch 3600, Loss: 2.6825, LR: 0.000089
2025-10-21 20:17:10,442 - INFO - Batch 3700, Loss: 2.8087, LR: 0.000089
2025-10-21 20:17:24,810 - INFO - Batch 3800, Loss: 2.8074, LR: 0.000089
2025-10-21 20:17:39,129 - INFO - Batch 3900, Loss: 3.0196, LR: 0.000089
2025-10-21 20:17:53,318 - INFO - Batch 4000, Loss: 2.3811, LR: 0.000089
2025-10-21 20:18:07,523 - INFO - Batch 4100, Loss: 2.4906, LR: 0.000089
2025-10-21 20:18:21,732 - INFO - Batch 4200, Loss: 2.5862, LR: 0.000089
2025-10-21 20:18:36,030 - INFO - Batch 4300, Loss: 3.0205, LR: 0.000089
2025-10-21 20:18:50,295 - INFO - Batch 4400, Loss: 2.7090, LR: 0.000089
2025-10-21 20:19:04,526 - INFO - Batch 4500, Loss: 2.5325, LR: 0.000089
2025-10-21 20:19:18,742 - INFO - Batch 4600, Loss: 2.6954, LR: 0.000089
2025-10-21 20:19:32,920 - INFO - Batch 4700, Loss: 2.4098, LR: 0.000089
2025-10-21 20:19:47,092 - INFO - Batch 4800, Loss: 2.7033, LR: 0.000089
2025-10-21 20:20:01,258 - INFO - Batch 4900, Loss: 2.7096, LR: 0.000089
2025-10-21 20:20:15,402 - INFO - Batch 5000, Loss: 2.3688, LR: 0.000089
2025-10-21 20:20:29,561 - INFO - Batch 5100, Loss: 2.8941, LR: 0.000089
2025-10-21 20:20:43,763 - INFO - Batch 5200, Loss: 2.6551, LR: 0.000089
2025-10-21 20:20:58,019 - INFO - Batch 5300, Loss: 2.9507, LR: 0.000089
2025-10-21 20:21:12,534 - INFO - Batch 5400, Loss: 2.4723, LR: 0.000089
2025-10-21 20:21:26,977 - INFO - Batch 5500, Loss: 2.2986, LR: 0.000088
2025-10-21 20:21:41,155 - INFO - Batch 5600, Loss: 2.8629, LR: 0.000088
2025-10-21 20:21:55,405 - INFO - Batch 5700, Loss: 2.9475, LR: 0.000088
2025-10-21 20:22:09,749 - INFO - Batch 5800, Loss: 2.5585, LR: 0.000088
2025-10-21 20:22:24,064 - INFO - Batch 5900, Loss: 2.5483, LR: 0.000088
2025-10-21 20:22:38,193 - INFO - Batch 6000, Loss: 2.6193, LR: 0.000088
2025-10-21 20:22:52,352 - INFO - Batch 6100, Loss: 2.7143, LR: 0.000088
2025-10-21 20:23:06,508 - INFO - Batch 6200, Loss: 2.6273, LR: 0.000088
2025-10-21 20:23:20,641 - INFO - Batch 6300, Loss: 2.7184, LR: 0.000088
2025-10-21 20:23:34,802 - INFO - Batch 6400, Loss: 2.7788, LR: 0.000088
2025-10-21 20:23:41,876 - INFO - Epoch 7/30: Train Loss: 2.7214, Val Loss: 2.5631, LR: 0.000088
2025-10-21 20:23:42,125 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 20:23:42,273 - INFO - Batch 0, Loss: 2.7289, LR: 0.000088
2025-10-21 20:23:56,867 - INFO - Batch 100, Loss: 2.5138, LR: 0.000088
2025-10-21 20:24:11,415 - INFO - Batch 200, Loss: 2.3577, LR: 0.000088
2025-10-21 20:24:25,943 - INFO - Batch 300, Loss: 2.8508, LR: 0.000088
2025-10-21 20:24:40,161 - INFO - Batch 400, Loss: 2.7534, LR: 0.000088
2025-10-21 20:24:54,342 - INFO - Batch 500, Loss: 2.7030, LR: 0.000088
2025-10-21 20:25:08,544 - INFO - Batch 600, Loss: 2.5033, LR: 0.000088
2025-10-21 20:25:22,757 - INFO - Batch 700, Loss: 2.4760, LR: 0.000088
2025-10-21 20:25:36,952 - INFO - Batch 800, Loss: 2.8280, LR: 0.000088
2025-10-21 20:25:51,152 - INFO - Batch 900, Loss: 2.7437, LR: 0.000088
2025-10-21 20:26:05,368 - INFO - Batch 1000, Loss: 2.6222, LR: 0.000087
2025-10-21 20:26:19,582 - INFO - Batch 1100, Loss: 2.7685, LR: 0.000087
2025-10-21 20:26:33,762 - INFO - Batch 1200, Loss: 2.5603, LR: 0.000087
2025-10-21 20:26:47,948 - INFO - Batch 1300, Loss: 2.8407, LR: 0.000087
2025-10-21 20:27:02,148 - INFO - Batch 1400, Loss: 2.6418, LR: 0.000087
2025-10-21 20:27:16,390 - INFO - Batch 1500, Loss: 2.6879, LR: 0.000087
2025-10-21 20:27:30,630 - INFO - Batch 1600, Loss: 2.5449, LR: 0.000087
2025-10-21 20:27:44,809 - INFO - Batch 1700, Loss: 2.4504, LR: 0.000087
2025-10-21 20:27:59,019 - INFO - Batch 1800, Loss: 2.6176, LR: 0.000087
2025-10-21 20:28:13,277 - INFO - Batch 1900, Loss: 2.3709, LR: 0.000087
2025-10-21 20:28:27,442 - INFO - Batch 2000, Loss: 2.4727, LR: 0.000087
2025-10-21 20:28:41,615 - INFO - Batch 2100, Loss: 2.3705, LR: 0.000087
2025-10-21 20:28:55,770 - INFO - Batch 2200, Loss: 2.4456, LR: 0.000087
2025-10-21 20:29:09,937 - INFO - Batch 2300, Loss: 2.6158, LR: 0.000087
2025-10-21 20:29:24,073 - INFO - Batch 2400, Loss: 2.2854, LR: 0.000087
2025-10-21 20:29:38,216 - INFO - Batch 2500, Loss: 2.5667, LR: 0.000087
2025-10-21 20:29:52,337 - INFO - Batch 2600, Loss: 2.6182, LR: 0.000087
2025-10-21 20:30:06,501 - INFO - Batch 2700, Loss: 2.5093, LR: 0.000087
2025-10-21 20:30:20,678 - INFO - Batch 2800, Loss: 2.3579, LR: 0.000086
2025-10-21 20:30:34,912 - INFO - Batch 2900, Loss: 2.5884, LR: 0.000086
2025-10-21 20:30:49,114 - INFO - Batch 3000, Loss: 2.3088, LR: 0.000086
2025-10-21 20:31:03,374 - INFO - Batch 3100, Loss: 2.7240, LR: 0.000086
2025-10-21 20:31:17,590 - INFO - Batch 3200, Loss: 2.6060, LR: 0.000086
2025-10-21 20:31:31,830 - INFO - Batch 3300, Loss: 2.6643, LR: 0.000086
2025-10-21 20:31:46,142 - INFO - Batch 3400, Loss: 2.8391, LR: 0.000086
2025-10-21 20:32:00,280 - INFO - Batch 3500, Loss: 2.4934, LR: 0.000086
2025-10-21 20:32:14,439 - INFO - Batch 3600, Loss: 2.7595, LR: 0.000086
2025-10-21 20:32:28,589 - INFO - Batch 3700, Loss: 2.7348, LR: 0.000086
2025-10-21 20:32:42,739 - INFO - Batch 3800, Loss: 2.3085, LR: 0.000086
2025-10-21 20:32:56,887 - INFO - Batch 3900, Loss: 2.7857, LR: 0.000086
2025-10-21 20:33:11,031 - INFO - Batch 4000, Loss: 2.5702, LR: 0.000086
2025-10-21 20:33:25,189 - INFO - Batch 4100, Loss: 2.4483, LR: 0.000086
2025-10-21 20:33:39,365 - INFO - Batch 4200, Loss: 2.7705, LR: 0.000086
2025-10-21 20:33:53,508 - INFO - Batch 4300, Loss: 2.5033, LR: 0.000086
2025-10-21 20:34:07,692 - INFO - Batch 4400, Loss: 2.4196, LR: 0.000086
2025-10-21 20:34:21,822 - INFO - Batch 4500, Loss: 2.6471, LR: 0.000085
2025-10-21 20:34:35,950 - INFO - Batch 4600, Loss: 2.4420, LR: 0.000085
2025-10-21 20:34:50,103 - INFO - Batch 4700, Loss: 2.6839, LR: 0.000085
2025-10-21 20:35:04,241 - INFO - Batch 4800, Loss: 2.7691, LR: 0.000085
2025-10-21 20:35:18,396 - INFO - Batch 4900, Loss: 2.4320, LR: 0.000085
2025-10-21 20:35:32,530 - INFO - Batch 5000, Loss: 2.7475, LR: 0.000085
2025-10-21 20:35:46,679 - INFO - Batch 5100, Loss: 2.7580, LR: 0.000085
2025-10-21 20:36:00,838 - INFO - Batch 5200, Loss: 2.6147, LR: 0.000085
2025-10-21 20:36:14,981 - INFO - Batch 5300, Loss: 2.6817, LR: 0.000085
2025-10-21 20:36:29,108 - INFO - Batch 5400, Loss: 2.3483, LR: 0.000085
2025-10-21 20:36:43,298 - INFO - Batch 5500, Loss: 2.4573, LR: 0.000085
2025-10-21 20:36:57,615 - INFO - Batch 5600, Loss: 2.4687, LR: 0.000085
2025-10-21 20:37:11,992 - INFO - Batch 5700, Loss: 2.6057, LR: 0.000085
2025-10-21 20:37:26,267 - INFO - Batch 5800, Loss: 2.8028, LR: 0.000085
2025-10-21 20:37:40,420 - INFO - Batch 5900, Loss: 2.7691, LR: 0.000085
2025-10-21 20:37:54,600 - INFO - Batch 6000, Loss: 2.1372, LR: 0.000085
2025-10-21 20:38:08,737 - INFO - Batch 6100, Loss: 2.7245, LR: 0.000085
2025-10-21 20:38:22,879 - INFO - Batch 6200, Loss: 2.3081, LR: 0.000084
2025-10-21 20:38:37,041 - INFO - Batch 6300, Loss: 2.6591, LR: 0.000084
2025-10-21 20:38:51,229 - INFO - Batch 6400, Loss: 2.5765, LR: 0.000084
2025-10-21 20:38:58,297 - INFO - Epoch 8/30: Train Loss: 2.5995, Val Loss: 2.4813, LR: 0.000084
2025-10-21 20:38:58,543 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 20:38:58,692 - INFO - Batch 0, Loss: 2.4375, LR: 0.000084
2025-10-21 20:39:12,946 - INFO - Batch 100, Loss: 2.6345, LR: 0.000084
2025-10-21 20:39:27,262 - INFO - Batch 200, Loss: 2.9228, LR: 0.000084
2025-10-21 20:39:41,512 - INFO - Batch 300, Loss: 2.5584, LR: 0.000084
2025-10-21 20:39:55,776 - INFO - Batch 400, Loss: 2.3646, LR: 0.000084
2025-10-21 20:40:10,033 - INFO - Batch 500, Loss: 2.3474, LR: 0.000084
2025-10-21 20:40:24,277 - INFO - Batch 600, Loss: 2.6749, LR: 0.000084
2025-10-21 20:40:38,572 - INFO - Batch 700, Loss: 2.5261, LR: 0.000084
2025-10-21 20:40:52,826 - INFO - Batch 800, Loss: 2.4178, LR: 0.000084
2025-10-21 20:41:07,165 - INFO - Batch 900, Loss: 2.3697, LR: 0.000084
2025-10-21 20:41:21,533 - INFO - Batch 1000, Loss: 2.2327, LR: 0.000084
2025-10-21 20:41:35,721 - INFO - Batch 1100, Loss: 2.5181, LR: 0.000084
2025-10-21 20:41:49,861 - INFO - Batch 1200, Loss: 2.6546, LR: 0.000084
2025-10-21 20:42:04,049 - INFO - Batch 1300, Loss: 2.6848, LR: 0.000084
2025-10-21 20:42:18,294 - INFO - Batch 1400, Loss: 2.2345, LR: 0.000083
2025-10-21 20:42:32,521 - INFO - Batch 1500, Loss: 2.6094, LR: 0.000083
2025-10-21 20:42:46,701 - INFO - Batch 1600, Loss: 2.7194, LR: 0.000083
2025-10-21 20:43:00,933 - INFO - Batch 1700, Loss: 2.5642, LR: 0.000083
2025-10-21 20:43:15,161 - INFO - Batch 1800, Loss: 2.3980, LR: 0.000083
2025-10-21 20:43:29,416 - INFO - Batch 1900, Loss: 2.6275, LR: 0.000083
2025-10-21 20:43:43,696 - INFO - Batch 2000, Loss: 2.6864, LR: 0.000083
2025-10-21 20:43:57,924 - INFO - Batch 2100, Loss: 2.4278, LR: 0.000083
2025-10-21 20:44:12,125 - INFO - Batch 2200, Loss: 2.4942, LR: 0.000083
2025-10-21 20:44:26,303 - INFO - Batch 2300, Loss: 2.3887, LR: 0.000083
2025-10-21 20:44:40,556 - INFO - Batch 2400, Loss: 2.6123, LR: 0.000083
2025-10-21 20:44:54,756 - INFO - Batch 2500, Loss: 2.5717, LR: 0.000083
2025-10-21 20:45:08,938 - INFO - Batch 2600, Loss: 2.4879, LR: 0.000083
2025-10-21 20:45:23,135 - INFO - Batch 2700, Loss: 2.5231, LR: 0.000083
2025-10-21 20:45:37,328 - INFO - Batch 2800, Loss: 2.3903, LR: 0.000083
2025-10-21 20:45:51,496 - INFO - Batch 2900, Loss: 2.5132, LR: 0.000083
2025-10-21 20:46:05,696 - INFO - Batch 3000, Loss: 2.4802, LR: 0.000083
2025-10-21 20:46:19,857 - INFO - Batch 3100, Loss: 2.3384, LR: 0.000082
2025-10-21 20:46:34,142 - INFO - Batch 3200, Loss: 2.6879, LR: 0.000082
2025-10-21 20:46:48,761 - INFO - Batch 3300, Loss: 2.5992, LR: 0.000082
2025-10-21 20:47:03,355 - INFO - Batch 3400, Loss: 2.3803, LR: 0.000082
2025-10-21 20:47:17,849 - INFO - Batch 3500, Loss: 2.6139, LR: 0.000082
2025-10-21 20:47:32,046 - INFO - Batch 3600, Loss: 2.4914, LR: 0.000082
2025-10-21 20:47:46,240 - INFO - Batch 3700, Loss: 2.6627, LR: 0.000082
2025-10-21 20:48:00,412 - INFO - Batch 3800, Loss: 2.6754, LR: 0.000082
2025-10-21 20:48:14,590 - INFO - Batch 3900, Loss: 2.3612, LR: 0.000082
2025-10-21 20:48:28,923 - INFO - Batch 4000, Loss: 2.6921, LR: 0.000082
2025-10-21 20:48:43,245 - INFO - Batch 4100, Loss: 2.5200, LR: 0.000082
2025-10-21 20:48:57,420 - INFO - Batch 4200, Loss: 2.4002, LR: 0.000082
2025-10-21 20:49:11,729 - INFO - Batch 4300, Loss: 2.2826, LR: 0.000082
2025-10-21 20:49:25,990 - INFO - Batch 4400, Loss: 2.5958, LR: 0.000082
2025-10-21 20:49:40,263 - INFO - Batch 4500, Loss: 2.5101, LR: 0.000082
2025-10-21 20:49:54,503 - INFO - Batch 4600, Loss: 2.8052, LR: 0.000081
2025-10-21 20:50:08,694 - INFO - Batch 4700, Loss: 2.8723, LR: 0.000081
2025-10-21 20:50:22,883 - INFO - Batch 4800, Loss: 2.5453, LR: 0.000081
2025-10-21 20:50:37,062 - INFO - Batch 4900, Loss: 2.2977, LR: 0.000081
2025-10-21 20:50:51,228 - INFO - Batch 5000, Loss: 2.7753, LR: 0.000081
2025-10-21 20:51:05,377 - INFO - Batch 5100, Loss: 2.2659, LR: 0.000081
2025-10-21 20:51:19,578 - INFO - Batch 5200, Loss: 2.1813, LR: 0.000081
2025-10-21 20:51:33,949 - INFO - Batch 5300, Loss: 2.3646, LR: 0.000081
2025-10-21 20:51:48,162 - INFO - Batch 5400, Loss: 2.5458, LR: 0.000081
2025-10-21 20:52:02,323 - INFO - Batch 5500, Loss: 2.6620, LR: 0.000081
2025-10-21 20:52:16,549 - INFO - Batch 5600, Loss: 2.6611, LR: 0.000081
2025-10-21 20:52:30,776 - INFO - Batch 5700, Loss: 2.6493, LR: 0.000081
2025-10-21 20:52:44,978 - INFO - Batch 5800, Loss: 2.6606, LR: 0.000081
2025-10-21 20:52:59,389 - INFO - Batch 5900, Loss: 2.6608, LR: 0.000081
2025-10-21 20:53:13,840 - INFO - Batch 6000, Loss: 2.3415, LR: 0.000081
2025-10-21 20:53:28,281 - INFO - Batch 6100, Loss: 2.2873, LR: 0.000081
2025-10-21 20:53:42,709 - INFO - Batch 6200, Loss: 2.5943, LR: 0.000080
2025-10-21 20:53:57,133 - INFO - Batch 6300, Loss: 2.4825, LR: 0.000080
2025-10-21 20:54:11,317 - INFO - Batch 6400, Loss: 2.4834, LR: 0.000080
2025-10-21 20:54:18,395 - INFO - Epoch 9/30: Train Loss: 2.5144, Val Loss: 2.3951, LR: 0.000080
2025-10-21 20:54:18,645 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 20:54:18,793 - INFO - Batch 0, Loss: 2.4908, LR: 0.000080
2025-10-21 20:54:32,888 - INFO - Batch 100, Loss: 2.4661, LR: 0.000080
2025-10-21 20:54:46,986 - INFO - Batch 200, Loss: 2.4781, LR: 0.000080
2025-10-21 20:55:01,099 - INFO - Batch 300, Loss: 2.6419, LR: 0.000080
2025-10-21 20:55:15,231 - INFO - Batch 400, Loss: 2.6087, LR: 0.000080
2025-10-21 20:55:29,349 - INFO - Batch 500, Loss: 2.5576, LR: 0.000080
2025-10-21 20:55:43,485 - INFO - Batch 600, Loss: 2.6991, LR: 0.000080
2025-10-21 20:55:57,627 - INFO - Batch 700, Loss: 2.1679, LR: 0.000080
2025-10-21 20:56:11,760 - INFO - Batch 800, Loss: 2.3862, LR: 0.000080
2025-10-21 20:56:26,207 - INFO - Batch 900, Loss: 2.6311, LR: 0.000080
2025-10-21 20:56:40,640 - INFO - Batch 1000, Loss: 2.4032, LR: 0.000080
2025-10-21 20:56:54,984 - INFO - Batch 1100, Loss: 2.2836, LR: 0.000080
2025-10-21 20:57:09,349 - INFO - Batch 1200, Loss: 2.5336, LR: 0.000080
2025-10-21 20:57:23,774 - INFO - Batch 1300, Loss: 2.6002, LR: 0.000079
2025-10-21 20:57:37,838 - INFO - Batch 1400, Loss: 2.5237, LR: 0.000079
2025-10-21 20:57:51,917 - INFO - Batch 1500, Loss: 2.4263, LR: 0.000079
2025-10-21 20:58:05,981 - INFO - Batch 1600, Loss: 2.3444, LR: 0.000079
2025-10-21 20:58:20,041 - INFO - Batch 1700, Loss: 2.5700, LR: 0.000079
2025-10-21 20:58:34,112 - INFO - Batch 1800, Loss: 2.4516, LR: 0.000079
2025-10-21 20:58:48,201 - INFO - Batch 1900, Loss: 2.5708, LR: 0.000079
2025-10-21 20:59:02,275 - INFO - Batch 2000, Loss: 2.4653, LR: 0.000079
2025-10-21 20:59:16,363 - INFO - Batch 2100, Loss: 2.4352, LR: 0.000079
2025-10-21 20:59:30,516 - INFO - Batch 2200, Loss: 2.2789, LR: 0.000079
2025-10-21 20:59:44,594 - INFO - Batch 2300, Loss: 2.3339, LR: 0.000079
2025-10-21 20:59:58,791 - INFO - Batch 2400, Loss: 2.4027, LR: 0.000079
2025-10-21 21:00:13,047 - INFO - Batch 2500, Loss: 2.5004, LR: 0.000079
2025-10-21 21:00:27,330 - INFO - Batch 2600, Loss: 2.4087, LR: 0.000079
2025-10-21 21:00:41,591 - INFO - Batch 2700, Loss: 2.4524, LR: 0.000079
2025-10-21 21:00:55,807 - INFO - Batch 2800, Loss: 2.4539, LR: 0.000078
2025-10-21 21:01:10,010 - INFO - Batch 2900, Loss: 2.3368, LR: 0.000078
2025-10-21 21:01:24,241 - INFO - Batch 3000, Loss: 2.4538, LR: 0.000078
2025-10-21 21:01:38,535 - INFO - Batch 3100, Loss: 2.6145, LR: 0.000078
2025-10-21 21:01:52,943 - INFO - Batch 3200, Loss: 2.3269, LR: 0.000078
2025-10-21 21:02:07,351 - INFO - Batch 3300, Loss: 2.5706, LR: 0.000078
2025-10-21 21:02:21,696 - INFO - Batch 3400, Loss: 2.6230, LR: 0.000078
2025-10-21 21:02:36,065 - INFO - Batch 3500, Loss: 2.4453, LR: 0.000078
2025-10-21 21:02:50,437 - INFO - Batch 3600, Loss: 2.4701, LR: 0.000078
2025-10-21 21:03:04,838 - INFO - Batch 3700, Loss: 2.2487, LR: 0.000078
2025-10-21 21:03:19,066 - INFO - Batch 3800, Loss: 2.3772, LR: 0.000078
2025-10-21 21:03:33,291 - INFO - Batch 3900, Loss: 2.4347, LR: 0.000078
2025-10-21 21:03:47,767 - INFO - Batch 4000, Loss: 2.4516, LR: 0.000078
2025-10-21 21:04:01,996 - INFO - Batch 4100, Loss: 2.3225, LR: 0.000078
2025-10-21 21:04:16,300 - INFO - Batch 4200, Loss: 2.6210, LR: 0.000077
2025-10-21 21:04:30,626 - INFO - Batch 4300, Loss: 2.3354, LR: 0.000077
2025-10-21 21:04:44,866 - INFO - Batch 4400, Loss: 2.2201, LR: 0.000077
2025-10-21 21:04:59,112 - INFO - Batch 4500, Loss: 2.4718, LR: 0.000077
2025-10-21 21:05:13,318 - INFO - Batch 4600, Loss: 2.2568, LR: 0.000077
2025-10-21 21:05:27,536 - INFO - Batch 4700, Loss: 2.4483, LR: 0.000077
2025-10-21 21:05:41,791 - INFO - Batch 4800, Loss: 2.5195, LR: 0.000077
2025-10-21 21:05:56,077 - INFO - Batch 4900, Loss: 2.4541, LR: 0.000077
2025-10-21 21:06:10,330 - INFO - Batch 5000, Loss: 2.3637, LR: 0.000077
2025-10-21 21:06:24,618 - INFO - Batch 5100, Loss: 2.6333, LR: 0.000077
2025-10-21 21:06:38,817 - INFO - Batch 5200, Loss: 2.4905, LR: 0.000077
2025-10-21 21:06:53,211 - INFO - Batch 5300, Loss: 2.3309, LR: 0.000077
2025-10-21 21:07:07,442 - INFO - Batch 5400, Loss: 2.5485, LR: 0.000077
2025-10-21 21:07:21,796 - INFO - Batch 5500, Loss: 2.4596, LR: 0.000077
2025-10-21 21:07:36,056 - INFO - Batch 5600, Loss: 2.5831, LR: 0.000077
2025-10-21 21:07:50,331 - INFO - Batch 5700, Loss: 2.2664, LR: 0.000076
2025-10-21 21:08:04,576 - INFO - Batch 5800, Loss: 2.5733, LR: 0.000076
2025-10-21 21:08:18,835 - INFO - Batch 5900, Loss: 2.4080, LR: 0.000076
2025-10-21 21:08:33,033 - INFO - Batch 6000, Loss: 2.2116, LR: 0.000076
2025-10-21 21:08:47,208 - INFO - Batch 6100, Loss: 2.3465, LR: 0.000076
2025-10-21 21:09:01,366 - INFO - Batch 6200, Loss: 2.4642, LR: 0.000076
2025-10-21 21:09:15,532 - INFO - Batch 6300, Loss: 2.3052, LR: 0.000076
2025-10-21 21:09:29,688 - INFO - Batch 6400, Loss: 2.5565, LR: 0.000076
2025-10-21 21:09:36,777 - INFO - Epoch 10/30: Train Loss: 2.4510, Val Loss: 2.3354, LR: 0.000076
2025-10-21 21:09:37,223 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 21:09:37,418 - INFO - 模型已保存到: ./checkpoints/checkpoint_epoch_10.pth
2025-10-21 21:09:37,567 - INFO - Batch 0, Loss: 2.2556, LR: 0.000076
2025-10-21 21:09:52,231 - INFO - Batch 100, Loss: 2.4092, LR: 0.000076
2025-10-21 21:10:06,726 - INFO - Batch 200, Loss: 2.5517, LR: 0.000076
2025-10-21 21:10:21,202 - INFO - Batch 300, Loss: 2.2173, LR: 0.000076
2025-10-21 21:10:35,481 - INFO - Batch 400, Loss: 2.4957, LR: 0.000076
2025-10-21 21:10:49,769 - INFO - Batch 500, Loss: 2.3661, LR: 0.000076
2025-10-21 21:11:04,211 - INFO - Batch 600, Loss: 1.9316, LR: 0.000076
2025-10-21 21:11:18,766 - INFO - Batch 700, Loss: 2.4919, LR: 0.000075
2025-10-21 21:11:33,197 - INFO - Batch 800, Loss: 2.5819, LR: 0.000075
2025-10-21 21:11:47,450 - INFO - Batch 900, Loss: 2.1138, LR: 0.000075
2025-10-21 21:12:01,684 - INFO - Batch 1000, Loss: 2.2265, LR: 0.000075
2025-10-21 21:12:15,910 - INFO - Batch 1100, Loss: 2.3297, LR: 0.000075
2025-10-21 21:12:30,204 - INFO - Batch 1200, Loss: 2.4231, LR: 0.000075
2025-10-21 21:12:44,359 - INFO - Batch 1300, Loss: 2.4550, LR: 0.000075
2025-10-21 21:12:58,593 - INFO - Batch 1400, Loss: 2.4439, LR: 0.000075
2025-10-21 21:13:12,819 - INFO - Batch 1500, Loss: 2.2074, LR: 0.000075
2025-10-21 21:13:27,066 - INFO - Batch 1600, Loss: 2.4755, LR: 0.000075
2025-10-21 21:13:41,427 - INFO - Batch 1700, Loss: 2.3421, LR: 0.000075
2025-10-21 21:13:55,830 - INFO - Batch 1800, Loss: 2.1251, LR: 0.000075
2025-10-21 21:14:10,267 - INFO - Batch 1900, Loss: 2.5608, LR: 0.000075
2025-10-21 21:14:24,668 - INFO - Batch 2000, Loss: 2.5490, LR: 0.000075
2025-10-21 21:14:39,061 - INFO - Batch 2100, Loss: 2.3983, LR: 0.000074
2025-10-21 21:14:53,656 - INFO - Batch 2200, Loss: 2.4178, LR: 0.000074
2025-10-21 21:15:08,275 - INFO - Batch 2300, Loss: 2.7410, LR: 0.000074
2025-10-21 21:15:22,846 - INFO - Batch 2400, Loss: 2.2612, LR: 0.000074
2025-10-21 21:15:37,262 - INFO - Batch 2500, Loss: 2.2585, LR: 0.000074
2025-10-21 21:15:51,759 - INFO - Batch 2600, Loss: 2.4587, LR: 0.000074
2025-10-21 21:16:06,189 - INFO - Batch 2700, Loss: 2.1986, LR: 0.000074
2025-10-21 21:16:20,630 - INFO - Batch 2800, Loss: 2.3765, LR: 0.000074
2025-10-21 21:16:34,900 - INFO - Batch 2900, Loss: 2.5842, LR: 0.000074
2025-10-21 21:16:49,128 - INFO - Batch 3000, Loss: 2.6746, LR: 0.000074
2025-10-21 21:17:03,325 - INFO - Batch 3100, Loss: 2.1293, LR: 0.000074
2025-10-21 21:17:17,639 - INFO - Batch 3200, Loss: 2.3454, LR: 0.000074
2025-10-21 21:17:31,857 - INFO - Batch 3300, Loss: 2.3879, LR: 0.000074
2025-10-21 21:17:46,138 - INFO - Batch 3400, Loss: 2.4438, LR: 0.000074
2025-10-21 21:18:00,409 - INFO - Batch 3500, Loss: 2.3120, LR: 0.000073
2025-10-21 21:18:14,678 - INFO - Batch 3600, Loss: 2.9815, LR: 0.000073
2025-10-21 21:18:28,938 - INFO - Batch 3700, Loss: 2.6318, LR: 0.000073
2025-10-21 21:18:43,454 - INFO - Batch 3800, Loss: 2.5986, LR: 0.000073
2025-10-21 21:18:57,963 - INFO - Batch 3900, Loss: 2.3487, LR: 0.000073
2025-10-21 21:19:12,182 - INFO - Batch 4000, Loss: 2.5523, LR: 0.000073
2025-10-21 21:19:26,386 - INFO - Batch 4100, Loss: 2.3654, LR: 0.000073
2025-10-21 21:19:40,636 - INFO - Batch 4200, Loss: 2.4212, LR: 0.000073
2025-10-21 21:19:54,830 - INFO - Batch 4300, Loss: 2.1096, LR: 0.000073
2025-10-21 21:20:09,075 - INFO - Batch 4400, Loss: 2.1387, LR: 0.000073
2025-10-21 21:20:23,244 - INFO - Batch 4500, Loss: 2.3007, LR: 0.000073
2025-10-21 21:20:37,463 - INFO - Batch 4600, Loss: 2.1447, LR: 0.000073
2025-10-21 21:20:51,953 - INFO - Batch 4700, Loss: 2.4968, LR: 0.000073
2025-10-21 21:21:06,218 - INFO - Batch 4800, Loss: 2.6531, LR: 0.000072
2025-10-21 21:21:20,392 - INFO - Batch 4900, Loss: 2.4066, LR: 0.000072
2025-10-21 21:21:34,525 - INFO - Batch 5000, Loss: 2.3566, LR: 0.000072
2025-10-21 21:21:48,678 - INFO - Batch 5100, Loss: 2.1057, LR: 0.000072
2025-10-21 21:22:02,859 - INFO - Batch 5200, Loss: 2.4297, LR: 0.000072
2025-10-21 21:22:17,047 - INFO - Batch 5300, Loss: 2.4269, LR: 0.000072
2025-10-21 21:22:31,316 - INFO - Batch 5400, Loss: 2.5839, LR: 0.000072
2025-10-21 21:22:45,494 - INFO - Batch 5500, Loss: 2.6724, LR: 0.000072
2025-10-21 21:22:59,687 - INFO - Batch 5600, Loss: 2.2894, LR: 0.000072
2025-10-21 21:23:13,866 - INFO - Batch 5700, Loss: 2.4758, LR: 0.000072
2025-10-21 21:23:28,060 - INFO - Batch 5800, Loss: 2.6462, LR: 0.000072
2025-10-21 21:23:42,297 - INFO - Batch 5900, Loss: 2.4524, LR: 0.000072
2025-10-21 21:23:56,537 - INFO - Batch 6000, Loss: 2.8630, LR: 0.000072
2025-10-21 21:24:10,754 - INFO - Batch 6100, Loss: 2.3374, LR: 0.000072
2025-10-21 21:24:25,032 - INFO - Batch 6200, Loss: 2.1572, LR: 0.000071
2025-10-21 21:24:39,548 - INFO - Batch 6300, Loss: 2.4121, LR: 0.000071
2025-10-21 21:24:53,875 - INFO - Batch 6400, Loss: 2.4708, LR: 0.000071
2025-10-21 21:25:00,990 - INFO - Epoch 11/30: Train Loss: 2.4002, Val Loss: 2.2908, LR: 0.000071
2025-10-21 21:25:01,245 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 21:25:01,395 - INFO - Batch 0, Loss: 1.9247, LR: 0.000071
2025-10-21 21:25:15,855 - INFO - Batch 100, Loss: 2.3696, LR: 0.000071
2025-10-21 21:25:30,254 - INFO - Batch 200, Loss: 2.4028, LR: 0.000071
2025-10-21 21:25:44,584 - INFO - Batch 300, Loss: 2.0725, LR: 0.000071
2025-10-21 21:25:58,884 - INFO - Batch 400, Loss: 2.2833, LR: 0.000071
2025-10-21 21:26:13,175 - INFO - Batch 500, Loss: 2.4089, LR: 0.000071
2025-10-21 21:26:27,481 - INFO - Batch 600, Loss: 2.2000, LR: 0.000071
2025-10-21 21:26:41,794 - INFO - Batch 700, Loss: 2.6615, LR: 0.000071
2025-10-21 21:26:56,335 - INFO - Batch 800, Loss: 2.3080, LR: 0.000071
2025-10-21 21:27:10,714 - INFO - Batch 900, Loss: 2.2445, LR: 0.000071
2025-10-21 21:27:25,069 - INFO - Batch 1000, Loss: 2.3960, LR: 0.000071
2025-10-21 21:27:39,369 - INFO - Batch 1100, Loss: 2.4547, LR: 0.000070
2025-10-21 21:27:53,657 - INFO - Batch 1200, Loss: 2.1163, LR: 0.000070
2025-10-21 21:28:08,185 - INFO - Batch 1300, Loss: 2.6476, LR: 0.000070
2025-10-21 21:28:22,709 - INFO - Batch 1400, Loss: 2.4701, LR: 0.000070
2025-10-21 21:28:37,059 - INFO - Batch 1500, Loss: 2.3079, LR: 0.000070
2025-10-21 21:28:51,431 - INFO - Batch 1600, Loss: 2.2996, LR: 0.000070
2025-10-21 21:29:05,721 - INFO - Batch 1700, Loss: 2.7352, LR: 0.000070
2025-10-21 21:29:20,009 - INFO - Batch 1800, Loss: 2.1433, LR: 0.000070
2025-10-21 21:29:34,311 - INFO - Batch 1900, Loss: 2.0554, LR: 0.000070
2025-10-21 21:29:48,608 - INFO - Batch 2000, Loss: 2.2515, LR: 0.000070
2025-10-21 21:30:02,941 - INFO - Batch 2100, Loss: 2.4667, LR: 0.000070
2025-10-21 21:30:17,262 - INFO - Batch 2200, Loss: 2.4721, LR: 0.000070
2025-10-21 21:30:31,580 - INFO - Batch 2300, Loss: 2.2531, LR: 0.000070
2025-10-21 21:30:45,915 - INFO - Batch 2400, Loss: 2.5931, LR: 0.000069
2025-10-21 21:31:00,280 - INFO - Batch 2500, Loss: 2.4238, LR: 0.000069
2025-10-21 21:31:14,794 - INFO - Batch 2600, Loss: 2.6372, LR: 0.000069
2025-10-21 21:31:29,302 - INFO - Batch 2700, Loss: 2.3928, LR: 0.000069
2025-10-21 21:31:43,620 - INFO - Batch 2800, Loss: 2.3658, LR: 0.000069
2025-10-21 21:31:58,110 - INFO - Batch 2900, Loss: 2.3991, LR: 0.000069
2025-10-21 21:32:12,537 - INFO - Batch 3000, Loss: 2.5206, LR: 0.000069
2025-10-21 21:32:27,038 - INFO - Batch 3100, Loss: 2.5700, LR: 0.000069
2025-10-21 21:32:41,459 - INFO - Batch 3200, Loss: 2.7848, LR: 0.000069
2025-10-21 21:32:55,662 - INFO - Batch 3300, Loss: 2.4451, LR: 0.000069
2025-10-21 21:33:09,827 - INFO - Batch 3400, Loss: 2.4037, LR: 0.000069
2025-10-21 21:33:23,917 - INFO - Batch 3500, Loss: 2.4842, LR: 0.000069
2025-10-21 21:33:38,046 - INFO - Batch 3600, Loss: 2.2862, LR: 0.000069
2025-10-21 21:33:52,178 - INFO - Batch 3700, Loss: 2.2122, LR: 0.000068
2025-10-21 21:34:06,354 - INFO - Batch 3800, Loss: 2.5041, LR: 0.000068
2025-10-21 21:34:20,502 - INFO - Batch 3900, Loss: 2.3207, LR: 0.000068
2025-10-21 21:34:34,604 - INFO - Batch 4000, Loss: 2.3518, LR: 0.000068
2025-10-21 21:34:48,713 - INFO - Batch 4100, Loss: 2.3920, LR: 0.000068
2025-10-21 21:35:02,815 - INFO - Batch 4200, Loss: 2.1804, LR: 0.000068
2025-10-21 21:35:16,967 - INFO - Batch 4300, Loss: 2.2315, LR: 0.000068
2025-10-21 21:35:31,090 - INFO - Batch 4400, Loss: 2.4611, LR: 0.000068
2025-10-21 21:35:45,260 - INFO - Batch 4500, Loss: 2.4041, LR: 0.000068
2025-10-21 21:35:59,369 - INFO - Batch 4600, Loss: 2.4672, LR: 0.000068
2025-10-21 21:36:13,521 - INFO - Batch 4700, Loss: 2.4492, LR: 0.000068
2025-10-21 21:36:27,710 - INFO - Batch 4800, Loss: 2.6507, LR: 0.000068
2025-10-21 21:36:41,840 - INFO - Batch 4900, Loss: 2.4035, LR: 0.000068
2025-10-21 21:36:55,959 - INFO - Batch 5000, Loss: 2.2637, LR: 0.000067
2025-10-21 21:37:10,074 - INFO - Batch 5100, Loss: 2.5193, LR: 0.000067
2025-10-21 21:37:24,348 - INFO - Batch 5200, Loss: 2.1058, LR: 0.000067
2025-10-21 21:37:38,713 - INFO - Batch 5300, Loss: 2.1877, LR: 0.000067
2025-10-21 21:37:53,092 - INFO - Batch 5400, Loss: 2.4374, LR: 0.000067
2025-10-21 21:38:07,466 - INFO - Batch 5500, Loss: 2.4927, LR: 0.000067
2025-10-21 21:38:21,833 - INFO - Batch 5600, Loss: 2.1974, LR: 0.000067
2025-10-21 21:38:36,098 - INFO - Batch 5700, Loss: 2.1397, LR: 0.000067
2025-10-21 21:38:50,200 - INFO - Batch 5800, Loss: 2.4550, LR: 0.000067
2025-10-21 21:39:04,261 - INFO - Batch 5900, Loss: 2.1682, LR: 0.000067
2025-10-21 21:39:18,326 - INFO - Batch 6000, Loss: 2.2106, LR: 0.000067
2025-10-21 21:39:32,519 - INFO - Batch 6100, Loss: 2.5777, LR: 0.000067
2025-10-21 21:39:46,893 - INFO - Batch 6200, Loss: 2.3462, LR: 0.000067
2025-10-21 21:40:01,278 - INFO - Batch 6300, Loss: 2.2831, LR: 0.000066
2025-10-21 21:40:15,575 - INFO - Batch 6400, Loss: 2.4988, LR: 0.000066
2025-10-21 21:40:22,637 - INFO - Epoch 12/30: Train Loss: 2.3604, Val Loss: 2.2573, LR: 0.000066
2025-10-21 21:40:22,889 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 21:40:23,037 - INFO - Batch 0, Loss: 2.3694, LR: 0.000066
2025-10-21 21:40:37,274 - INFO - Batch 100, Loss: 1.9303, LR: 0.000066
2025-10-21 21:40:51,457 - INFO - Batch 200, Loss: 2.3118, LR: 0.000066
2025-10-21 21:41:05,659 - INFO - Batch 300, Loss: 2.2637, LR: 0.000066
2025-10-21 21:41:19,862 - INFO - Batch 400, Loss: 2.2427, LR: 0.000066
2025-10-21 21:41:34,110 - INFO - Batch 500, Loss: 2.3122, LR: 0.000066
2025-10-21 21:41:48,356 - INFO - Batch 600, Loss: 2.4349, LR: 0.000066
2025-10-21 21:42:02,622 - INFO - Batch 700, Loss: 2.2759, LR: 0.000066
2025-10-21 21:42:16,899 - INFO - Batch 800, Loss: 1.9853, LR: 0.000066
2025-10-21 21:42:31,354 - INFO - Batch 900, Loss: 2.2855, LR: 0.000066
2025-10-21 21:42:45,854 - INFO - Batch 1000, Loss: 2.1904, LR: 0.000066
2025-10-21 21:43:00,337 - INFO - Batch 1100, Loss: 2.2007, LR: 0.000066
2025-10-21 21:43:14,905 - INFO - Batch 1200, Loss: 2.4688, LR: 0.000065
2025-10-21 21:43:29,451 - INFO - Batch 1300, Loss: 2.5636, LR: 0.000065
2025-10-21 21:43:44,011 - INFO - Batch 1400, Loss: 2.4580, LR: 0.000065
2025-10-21 21:43:58,328 - INFO - Batch 1500, Loss: 2.0942, LR: 0.000065
2025-10-21 21:44:12,873 - INFO - Batch 1600, Loss: 2.2745, LR: 0.000065
2025-10-21 21:44:27,431 - INFO - Batch 1700, Loss: 2.5568, LR: 0.000065
2025-10-21 21:44:41,866 - INFO - Batch 1800, Loss: 2.0958, LR: 0.000065
2025-10-21 21:44:56,162 - INFO - Batch 1900, Loss: 2.3106, LR: 0.000065
2025-10-21 21:45:10,687 - INFO - Batch 2000, Loss: 2.0598, LR: 0.000065
2025-10-21 21:45:25,144 - INFO - Batch 2100, Loss: 2.4705, LR: 0.000065
2025-10-21 21:45:39,432 - INFO - Batch 2200, Loss: 2.5310, LR: 0.000065
2025-10-21 21:45:53,745 - INFO - Batch 2300, Loss: 2.2997, LR: 0.000065
2025-10-21 21:46:08,034 - INFO - Batch 2400, Loss: 2.2300, LR: 0.000065
2025-10-21 21:46:22,285 - INFO - Batch 2500, Loss: 2.2683, LR: 0.000064
2025-10-21 21:46:36,561 - INFO - Batch 2600, Loss: 2.1982, LR: 0.000064
2025-10-21 21:46:50,811 - INFO - Batch 2700, Loss: 2.1526, LR: 0.000064
2025-10-21 21:47:05,121 - INFO - Batch 2800, Loss: 2.1597, LR: 0.000064
2025-10-21 21:47:19,450 - INFO - Batch 2900, Loss: 2.1934, LR: 0.000064
2025-10-21 21:47:33,798 - INFO - Batch 3000, Loss: 2.5864, LR: 0.000064
2025-10-21 21:47:48,213 - INFO - Batch 3100, Loss: 2.5670, LR: 0.000064
2025-10-21 21:48:02,424 - INFO - Batch 3200, Loss: 2.0345, LR: 0.000064
2025-10-21 21:48:16,722 - INFO - Batch 3300, Loss: 2.0428, LR: 0.000064
2025-10-21 21:48:31,073 - INFO - Batch 3400, Loss: 2.2285, LR: 0.000064
2025-10-21 21:48:45,353 - INFO - Batch 3500, Loss: 2.2738, LR: 0.000064
2025-10-21 21:48:59,562 - INFO - Batch 3600, Loss: 2.3569, LR: 0.000064
2025-10-21 21:49:13,827 - INFO - Batch 3700, Loss: 2.1387, LR: 0.000063
2025-10-21 21:49:28,049 - INFO - Batch 3800, Loss: 2.2892, LR: 0.000063
2025-10-21 21:49:42,296 - INFO - Batch 3900, Loss: 2.1727, LR: 0.000063
2025-10-21 21:49:56,535 - INFO - Batch 4000, Loss: 2.2430, LR: 0.000063
2025-10-21 21:50:10,881 - INFO - Batch 4100, Loss: 2.3117, LR: 0.000063
2025-10-21 21:50:25,124 - INFO - Batch 4200, Loss: 2.2727, LR: 0.000063
2025-10-21 21:50:39,476 - INFO - Batch 4300, Loss: 2.4978, LR: 0.000063
2025-10-21 21:50:53,936 - INFO - Batch 4400, Loss: 2.4003, LR: 0.000063
2025-10-21 21:51:08,407 - INFO - Batch 4500, Loss: 2.2194, LR: 0.000063
2025-10-21 21:51:22,803 - INFO - Batch 4600, Loss: 2.1278, LR: 0.000063
2025-10-21 21:51:37,159 - INFO - Batch 4700, Loss: 2.2225, LR: 0.000063
2025-10-21 21:51:51,635 - INFO - Batch 4800, Loss: 2.3069, LR: 0.000063
2025-10-21 21:52:06,070 - INFO - Batch 4900, Loss: 2.2446, LR: 0.000063
2025-10-21 21:52:20,536 - INFO - Batch 5000, Loss: 2.1760, LR: 0.000062
2025-10-21 21:52:35,126 - INFO - Batch 5100, Loss: 2.0797, LR: 0.000062
2025-10-21 21:52:49,699 - INFO - Batch 5200, Loss: 2.4500, LR: 0.000062
2025-10-21 21:53:04,149 - INFO - Batch 5300, Loss: 2.1949, LR: 0.000062
2025-10-21 21:53:18,411 - INFO - Batch 5400, Loss: 2.1381, LR: 0.000062
2025-10-21 21:53:32,717 - INFO - Batch 5500, Loss: 2.1485, LR: 0.000062
2025-10-21 21:53:47,163 - INFO - Batch 5600, Loss: 2.1753, LR: 0.000062
2025-10-21 21:54:01,740 - INFO - Batch 5700, Loss: 2.4276, LR: 0.000062
2025-10-21 21:54:16,308 - INFO - Batch 5800, Loss: 2.2859, LR: 0.000062
2025-10-21 21:54:30,835 - INFO - Batch 5900, Loss: 1.9688, LR: 0.000062
2025-10-21 21:54:45,454 - INFO - Batch 6000, Loss: 2.3774, LR: 0.000062
2025-10-21 21:55:00,021 - INFO - Batch 6100, Loss: 2.6748, LR: 0.000062
2025-10-21 21:55:14,535 - INFO - Batch 6200, Loss: 2.2378, LR: 0.000061
2025-10-21 21:55:29,091 - INFO - Batch 6300, Loss: 2.4292, LR: 0.000061
2025-10-21 21:55:43,624 - INFO - Batch 6400, Loss: 2.2599, LR: 0.000061
2025-10-21 21:55:50,810 - INFO - Epoch 13/30: Train Loss: 2.3261, Val Loss: 2.2298, LR: 0.000061
2025-10-21 21:55:51,076 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 21:55:51,226 - INFO - Batch 0, Loss: 2.3464, LR: 0.000061
2025-10-21 21:56:05,694 - INFO - Batch 100, Loss: 2.1836, LR: 0.000061
2025-10-21 21:56:20,345 - INFO - Batch 200, Loss: 2.1015, LR: 0.000061
2025-10-21 21:56:34,934 - INFO - Batch 300, Loss: 2.4576, LR: 0.000061
2025-10-21 21:56:49,434 - INFO - Batch 400, Loss: 2.2989, LR: 0.000061
2025-10-21 21:57:03,892 - INFO - Batch 500, Loss: 2.3023, LR: 0.000061
2025-10-21 21:57:18,462 - INFO - Batch 600, Loss: 2.3379, LR: 0.000061
2025-10-21 21:57:32,651 - INFO - Batch 700, Loss: 2.1289, LR: 0.000061
2025-10-21 21:57:46,841 - INFO - Batch 800, Loss: 2.1322, LR: 0.000061
2025-10-21 21:58:01,001 - INFO - Batch 900, Loss: 2.2825, LR: 0.000061
2025-10-21 21:58:15,181 - INFO - Batch 1000, Loss: 2.3758, LR: 0.000061
2025-10-21 21:58:29,334 - INFO - Batch 1100, Loss: 2.1893, LR: 0.000060
2025-10-21 21:58:43,505 - INFO - Batch 1200, Loss: 1.9758, LR: 0.000060
2025-10-21 21:58:57,639 - INFO - Batch 1300, Loss: 2.0010, LR: 0.000060
2025-10-21 21:59:11,913 - INFO - Batch 1400, Loss: 2.4692, LR: 0.000060
2025-10-21 21:59:26,162 - INFO - Batch 1500, Loss: 2.4429, LR: 0.000060
2025-10-21 21:59:40,481 - INFO - Batch 1600, Loss: 2.5182, LR: 0.000060
2025-10-21 21:59:55,099 - INFO - Batch 1700, Loss: 2.5416, LR: 0.000060
2025-10-21 22:00:09,784 - INFO - Batch 1800, Loss: 2.1852, LR: 0.000060
2025-10-21 22:00:24,229 - INFO - Batch 1900, Loss: 2.1808, LR: 0.000060
2025-10-21 22:00:38,530 - INFO - Batch 2000, Loss: 2.2868, LR: 0.000060
2025-10-21 22:00:52,905 - INFO - Batch 2100, Loss: 2.1952, LR: 0.000060
2025-10-21 22:01:07,286 - INFO - Batch 2200, Loss: 2.2419, LR: 0.000060
2025-10-21 22:01:21,737 - INFO - Batch 2300, Loss: 2.2690, LR: 0.000059
2025-10-21 22:01:36,122 - INFO - Batch 2400, Loss: 2.3066, LR: 0.000059
2025-10-21 22:01:50,544 - INFO - Batch 2500, Loss: 2.1569, LR: 0.000059
2025-10-21 22:02:04,945 - INFO - Batch 2600, Loss: 2.4620, LR: 0.000059
2025-10-21 22:02:19,404 - INFO - Batch 2700, Loss: 2.2584, LR: 0.000059
2025-10-21 22:02:33,839 - INFO - Batch 2800, Loss: 2.1720, LR: 0.000059
2025-10-21 22:02:48,250 - INFO - Batch 2900, Loss: 2.2111, LR: 0.000059
2025-10-21 22:03:02,710 - INFO - Batch 3000, Loss: 2.3432, LR: 0.000059
2025-10-21 22:03:17,151 - INFO - Batch 3100, Loss: 2.2107, LR: 0.000059
2025-10-21 22:03:31,593 - INFO - Batch 3200, Loss: 2.0485, LR: 0.000059
2025-10-21 22:03:46,044 - INFO - Batch 3300, Loss: 2.2005, LR: 0.000059
2025-10-21 22:04:00,476 - INFO - Batch 3400, Loss: 2.3177, LR: 0.000059
2025-10-21 22:04:14,845 - INFO - Batch 3500, Loss: 2.2375, LR: 0.000058
2025-10-21 22:04:29,273 - INFO - Batch 3600, Loss: 2.3649, LR: 0.000058
2025-10-21 22:04:43,510 - INFO - Batch 3700, Loss: 2.3275, LR: 0.000058
2025-10-21 22:04:57,689 - INFO - Batch 3800, Loss: 2.2695, LR: 0.000058
2025-10-21 22:05:12,027 - INFO - Batch 3900, Loss: 2.1533, LR: 0.000058
2025-10-21 22:05:26,408 - INFO - Batch 4000, Loss: 2.4030, LR: 0.000058
2025-10-21 22:05:40,814 - INFO - Batch 4100, Loss: 1.9800, LR: 0.000058
2025-10-21 22:05:55,296 - INFO - Batch 4200, Loss: 2.3606, LR: 0.000058
2025-10-21 22:06:09,657 - INFO - Batch 4300, Loss: 2.3112, LR: 0.000058
2025-10-21 22:06:23,851 - INFO - Batch 4400, Loss: 2.3538, LR: 0.000058
2025-10-21 22:06:38,226 - INFO - Batch 4500, Loss: 2.2534, LR: 0.000058
2025-10-21 22:06:52,638 - INFO - Batch 4600, Loss: 2.3278, LR: 0.000058
2025-10-21 22:07:07,083 - INFO - Batch 4700, Loss: 2.1158, LR: 0.000058
2025-10-21 22:07:21,632 - INFO - Batch 4800, Loss: 2.3433, LR: 0.000057
2025-10-21 22:07:35,886 - INFO - Batch 4900, Loss: 2.1574, LR: 0.000057
2025-10-21 22:07:50,190 - INFO - Batch 5000, Loss: 2.4642, LR: 0.000057
2025-10-21 22:08:04,685 - INFO - Batch 5100, Loss: 2.4711, LR: 0.000057
2025-10-21 22:08:18,976 - INFO - Batch 5200, Loss: 2.4649, LR: 0.000057
2025-10-21 22:08:33,234 - INFO - Batch 5300, Loss: 2.3193, LR: 0.000057
2025-10-21 22:08:47,491 - INFO - Batch 5400, Loss: 2.3359, LR: 0.000057
2025-10-21 22:09:01,654 - INFO - Batch 5500, Loss: 2.4237, LR: 0.000057
2025-10-21 22:09:16,025 - INFO - Batch 5600, Loss: 2.3220, LR: 0.000057
2025-10-21 22:09:30,046 - INFO - Batch 5700, Loss: 2.5150, LR: 0.000057
2025-10-21 22:09:44,266 - INFO - Batch 5800, Loss: 2.3018, LR: 0.000057
2025-10-21 22:09:58,552 - INFO - Batch 5900, Loss: 2.2804, LR: 0.000057
2025-10-21 22:10:12,671 - INFO - Batch 6000, Loss: 2.1253, LR: 0.000056
2025-10-21 22:10:26,898 - INFO - Batch 6100, Loss: 2.1801, LR: 0.000056
2025-10-21 22:10:41,271 - INFO - Batch 6200, Loss: 2.2238, LR: 0.000056
2025-10-21 22:10:55,395 - INFO - Batch 6300, Loss: 2.2279, LR: 0.000056
2025-10-21 22:11:09,509 - INFO - Batch 6400, Loss: 2.2050, LR: 0.000056
2025-10-21 22:11:16,570 - INFO - Epoch 14/30: Train Loss: 2.2966, Val Loss: 2.2080, LR: 0.000056
2025-10-21 22:11:16,841 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 22:11:16,994 - INFO - Batch 0, Loss: 2.3535, LR: 0.000056
2025-10-21 22:11:31,306 - INFO - Batch 100, Loss: 2.0763, LR: 0.000056
2025-10-21 22:11:45,812 - INFO - Batch 200, Loss: 2.2855, LR: 0.000056
2025-10-21 22:12:00,173 - INFO - Batch 300, Loss: 2.1850, LR: 0.000056
2025-10-21 22:12:14,513 - INFO - Batch 400, Loss: 2.1228, LR: 0.000056
2025-10-21 22:12:28,818 - INFO - Batch 500, Loss: 2.4277, LR: 0.000056
2025-10-21 22:12:43,268 - INFO - Batch 600, Loss: 2.3910, LR: 0.000056
2025-10-21 22:12:57,680 - INFO - Batch 700, Loss: 2.5677, LR: 0.000056
2025-10-21 22:13:12,276 - INFO - Batch 800, Loss: 2.3056, LR: 0.000055
2025-10-21 22:13:26,616 - INFO - Batch 900, Loss: 2.1599, LR: 0.000055
2025-10-21 22:13:40,938 - INFO - Batch 1000, Loss: 2.4798, LR: 0.000055
2025-10-21 22:13:55,218 - INFO - Batch 1100, Loss: 2.3763, LR: 0.000055
2025-10-21 22:14:09,513 - INFO - Batch 1200, Loss: 2.2493, LR: 0.000055
2025-10-21 22:14:23,880 - INFO - Batch 1300, Loss: 2.1012, LR: 0.000055
2025-10-21 22:14:38,288 - INFO - Batch 1400, Loss: 2.4172, LR: 0.000055
2025-10-21 22:14:52,675 - INFO - Batch 1500, Loss: 2.2777, LR: 0.000055
2025-10-21 22:15:07,139 - INFO - Batch 1600, Loss: 2.3846, LR: 0.000055
2025-10-21 22:15:21,675 - INFO - Batch 1700, Loss: 2.2076, LR: 0.000055
2025-10-21 22:15:36,233 - INFO - Batch 1800, Loss: 1.8952, LR: 0.000055
2025-10-21 22:15:50,855 - INFO - Batch 1900, Loss: 2.5091, LR: 0.000055
2025-10-21 22:16:05,467 - INFO - Batch 2000, Loss: 2.2997, LR: 0.000054
2025-10-21 22:16:19,955 - INFO - Batch 2100, Loss: 2.1544, LR: 0.000054
2025-10-21 22:16:34,563 - INFO - Batch 2200, Loss: 2.3777, LR: 0.000054
2025-10-21 22:16:49,106 - INFO - Batch 2300, Loss: 2.4686, LR: 0.000054
2025-10-21 22:17:03,710 - INFO - Batch 2400, Loss: 2.2617, LR: 0.000054
2025-10-21 22:17:18,231 - INFO - Batch 2500, Loss: 2.3388, LR: 0.000054
2025-10-21 22:17:32,818 - INFO - Batch 2600, Loss: 2.2374, LR: 0.000054
2025-10-21 22:17:47,153 - INFO - Batch 2700, Loss: 2.2695, LR: 0.000054
2025-10-21 22:18:01,622 - INFO - Batch 2800, Loss: 1.9098, LR: 0.000054
2025-10-21 22:18:16,147 - INFO - Batch 2900, Loss: 2.1243, LR: 0.000054
2025-10-21 22:18:30,700 - INFO - Batch 3000, Loss: 2.2385, LR: 0.000054
2025-10-21 22:18:45,162 - INFO - Batch 3100, Loss: 2.3069, LR: 0.000054
2025-10-21 22:18:59,587 - INFO - Batch 3200, Loss: 2.2557, LR: 0.000053
2025-10-21 22:19:13,961 - INFO - Batch 3300, Loss: 2.3824, LR: 0.000053
2025-10-21 22:19:28,403 - INFO - Batch 3400, Loss: 2.3657, LR: 0.000053
2025-10-21 22:19:42,828 - INFO - Batch 3500, Loss: 2.2000, LR: 0.000053
2025-10-21 22:19:57,245 - INFO - Batch 3600, Loss: 2.1552, LR: 0.000053
2025-10-21 22:20:11,490 - INFO - Batch 3700, Loss: 2.2546, LR: 0.000053
2025-10-21 22:20:25,919 - INFO - Batch 3800, Loss: 2.3220, LR: 0.000053
2025-10-21 22:20:40,450 - INFO - Batch 3900, Loss: 2.4052, LR: 0.000053
2025-10-21 22:20:54,770 - INFO - Batch 4000, Loss: 2.3459, LR: 0.000053
2025-10-21 22:21:09,208 - INFO - Batch 4100, Loss: 2.1362, LR: 0.000053
2025-10-21 22:21:23,618 - INFO - Batch 4200, Loss: 2.3658, LR: 0.000053
2025-10-21 22:21:38,003 - INFO - Batch 4300, Loss: 2.1666, LR: 0.000053
2025-10-21 22:21:52,413 - INFO - Batch 4400, Loss: 2.2917, LR: 0.000052
2025-10-21 22:22:06,943 - INFO - Batch 4500, Loss: 2.3203, LR: 0.000052
2025-10-21 22:22:21,390 - INFO - Batch 4600, Loss: 2.2125, LR: 0.000052
2025-10-21 22:22:35,852 - INFO - Batch 4700, Loss: 2.2491, LR: 0.000052
2025-10-21 22:22:50,349 - INFO - Batch 4800, Loss: 2.5483, LR: 0.000052
2025-10-21 22:23:04,730 - INFO - Batch 4900, Loss: 2.4578, LR: 0.000052
2025-10-21 22:23:19,182 - INFO - Batch 5000, Loss: 2.4075, LR: 0.000052
2025-10-21 22:23:33,643 - INFO - Batch 5100, Loss: 2.0881, LR: 0.000052
2025-10-21 22:23:47,965 - INFO - Batch 5200, Loss: 2.2177, LR: 0.000052
2025-10-21 22:24:02,235 - INFO - Batch 5300, Loss: 2.3948, LR: 0.000052
2025-10-21 22:24:16,530 - INFO - Batch 5400, Loss: 2.1234, LR: 0.000052
2025-10-21 22:24:30,954 - INFO - Batch 5500, Loss: 2.2609, LR: 0.000052
2025-10-21 22:24:45,281 - INFO - Batch 5600, Loss: 2.2591, LR: 0.000052
2025-10-21 22:24:59,616 - INFO - Batch 5700, Loss: 2.1473, LR: 0.000051
2025-10-21 22:25:13,948 - INFO - Batch 5800, Loss: 2.3681, LR: 0.000051
2025-10-21 22:25:28,269 - INFO - Batch 5900, Loss: 2.2250, LR: 0.000051
2025-10-21 22:25:42,630 - INFO - Batch 6000, Loss: 2.3346, LR: 0.000051
2025-10-21 22:25:57,049 - INFO - Batch 6100, Loss: 2.4332, LR: 0.000051
2025-10-21 22:26:11,345 - INFO - Batch 6200, Loss: 2.3355, LR: 0.000051
2025-10-21 22:26:25,691 - INFO - Batch 6300, Loss: 2.2708, LR: 0.000051
2025-10-21 22:26:39,998 - INFO - Batch 6400, Loss: 2.0068, LR: 0.000051
2025-10-21 22:26:47,195 - INFO - Epoch 15/30: Train Loss: 2.2697, Val Loss: 2.1815, LR: 0.000051
2025-10-21 22:26:47,471 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 22:26:47,622 - INFO - Batch 0, Loss: 1.9454, LR: 0.000051
2025-10-21 22:27:01,993 - INFO - Batch 100, Loss: 2.0609, LR: 0.000051
2025-10-21 22:27:16,361 - INFO - Batch 200, Loss: 2.2746, LR: 0.000051
2025-10-21 22:27:30,728 - INFO - Batch 300, Loss: 2.2070, LR: 0.000051
2025-10-21 22:27:45,128 - INFO - Batch 400, Loss: 2.4437, LR: 0.000050
2025-10-21 22:27:59,529 - INFO - Batch 500, Loss: 2.1328, LR: 0.000050
2025-10-21 22:28:13,924 - INFO - Batch 600, Loss: 2.4613, LR: 0.000050
2025-10-21 22:28:28,400 - INFO - Batch 700, Loss: 2.2395, LR: 0.000050
2025-10-21 22:28:42,844 - INFO - Batch 800, Loss: 1.9931, LR: 0.000050
2025-10-21 22:28:57,232 - INFO - Batch 900, Loss: 2.1802, LR: 0.000050
2025-10-21 22:29:12,395 - INFO - Batch 1000, Loss: 2.4613, LR: 0.000050
2025-10-21 22:29:28,397 - INFO - Batch 1100, Loss: 2.1793, LR: 0.000050
2025-10-21 22:29:44,384 - INFO - Batch 1200, Loss: 2.1042, LR: 0.000050
2025-10-21 22:30:00,428 - INFO - Batch 1300, Loss: 2.2284, LR: 0.000050
2025-10-21 22:30:16,490 - INFO - Batch 1400, Loss: 2.0194, LR: 0.000050
2025-10-21 22:30:32,521 - INFO - Batch 1500, Loss: 2.1388, LR: 0.000050
2025-10-21 22:30:48,552 - INFO - Batch 1600, Loss: 2.2428, LR: 0.000050
2025-10-21 22:31:04,606 - INFO - Batch 1700, Loss: 2.2117, LR: 0.000049
2025-10-21 22:31:20,535 - INFO - Batch 1800, Loss: 2.2725, LR: 0.000049
2025-10-21 22:31:36,421 - INFO - Batch 1900, Loss: 2.2801, LR: 0.000049
2025-10-21 22:31:52,305 - INFO - Batch 2000, Loss: 1.9338, LR: 0.000049
2025-10-21 22:32:08,113 - INFO - Batch 2100, Loss: 2.1968, LR: 0.000049
2025-10-21 22:32:24,120 - INFO - Batch 2200, Loss: 2.2370, LR: 0.000049
2025-10-21 22:32:40,294 - INFO - Batch 2300, Loss: 2.1463, LR: 0.000049
2025-10-21 22:32:56,345 - INFO - Batch 2400, Loss: 2.3125, LR: 0.000049
2025-10-21 22:33:12,367 - INFO - Batch 2500, Loss: 2.1415, LR: 0.000049
2025-10-21 22:33:28,456 - INFO - Batch 2600, Loss: 2.3394, LR: 0.000049
2025-10-21 22:33:44,453 - INFO - Batch 2700, Loss: 2.2712, LR: 0.000049
2025-10-21 22:34:00,451 - INFO - Batch 2800, Loss: 2.3902, LR: 0.000049
2025-10-21 22:34:16,567 - INFO - Batch 2900, Loss: 2.4731, LR: 0.000048
2025-10-21 22:34:32,412 - INFO - Batch 3000, Loss: 2.1871, LR: 0.000048
2025-10-21 22:34:48,218 - INFO - Batch 3100, Loss: 2.1152, LR: 0.000048
2025-10-21 22:35:04,234 - INFO - Batch 3200, Loss: 2.3584, LR: 0.000048
2025-10-21 22:35:20,302 - INFO - Batch 3300, Loss: 2.1506, LR: 0.000048
2025-10-21 22:35:36,355 - INFO - Batch 3400, Loss: 2.3410, LR: 0.000048
2025-10-21 22:35:52,476 - INFO - Batch 3500, Loss: 2.3064, LR: 0.000048
2025-10-21 22:36:08,558 - INFO - Batch 3600, Loss: 2.4084, LR: 0.000048
2025-10-21 22:36:24,561 - INFO - Batch 3700, Loss: 2.3597, LR: 0.000048
2025-10-21 22:36:40,582 - INFO - Batch 3800, Loss: 2.2438, LR: 0.000048
2025-10-21 22:36:56,591 - INFO - Batch 3900, Loss: 2.0663, LR: 0.000048
2025-10-21 22:37:12,607 - INFO - Batch 4000, Loss: 2.2242, LR: 0.000048
2025-10-21 22:37:28,428 - INFO - Batch 4100, Loss: 2.4855, LR: 0.000047
2025-10-21 22:37:44,307 - INFO - Batch 4200, Loss: 2.3918, LR: 0.000047
2025-10-21 22:38:00,119 - INFO - Batch 4300, Loss: 2.3946, LR: 0.000047
2025-10-21 22:38:16,050 - INFO - Batch 4400, Loss: 2.3311, LR: 0.000047
2025-10-21 22:38:32,119 - INFO - Batch 4500, Loss: 2.5207, LR: 0.000047
2025-10-21 22:38:48,139 - INFO - Batch 4600, Loss: 2.4696, LR: 0.000047
2025-10-21 22:39:04,115 - INFO - Batch 4700, Loss: 2.2418, LR: 0.000047
2025-10-21 22:39:20,143 - INFO - Batch 4800, Loss: 2.7474, LR: 0.000047
2025-10-21 22:39:36,164 - INFO - Batch 4900, Loss: 2.4390, LR: 0.000047
2025-10-21 22:39:52,185 - INFO - Batch 5000, Loss: 2.1069, LR: 0.000047
2025-10-21 22:40:08,204 - INFO - Batch 5100, Loss: 2.0514, LR: 0.000047
2025-10-21 22:40:24,023 - INFO - Batch 5200, Loss: 2.4861, LR: 0.000047
2025-10-21 22:40:39,806 - INFO - Batch 5300, Loss: 1.9435, LR: 0.000046
2025-10-21 22:40:55,604 - INFO - Batch 5400, Loss: 2.6409, LR: 0.000046
2025-10-21 22:41:11,526 - INFO - Batch 5500, Loss: 2.0682, LR: 0.000046
2025-10-21 22:41:27,529 - INFO - Batch 5600, Loss: 2.1565, LR: 0.000046
2025-10-21 22:41:43,598 - INFO - Batch 5700, Loss: 2.1607, LR: 0.000046
2025-10-21 22:41:59,641 - INFO - Batch 5800, Loss: 2.0802, LR: 0.000046
2025-10-21 22:42:15,674 - INFO - Batch 5900, Loss: 2.3988, LR: 0.000046
2025-10-21 22:42:31,727 - INFO - Batch 6000, Loss: 2.3049, LR: 0.000046
2025-10-21 22:42:47,821 - INFO - Batch 6100, Loss: 2.1856, LR: 0.000046
2025-10-21 22:43:03,852 - INFO - Batch 6200, Loss: 2.0070, LR: 0.000046
2025-10-21 22:43:19,701 - INFO - Batch 6300, Loss: 2.2356, LR: 0.000046
2025-10-21 22:43:35,537 - INFO - Batch 6400, Loss: 2.5294, LR: 0.000046
2025-10-21 22:43:43,491 - INFO - Epoch 16/30: Train Loss: 2.2466, Val Loss: 2.1676, LR: 0.000046
2025-10-21 22:43:43,780 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 22:43:43,952 - INFO - Batch 0, Loss: 2.1521, LR: 0.000046
2025-10-21 22:43:59,918 - INFO - Batch 100, Loss: 2.1489, LR: 0.000045
2025-10-21 22:44:16,136 - INFO - Batch 200, Loss: 2.1972, LR: 0.000045
2025-10-21 22:44:32,280 - INFO - Batch 300, Loss: 2.1938, LR: 0.000045
2025-10-21 22:44:48,430 - INFO - Batch 400, Loss: 2.1238, LR: 0.000045
2025-10-21 22:45:04,581 - INFO - Batch 500, Loss: 2.2986, LR: 0.000045
2025-10-21 22:45:20,727 - INFO - Batch 600, Loss: 2.2653, LR: 0.000045
2025-10-21 22:45:36,877 - INFO - Batch 700, Loss: 2.0392, LR: 0.000045
2025-10-21 22:45:52,996 - INFO - Batch 800, Loss: 2.2848, LR: 0.000045
2025-10-21 22:46:09,037 - INFO - Batch 900, Loss: 2.2778, LR: 0.000045
2025-10-21 22:46:25,051 - INFO - Batch 1000, Loss: 2.5135, LR: 0.000045
2025-10-21 22:46:41,007 - INFO - Batch 1100, Loss: 2.2775, LR: 0.000045
2025-10-21 22:46:57,057 - INFO - Batch 1200, Loss: 2.2870, LR: 0.000045
2025-10-21 22:47:13,191 - INFO - Batch 1300, Loss: 2.0999, LR: 0.000044
2025-10-21 22:47:29,294 - INFO - Batch 1400, Loss: 2.3435, LR: 0.000044
2025-10-21 22:47:45,489 - INFO - Batch 1500, Loss: 2.1816, LR: 0.000044
2025-10-21 22:48:01,624 - INFO - Batch 1600, Loss: 2.2884, LR: 0.000044
2025-10-21 22:48:17,740 - INFO - Batch 1700, Loss: 2.0038, LR: 0.000044
2025-10-21 22:48:33,893 - INFO - Batch 1800, Loss: 2.5028, LR: 0.000044
2025-10-21 22:48:49,983 - INFO - Batch 1900, Loss: 1.9903, LR: 0.000044
2025-10-21 22:49:05,892 - INFO - Batch 2000, Loss: 2.5179, LR: 0.000044
2025-10-21 22:49:21,827 - INFO - Batch 2100, Loss: 2.0989, LR: 0.000044
2025-10-21 22:49:37,800 - INFO - Batch 2200, Loss: 2.3924, LR: 0.000044
2025-10-21 22:49:53,765 - INFO - Batch 2300, Loss: 2.2521, LR: 0.000044
2025-10-21 22:50:09,922 - INFO - Batch 2400, Loss: 2.4236, LR: 0.000044
2025-10-21 22:50:26,040 - INFO - Batch 2500, Loss: 2.5408, LR: 0.000043
2025-10-21 22:50:42,175 - INFO - Batch 2600, Loss: 2.3381, LR: 0.000043
2025-10-21 22:50:58,262 - INFO - Batch 2700, Loss: 2.0905, LR: 0.000043
2025-10-21 22:51:14,377 - INFO - Batch 2800, Loss: 2.2474, LR: 0.000043
2025-10-21 22:51:30,484 - INFO - Batch 2900, Loss: 2.3767, LR: 0.000043
2025-10-21 22:51:46,563 - INFO - Batch 3000, Loss: 2.3146, LR: 0.000043
2025-10-21 22:52:02,497 - INFO - Batch 3100, Loss: 2.2111, LR: 0.000043
2025-10-21 22:52:18,366 - INFO - Batch 3200, Loss: 2.1070, LR: 0.000043
2025-10-21 22:52:34,190 - INFO - Batch 3300, Loss: 2.3184, LR: 0.000043
2025-10-21 22:52:50,188 - INFO - Batch 3400, Loss: 2.0391, LR: 0.000043
2025-10-21 22:53:06,287 - INFO - Batch 3500, Loss: 2.4632, LR: 0.000043
2025-10-21 22:53:22,380 - INFO - Batch 3600, Loss: 2.2765, LR: 0.000043
2025-10-21 22:53:38,415 - INFO - Batch 3700, Loss: 2.2672, LR: 0.000043
2025-10-21 22:53:54,559 - INFO - Batch 3800, Loss: 2.4667, LR: 0.000042
2025-10-21 22:54:10,639 - INFO - Batch 3900, Loss: 2.4043, LR: 0.000042
2025-10-21 22:54:26,710 - INFO - Batch 4000, Loss: 2.2188, LR: 0.000042
2025-10-21 22:54:42,750 - INFO - Batch 4100, Loss: 2.2594, LR: 0.000042
2025-10-21 22:54:58,675 - INFO - Batch 4200, Loss: 1.9683, LR: 0.000042
2025-10-21 22:55:14,590 - INFO - Batch 4300, Loss: 2.2104, LR: 0.000042
2025-10-21 22:55:30,449 - INFO - Batch 4400, Loss: 2.4222, LR: 0.000042
2025-10-21 22:55:46,384 - INFO - Batch 4500, Loss: 2.3066, LR: 0.000042
2025-10-21 22:56:02,426 - INFO - Batch 4600, Loss: 2.5525, LR: 0.000042
2025-10-21 22:56:18,451 - INFO - Batch 4700, Loss: 2.3681, LR: 0.000042
2025-10-21 22:56:34,508 - INFO - Batch 4800, Loss: 2.0409, LR: 0.000042
2025-10-21 22:56:50,533 - INFO - Batch 4900, Loss: 2.1638, LR: 0.000042
2025-10-21 22:57:06,594 - INFO - Batch 5000, Loss: 2.1578, LR: 0.000041
2025-10-21 22:57:22,638 - INFO - Batch 5100, Loss: 2.2039, LR: 0.000041
2025-10-21 22:57:38,679 - INFO - Batch 5200, Loss: 2.1667, LR: 0.000041
2025-10-21 22:57:54,569 - INFO - Batch 5300, Loss: 2.1626, LR: 0.000041
2025-10-21 22:58:10,375 - INFO - Batch 5400, Loss: 2.3665, LR: 0.000041
2025-10-21 22:58:26,165 - INFO - Batch 5500, Loss: 2.0162, LR: 0.000041
2025-10-21 22:58:42,012 - INFO - Batch 5600, Loss: 2.2573, LR: 0.000041
2025-10-21 22:58:58,037 - INFO - Batch 5700, Loss: 2.0665, LR: 0.000041
2025-10-21 22:59:14,065 - INFO - Batch 5800, Loss: 2.4800, LR: 0.000041
2025-10-21 22:59:30,098 - INFO - Batch 5900, Loss: 2.2579, LR: 0.000041
2025-10-21 22:59:46,151 - INFO - Batch 6000, Loss: 2.2388, LR: 0.000041
2025-10-21 23:00:02,274 - INFO - Batch 6100, Loss: 2.0883, LR: 0.000041
2025-10-21 23:00:18,323 - INFO - Batch 6200, Loss: 2.4926, LR: 0.000040
2025-10-21 23:00:34,307 - INFO - Batch 6300, Loss: 2.2115, LR: 0.000040
2025-10-21 23:00:50,226 - INFO - Batch 6400, Loss: 2.2233, LR: 0.000040
2025-10-21 23:00:58,189 - INFO - Epoch 17/30: Train Loss: 2.2260, Val Loss: 2.1497, LR: 0.000040
2025-10-21 23:00:58,757 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 23:00:58,925 - INFO - Batch 0, Loss: 2.3203, LR: 0.000040
2025-10-21 23:01:14,625 - INFO - Batch 100, Loss: 2.4186, LR: 0.000040
2025-10-21 23:01:30,255 - INFO - Batch 200, Loss: 2.4386, LR: 0.000040
2025-10-21 23:01:46,190 - INFO - Batch 300, Loss: 2.1242, LR: 0.000040
2025-10-21 23:02:02,160 - INFO - Batch 400, Loss: 2.1616, LR: 0.000040
2025-10-21 23:02:18,078 - INFO - Batch 500, Loss: 2.2139, LR: 0.000040
2025-10-21 23:02:34,014 - INFO - Batch 600, Loss: 2.4851, LR: 0.000040
2025-10-21 23:02:49,915 - INFO - Batch 700, Loss: 2.4221, LR: 0.000040
2025-10-21 23:03:05,843 - INFO - Batch 800, Loss: 2.0277, LR: 0.000040
2025-10-21 23:03:21,803 - INFO - Batch 900, Loss: 2.0444, LR: 0.000040
2025-10-21 23:03:37,671 - INFO - Batch 1000, Loss: 2.3977, LR: 0.000039
2025-10-21 23:03:53,442 - INFO - Batch 1100, Loss: 2.3918, LR: 0.000039
2025-10-21 23:04:09,237 - INFO - Batch 1200, Loss: 2.1455, LR: 0.000039
2025-10-21 23:04:24,950 - INFO - Batch 1300, Loss: 2.2088, LR: 0.000039
2025-10-21 23:04:40,923 - INFO - Batch 1400, Loss: 2.3344, LR: 0.000039
2025-10-21 23:04:56,974 - INFO - Batch 1500, Loss: 2.2888, LR: 0.000039
2025-10-21 23:05:12,961 - INFO - Batch 1600, Loss: 2.1282, LR: 0.000039
2025-10-21 23:05:28,980 - INFO - Batch 1700, Loss: 2.1107, LR: 0.000039
2025-10-21 23:05:45,012 - INFO - Batch 1800, Loss: 2.4027, LR: 0.000039
2025-10-21 23:06:01,033 - INFO - Batch 1900, Loss: 2.2265, LR: 0.000039
2025-10-21 23:06:17,135 - INFO - Batch 2000, Loss: 2.4056, LR: 0.000039
2025-10-21 23:06:33,027 - INFO - Batch 2100, Loss: 2.1231, LR: 0.000039
2025-10-21 23:06:48,832 - INFO - Batch 2200, Loss: 2.2255, LR: 0.000039
2025-10-21 23:07:04,646 - INFO - Batch 2300, Loss: 2.0486, LR: 0.000038
2025-10-21 23:07:20,454 - INFO - Batch 2400, Loss: 2.1660, LR: 0.000038
2025-10-21 23:07:36,433 - INFO - Batch 2500, Loss: 2.2669, LR: 0.000038
2025-10-21 23:07:52,568 - INFO - Batch 2600, Loss: 2.2834, LR: 0.000038
2025-10-21 23:08:08,609 - INFO - Batch 2700, Loss: 2.1723, LR: 0.000038
2025-10-21 23:08:24,625 - INFO - Batch 2800, Loss: 2.0959, LR: 0.000038
2025-10-21 23:08:40,699 - INFO - Batch 2900, Loss: 2.4679, LR: 0.000038
2025-10-21 23:08:56,699 - INFO - Batch 3000, Loss: 2.2628, LR: 0.000038
2025-10-21 23:09:12,698 - INFO - Batch 3100, Loss: 2.2899, LR: 0.000038
2025-10-21 23:09:28,597 - INFO - Batch 3200, Loss: 1.9381, LR: 0.000038
2025-10-21 23:09:44,386 - INFO - Batch 3300, Loss: 2.5769, LR: 0.000038
2025-10-21 23:10:00,211 - INFO - Batch 3400, Loss: 2.2287, LR: 0.000038
2025-10-21 23:10:16,045 - INFO - Batch 3500, Loss: 2.2399, LR: 0.000037
2025-10-21 23:10:32,120 - INFO - Batch 3600, Loss: 2.4131, LR: 0.000037
2025-10-21 23:10:48,197 - INFO - Batch 3700, Loss: 2.2922, LR: 0.000037
2025-10-21 23:11:04,168 - INFO - Batch 3800, Loss: 2.0950, LR: 0.000037
2025-10-21 23:11:20,262 - INFO - Batch 3900, Loss: 2.2002, LR: 0.000037
2025-10-21 23:11:36,276 - INFO - Batch 4000, Loss: 2.0455, LR: 0.000037
2025-10-21 23:11:52,334 - INFO - Batch 4100, Loss: 2.1195, LR: 0.000037
2025-10-21 23:12:08,450 - INFO - Batch 4200, Loss: 2.1603, LR: 0.000037
2025-10-21 23:12:24,357 - INFO - Batch 4300, Loss: 2.1099, LR: 0.000037
2025-10-21 23:12:40,163 - INFO - Batch 4400, Loss: 2.2464, LR: 0.000037
2025-10-21 23:12:56,104 - INFO - Batch 4500, Loss: 2.2071, LR: 0.000037
2025-10-21 23:13:11,897 - INFO - Batch 4600, Loss: 2.3092, LR: 0.000037
2025-10-21 23:13:27,855 - INFO - Batch 4700, Loss: 2.1419, LR: 0.000037
2025-10-21 23:13:43,884 - INFO - Batch 4800, Loss: 2.3405, LR: 0.000036
2025-10-21 23:13:59,983 - INFO - Batch 4900, Loss: 2.4174, LR: 0.000036
2025-10-21 23:14:16,031 - INFO - Batch 5000, Loss: 2.2444, LR: 0.000036
2025-10-21 23:14:32,073 - INFO - Batch 5100, Loss: 2.1561, LR: 0.000036
2025-10-21 23:14:48,089 - INFO - Batch 5200, Loss: 2.2455, LR: 0.000036
2025-10-21 23:15:04,095 - INFO - Batch 5300, Loss: 2.1805, LR: 0.000036
2025-10-21 23:15:19,939 - INFO - Batch 5400, Loss: 2.1257, LR: 0.000036
2025-10-21 23:15:35,754 - INFO - Batch 5500, Loss: 2.0366, LR: 0.000036
2025-10-21 23:15:51,531 - INFO - Batch 5600, Loss: 2.2112, LR: 0.000036
2025-10-21 23:16:07,377 - INFO - Batch 5700, Loss: 2.2669, LR: 0.000036
2025-10-21 23:16:23,392 - INFO - Batch 5800, Loss: 2.1786, LR: 0.000036
2025-10-21 23:16:39,450 - INFO - Batch 5900, Loss: 1.8611, LR: 0.000036
2025-10-21 23:16:55,466 - INFO - Batch 6000, Loss: 2.3201, LR: 0.000036
2025-10-21 23:17:11,444 - INFO - Batch 6100, Loss: 2.3301, LR: 0.000035
2025-10-21 23:17:27,475 - INFO - Batch 6200, Loss: 2.3208, LR: 0.000035
2025-10-21 23:17:43,471 - INFO - Batch 6300, Loss: 2.2224, LR: 0.000035
2025-10-21 23:17:59,547 - INFO - Batch 6400, Loss: 2.3571, LR: 0.000035
2025-10-21 23:18:07,519 - INFO - Epoch 18/30: Train Loss: 2.2068, Val Loss: 2.1406, LR: 0.000035
2025-10-21 23:18:08,081 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 23:18:08,246 - INFO - Batch 0, Loss: 2.1187, LR: 0.000035
2025-10-21 23:18:24,175 - INFO - Batch 100, Loss: 2.3256, LR: 0.000035
2025-10-21 23:18:40,112 - INFO - Batch 200, Loss: 2.2707, LR: 0.000035
2025-10-21 23:18:56,049 - INFO - Batch 300, Loss: 2.1552, LR: 0.000035
2025-10-21 23:19:12,155 - INFO - Batch 400, Loss: 2.1638, LR: 0.000035
2025-10-21 23:19:28,248 - INFO - Batch 500, Loss: 2.2281, LR: 0.000035
2025-10-21 23:19:44,366 - INFO - Batch 600, Loss: 2.1794, LR: 0.000035
2025-10-21 23:20:00,512 - INFO - Batch 700, Loss: 1.9443, LR: 0.000035
2025-10-21 23:20:16,601 - INFO - Batch 800, Loss: 2.1773, LR: 0.000035
2025-10-21 23:20:32,693 - INFO - Batch 900, Loss: 2.3190, LR: 0.000034
2025-10-21 23:20:48,818 - INFO - Batch 1000, Loss: 1.9708, LR: 0.000034
2025-10-21 23:21:04,811 - INFO - Batch 1100, Loss: 2.2644, LR: 0.000034
2025-10-21 23:21:20,710 - INFO - Batch 1200, Loss: 1.8743, LR: 0.000034
2025-10-21 23:21:36,603 - INFO - Batch 1300, Loss: 2.0098, LR: 0.000034
2025-10-21 23:21:52,629 - INFO - Batch 1400, Loss: 2.0232, LR: 0.000034
2025-10-21 23:22:08,752 - INFO - Batch 1500, Loss: 2.3014, LR: 0.000034
2025-10-21 23:22:24,859 - INFO - Batch 1600, Loss: 2.4121, LR: 0.000034
2025-10-21 23:22:40,970 - INFO - Batch 1700, Loss: 2.0694, LR: 0.000034
2025-10-21 23:22:57,107 - INFO - Batch 1800, Loss: 1.9947, LR: 0.000034
2025-10-21 23:23:13,194 - INFO - Batch 1900, Loss: 1.9979, LR: 0.000034
2025-10-21 23:23:29,333 - INFO - Batch 2000, Loss: 2.0220, LR: 0.000034
2025-10-21 23:23:45,421 - INFO - Batch 2100, Loss: 2.2093, LR: 0.000034
2025-10-21 23:24:01,366 - INFO - Batch 2200, Loss: 2.0332, LR: 0.000033
2025-10-21 23:24:17,291 - INFO - Batch 2300, Loss: 2.2264, LR: 0.000033
2025-10-21 23:24:33,198 - INFO - Batch 2400, Loss: 2.1290, LR: 0.000033
2025-10-21 23:24:49,184 - INFO - Batch 2500, Loss: 2.1271, LR: 0.000033
2025-10-21 23:25:05,308 - INFO - Batch 2600, Loss: 2.1522, LR: 0.000033
2025-10-21 23:25:21,405 - INFO - Batch 2700, Loss: 2.3266, LR: 0.000033
2025-10-21 23:25:37,521 - INFO - Batch 2800, Loss: 1.8752, LR: 0.000033
2025-10-21 23:25:53,703 - INFO - Batch 2900, Loss: 2.1506, LR: 0.000033
2025-10-21 23:26:09,805 - INFO - Batch 3000, Loss: 2.0737, LR: 0.000033
2025-10-21 23:26:25,951 - INFO - Batch 3100, Loss: 2.4050, LR: 0.000033
2025-10-21 23:26:42,053 - INFO - Batch 3200, Loss: 2.4002, LR: 0.000033
2025-10-21 23:26:58,003 - INFO - Batch 3300, Loss: 2.2434, LR: 0.000033
2025-10-21 23:27:13,900 - INFO - Batch 3400, Loss: 2.1957, LR: 0.000033
2025-10-21 23:27:29,824 - INFO - Batch 3500, Loss: 2.0879, LR: 0.000032
2025-10-21 23:27:45,901 - INFO - Batch 3600, Loss: 2.1759, LR: 0.000032
2025-10-21 23:28:02,066 - INFO - Batch 3700, Loss: 2.1342, LR: 0.000032
2025-10-21 23:28:18,187 - INFO - Batch 3800, Loss: 2.0711, LR: 0.000032
2025-10-21 23:28:34,283 - INFO - Batch 3900, Loss: 2.1144, LR: 0.000032
2025-10-21 23:28:50,459 - INFO - Batch 4000, Loss: 2.0787, LR: 0.000032
2025-10-21 23:29:06,558 - INFO - Batch 4100, Loss: 2.1712, LR: 0.000032
2025-10-21 23:29:22,685 - INFO - Batch 4200, Loss: 2.2296, LR: 0.000032
2025-10-21 23:29:38,744 - INFO - Batch 4300, Loss: 2.4884, LR: 0.000032
2025-10-21 23:29:54,616 - INFO - Batch 4400, Loss: 2.2377, LR: 0.000032
2025-10-21 23:30:10,490 - INFO - Batch 4500, Loss: 2.2485, LR: 0.000032
2025-10-21 23:30:26,386 - INFO - Batch 4600, Loss: 2.1569, LR: 0.000032
2025-10-21 23:30:42,439 - INFO - Batch 4700, Loss: 2.0393, LR: 0.000032
2025-10-21 23:30:58,557 - INFO - Batch 4800, Loss: 2.2288, LR: 0.000031
2025-10-21 23:31:14,729 - INFO - Batch 4900, Loss: 2.1225, LR: 0.000031
2025-10-21 23:31:30,857 - INFO - Batch 5000, Loss: 2.1516, LR: 0.000031
2025-10-21 23:31:46,990 - INFO - Batch 5100, Loss: 2.0433, LR: 0.000031
2025-10-21 23:32:03,132 - INFO - Batch 5200, Loss: 2.2304, LR: 0.000031
2025-10-21 23:32:19,244 - INFO - Batch 5300, Loss: 2.1714, LR: 0.000031
2025-10-21 23:32:35,297 - INFO - Batch 5400, Loss: 2.4056, LR: 0.000031
2025-10-21 23:32:51,274 - INFO - Batch 5500, Loss: 1.7752, LR: 0.000031
2025-10-21 23:33:07,184 - INFO - Batch 5600, Loss: 2.1147, LR: 0.000031
2025-10-21 23:33:23,051 - INFO - Batch 5700, Loss: 2.1872, LR: 0.000031
2025-10-21 23:33:39,172 - INFO - Batch 5800, Loss: 2.1233, LR: 0.000031
2025-10-21 23:33:55,372 - INFO - Batch 5900, Loss: 2.0416, LR: 0.000031
2025-10-21 23:34:11,465 - INFO - Batch 6000, Loss: 2.0335, LR: 0.000031
2025-10-21 23:34:27,549 - INFO - Batch 6100, Loss: 1.8819, LR: 0.000030
2025-10-21 23:34:43,776 - INFO - Batch 6200, Loss: 2.5071, LR: 0.000030
2025-10-21 23:34:59,848 - INFO - Batch 6300, Loss: 2.2303, LR: 0.000030
2025-10-21 23:35:15,923 - INFO - Batch 6400, Loss: 2.2165, LR: 0.000030
2025-10-21 23:35:23,946 - INFO - Epoch 19/30: Train Loss: 2.1897, Val Loss: 2.1202, LR: 0.000030
2025-10-21 23:35:24,205 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 23:35:24,374 - INFO - Batch 0, Loss: 2.2394, LR: 0.000030
2025-10-21 23:35:40,227 - INFO - Batch 100, Loss: 2.0708, LR: 0.000030
2025-10-21 23:35:56,075 - INFO - Batch 200, Loss: 2.1170, LR: 0.000030
2025-10-21 23:36:11,959 - INFO - Batch 300, Loss: 2.1834, LR: 0.000030
2025-10-21 23:36:28,111 - INFO - Batch 400, Loss: 2.2400, LR: 0.000030
2025-10-21 23:36:44,232 - INFO - Batch 500, Loss: 1.9663, LR: 0.000030
2025-10-21 23:37:00,384 - INFO - Batch 600, Loss: 2.1407, LR: 0.000030
2025-10-21 23:37:16,470 - INFO - Batch 700, Loss: 2.1120, LR: 0.000030
2025-10-21 23:37:32,539 - INFO - Batch 800, Loss: 2.3047, LR: 0.000030
2025-10-21 23:37:48,675 - INFO - Batch 900, Loss: 2.2137, LR: 0.000030
2025-10-21 23:38:04,911 - INFO - Batch 1000, Loss: 1.9939, LR: 0.000029
2025-10-21 23:38:20,851 - INFO - Batch 1100, Loss: 2.4554, LR: 0.000029
2025-10-21 23:38:36,655 - INFO - Batch 1200, Loss: 2.1036, LR: 0.000029
2025-10-21 23:38:52,549 - INFO - Batch 1300, Loss: 2.1571, LR: 0.000029
2025-10-21 23:39:08,447 - INFO - Batch 1400, Loss: 2.0905, LR: 0.000029
2025-10-21 23:39:24,561 - INFO - Batch 1500, Loss: 2.0050, LR: 0.000029
2025-10-21 23:39:40,697 - INFO - Batch 1600, Loss: 2.0101, LR: 0.000029
2025-10-21 23:39:56,914 - INFO - Batch 1700, Loss: 2.2478, LR: 0.000029
2025-10-21 23:40:12,985 - INFO - Batch 1800, Loss: 2.0110, LR: 0.000029
2025-10-21 23:40:29,097 - INFO - Batch 1900, Loss: 2.1596, LR: 0.000029
2025-10-21 23:40:45,419 - INFO - Batch 2000, Loss: 1.9736, LR: 0.000029
2025-10-21 23:41:01,611 - INFO - Batch 2100, Loss: 2.1859, LR: 0.000029
2025-10-21 23:41:17,603 - INFO - Batch 2200, Loss: 2.3064, LR: 0.000029
2025-10-21 23:41:33,597 - INFO - Batch 2300, Loss: 2.1260, LR: 0.000028
2025-10-21 23:41:49,468 - INFO - Batch 2400, Loss: 2.1327, LR: 0.000028
2025-10-21 23:42:05,436 - INFO - Batch 2500, Loss: 2.2198, LR: 0.000028
2025-10-21 23:42:21,621 - INFO - Batch 2600, Loss: 2.3557, LR: 0.000028
2025-10-21 23:42:37,798 - INFO - Batch 2700, Loss: 2.5918, LR: 0.000028
2025-10-21 23:42:53,956 - INFO - Batch 2800, Loss: 2.3156, LR: 0.000028
2025-10-21 23:43:10,056 - INFO - Batch 2900, Loss: 1.9982, LR: 0.000028
2025-10-21 23:43:26,157 - INFO - Batch 3000, Loss: 2.2856, LR: 0.000028
2025-10-21 23:43:42,365 - INFO - Batch 3100, Loss: 2.2083, LR: 0.000028
2025-10-21 23:43:58,582 - INFO - Batch 3200, Loss: 2.1827, LR: 0.000028
2025-10-21 23:44:14,555 - INFO - Batch 3300, Loss: 2.3841, LR: 0.000028
2025-10-21 23:44:30,510 - INFO - Batch 3400, Loss: 2.1576, LR: 0.000028
2025-10-21 23:44:46,654 - INFO - Batch 3500, Loss: 2.4174, LR: 0.000028
2025-10-21 23:45:02,792 - INFO - Batch 3600, Loss: 2.4433, LR: 0.000028
2025-10-21 23:45:18,951 - INFO - Batch 3700, Loss: 1.9157, LR: 0.000027
2025-10-21 23:45:35,113 - INFO - Batch 3800, Loss: 2.0740, LR: 0.000027
2025-10-21 23:45:51,338 - INFO - Batch 3900, Loss: 2.1694, LR: 0.000027
2025-10-21 23:46:07,411 - INFO - Batch 4000, Loss: 2.1799, LR: 0.000027
2025-10-21 23:46:23,531 - INFO - Batch 4100, Loss: 2.1659, LR: 0.000027
2025-10-21 23:46:39,815 - INFO - Batch 4200, Loss: 2.2144, LR: 0.000027
2025-10-21 23:46:55,889 - INFO - Batch 4300, Loss: 2.3753, LR: 0.000027
2025-10-21 23:47:11,792 - INFO - Batch 4400, Loss: 2.4591, LR: 0.000027
2025-10-21 23:47:27,838 - INFO - Batch 4500, Loss: 2.0311, LR: 0.000027
2025-10-21 23:47:43,934 - INFO - Batch 4600, Loss: 2.4691, LR: 0.000027
2025-10-21 23:48:00,077 - INFO - Batch 4700, Loss: 2.3683, LR: 0.000027
2025-10-21 23:48:16,291 - INFO - Batch 4800, Loss: 2.0193, LR: 0.000027
2025-10-21 23:48:32,472 - INFO - Batch 4900, Loss: 2.1788, LR: 0.000027
2025-10-21 23:48:48,639 - INFO - Batch 5000, Loss: 2.2072, LR: 0.000027
2025-10-21 23:49:04,810 - INFO - Batch 5100, Loss: 2.1272, LR: 0.000026
2025-10-21 23:49:20,909 - INFO - Batch 5200, Loss: 2.2422, LR: 0.000026
2025-10-21 23:49:37,120 - INFO - Batch 5300, Loss: 2.0365, LR: 0.000026
2025-10-21 23:49:53,213 - INFO - Batch 5400, Loss: 2.1355, LR: 0.000026
2025-10-21 23:50:09,146 - INFO - Batch 5500, Loss: 2.3022, LR: 0.000026
2025-10-21 23:50:25,089 - INFO - Batch 5600, Loss: 2.1097, LR: 0.000026
2025-10-21 23:50:41,251 - INFO - Batch 5700, Loss: 2.0659, LR: 0.000026
2025-10-21 23:50:57,376 - INFO - Batch 5800, Loss: 2.1292, LR: 0.000026
2025-10-21 23:51:13,505 - INFO - Batch 5900, Loss: 2.2567, LR: 0.000026
2025-10-21 23:51:29,678 - INFO - Batch 6000, Loss: 2.2159, LR: 0.000026
2025-10-21 23:51:46,008 - INFO - Batch 6100, Loss: 1.9605, LR: 0.000026
2025-10-21 23:52:02,250 - INFO - Batch 6200, Loss: 2.3025, LR: 0.000026
2025-10-21 23:52:18,456 - INFO - Batch 6300, Loss: 2.2546, LR: 0.000026
2025-10-21 23:52:34,773 - INFO - Batch 6400, Loss: 2.1946, LR: 0.000026
2025-10-21 23:52:42,829 - INFO - Epoch 20/30: Train Loss: 2.1746, Val Loss: 2.1079, LR: 0.000025
2025-10-21 23:52:43,097 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-21 23:52:43,311 - INFO - 模型已保存到: ./checkpoints/checkpoint_epoch_20.pth
2025-10-21 23:52:43,474 - INFO - Batch 0, Loss: 2.0328, LR: 0.000025
2025-10-21 23:52:59,547 - INFO - Batch 100, Loss: 2.0562, LR: 0.000025
2025-10-21 23:53:15,666 - INFO - Batch 200, Loss: 2.2489, LR: 0.000025
2025-10-21 23:53:31,903 - INFO - Batch 300, Loss: 1.9901, LR: 0.000025
2025-10-21 23:53:48,115 - INFO - Batch 400, Loss: 2.2762, LR: 0.000025
2025-10-21 23:54:04,320 - INFO - Batch 500, Loss: 2.2997, LR: 0.000025
2025-10-21 23:54:20,516 - INFO - Batch 600, Loss: 2.1194, LR: 0.000025
2025-10-21 23:54:36,805 - INFO - Batch 700, Loss: 2.2370, LR: 0.000025
2025-10-21 23:54:53,029 - INFO - Batch 800, Loss: 2.1472, LR: 0.000025
2025-10-21 23:55:09,242 - INFO - Batch 900, Loss: 2.2520, LR: 0.000025
2025-10-21 23:55:25,324 - INFO - Batch 1000, Loss: 2.1244, LR: 0.000025
2025-10-21 23:55:41,316 - INFO - Batch 1100, Loss: 2.0231, LR: 0.000025
2025-10-21 23:55:57,350 - INFO - Batch 1200, Loss: 2.1639, LR: 0.000025
2025-10-21 23:56:13,370 - INFO - Batch 1300, Loss: 2.1268, LR: 0.000025
2025-10-21 23:56:29,703 - INFO - Batch 1400, Loss: 1.9037, LR: 0.000024
2025-10-21 23:56:45,922 - INFO - Batch 1500, Loss: 2.0724, LR: 0.000024
2025-10-21 23:57:02,082 - INFO - Batch 1600, Loss: 1.9699, LR: 0.000024
2025-10-21 23:57:18,357 - INFO - Batch 1700, Loss: 1.9541, LR: 0.000024
2025-10-21 23:57:34,582 - INFO - Batch 1800, Loss: 2.3390, LR: 0.000024
2025-10-21 23:57:50,779 - INFO - Batch 1900, Loss: 2.0217, LR: 0.000024
2025-10-21 23:58:06,905 - INFO - Batch 2000, Loss: 2.0000, LR: 0.000024
2025-10-21 23:58:22,837 - INFO - Batch 2100, Loss: 2.2709, LR: 0.000024
2025-10-21 23:58:38,908 - INFO - Batch 2200, Loss: 2.0489, LR: 0.000024
2025-10-21 23:58:55,022 - INFO - Batch 2300, Loss: 2.1617, LR: 0.000024
2025-10-21 23:59:11,241 - INFO - Batch 2400, Loss: 1.9440, LR: 0.000024
2025-10-21 23:59:27,510 - INFO - Batch 2500, Loss: 2.0751, LR: 0.000024
2025-10-21 23:59:43,703 - INFO - Batch 2600, Loss: 2.1606, LR: 0.000024
2025-10-21 23:59:59,905 - INFO - Batch 2700, Loss: 1.9147, LR: 0.000024
2025-10-22 00:00:16,169 - INFO - Batch 2800, Loss: 2.2134, LR: 0.000023
2025-10-22 00:00:32,428 - INFO - Batch 2900, Loss: 2.1797, LR: 0.000023
2025-10-22 00:00:48,635 - INFO - Batch 3000, Loss: 2.0230, LR: 0.000023
2025-10-22 00:01:04,793 - INFO - Batch 3100, Loss: 2.2085, LR: 0.000023
2025-10-22 00:01:20,895 - INFO - Batch 3200, Loss: 2.0380, LR: 0.000023
2025-10-22 00:01:36,962 - INFO - Batch 3300, Loss: 2.3404, LR: 0.000023
2025-10-22 00:01:53,146 - INFO - Batch 3400, Loss: 2.2364, LR: 0.000023
2025-10-22 00:02:09,406 - INFO - Batch 3500, Loss: 2.2265, LR: 0.000023
2025-10-22 00:02:25,657 - INFO - Batch 3600, Loss: 2.2381, LR: 0.000023
2025-10-22 00:02:41,850 - INFO - Batch 3700, Loss: 2.0290, LR: 0.000023
2025-10-22 00:02:58,003 - INFO - Batch 3800, Loss: 2.3591, LR: 0.000023
2025-10-22 00:03:14,267 - INFO - Batch 3900, Loss: 2.0847, LR: 0.000023
2025-10-22 00:03:30,471 - INFO - Batch 4000, Loss: 2.0617, LR: 0.000023
2025-10-22 00:03:46,612 - INFO - Batch 4100, Loss: 2.2043, LR: 0.000023
2025-10-22 00:04:02,721 - INFO - Batch 4200, Loss: 1.9879, LR: 0.000023
2025-10-22 00:04:18,836 - INFO - Batch 4300, Loss: 2.1175, LR: 0.000022
2025-10-22 00:04:34,914 - INFO - Batch 4400, Loss: 1.9707, LR: 0.000022
2025-10-22 00:04:51,098 - INFO - Batch 4500, Loss: 2.0370, LR: 0.000022
2025-10-22 00:05:07,470 - INFO - Batch 4600, Loss: 2.0575, LR: 0.000022
2025-10-22 00:05:23,665 - INFO - Batch 4700, Loss: 2.2282, LR: 0.000022
2025-10-22 00:05:39,864 - INFO - Batch 4800, Loss: 1.9016, LR: 0.000022
2025-10-22 00:05:56,082 - INFO - Batch 4900, Loss: 1.8881, LR: 0.000022
2025-10-22 00:06:12,347 - INFO - Batch 5000, Loss: 2.0702, LR: 0.000022
2025-10-22 00:06:28,477 - INFO - Batch 5100, Loss: 2.2472, LR: 0.000022
2025-10-22 00:06:44,546 - INFO - Batch 5200, Loss: 2.1371, LR: 0.000022
2025-10-22 00:07:00,707 - INFO - Batch 5300, Loss: 2.2996, LR: 0.000022
2025-10-22 00:07:16,819 - INFO - Batch 5400, Loss: 2.2533, LR: 0.000022
2025-10-22 00:07:32,891 - INFO - Batch 5500, Loss: 2.2243, LR: 0.000022
2025-10-22 00:07:49,127 - INFO - Batch 5600, Loss: 2.0006, LR: 0.000022
2025-10-22 00:08:05,450 - INFO - Batch 5700, Loss: 2.1785, LR: 0.000022
2025-10-22 00:08:21,598 - INFO - Batch 5800, Loss: 2.1589, LR: 0.000021
2025-10-22 00:08:37,778 - INFO - Batch 5900, Loss: 2.2322, LR: 0.000021
2025-10-22 00:08:54,073 - INFO - Batch 6000, Loss: 2.2570, LR: 0.000021
2025-10-22 00:09:10,286 - INFO - Batch 6100, Loss: 2.1474, LR: 0.000021
2025-10-22 00:09:26,382 - INFO - Batch 6200, Loss: 2.0596, LR: 0.000021
2025-10-22 00:09:42,483 - INFO - Batch 6300, Loss: 2.5126, LR: 0.000021
2025-10-22 00:09:58,618 - INFO - Batch 6400, Loss: 2.3412, LR: 0.000021
2025-10-22 00:10:06,653 - INFO - Epoch 21/30: Train Loss: 2.1613, Val Loss: 2.1014, LR: 0.000021
2025-10-22 00:10:06,936 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-22 00:10:07,107 - INFO - Batch 0, Loss: 2.2709, LR: 0.000021
2025-10-22 00:10:23,278 - INFO - Batch 100, Loss: 2.2831, LR: 0.000021
2025-10-22 00:10:39,434 - INFO - Batch 200, Loss: 2.1002, LR: 0.000021
2025-10-22 00:10:55,742 - INFO - Batch 300, Loss: 1.9817, LR: 0.000021
2025-10-22 00:11:11,916 - INFO - Batch 400, Loss: 2.1212, LR: 0.000021
2025-10-22 00:11:28,116 - INFO - Batch 500, Loss: 2.3084, LR: 0.000021
2025-10-22 00:11:44,347 - INFO - Batch 600, Loss: 2.0128, LR: 0.000021
2025-10-22 00:12:00,574 - INFO - Batch 700, Loss: 2.0956, LR: 0.000021
2025-10-22 00:12:16,561 - INFO - Batch 800, Loss: 2.0833, LR: 0.000020
2025-10-22 00:12:32,640 - INFO - Batch 900, Loss: 2.1034, LR: 0.000020
2025-10-22 00:12:48,594 - INFO - Batch 1000, Loss: 2.1683, LR: 0.000020
2025-10-22 00:13:04,695 - INFO - Batch 1100, Loss: 2.1469, LR: 0.000020
2025-10-22 00:13:20,810 - INFO - Batch 1200, Loss: 2.1583, LR: 0.000020
2025-10-22 00:13:36,980 - INFO - Batch 1300, Loss: 2.1004, LR: 0.000020
2025-10-22 00:13:53,326 - INFO - Batch 1400, Loss: 2.1338, LR: 0.000020
2025-10-22 00:14:09,628 - INFO - Batch 1500, Loss: 2.2690, LR: 0.000020
2025-10-22 00:14:25,948 - INFO - Batch 1600, Loss: 1.9305, LR: 0.000020
2025-10-22 00:14:42,281 - INFO - Batch 1700, Loss: 2.1849, LR: 0.000020
2025-10-22 00:14:58,333 - INFO - Batch 1800, Loss: 2.2321, LR: 0.000020
2025-10-22 00:15:14,139 - INFO - Batch 1900, Loss: 1.9011, LR: 0.000020
2025-10-22 00:15:29,939 - INFO - Batch 2000, Loss: 2.1910, LR: 0.000020
2025-10-22 00:15:45,868 - INFO - Batch 2100, Loss: 2.1524, LR: 0.000020
2025-10-22 00:16:02,190 - INFO - Batch 2200, Loss: 1.8070, LR: 0.000020
2025-10-22 00:16:18,523 - INFO - Batch 2300, Loss: 2.2273, LR: 0.000019
2025-10-22 00:16:34,844 - INFO - Batch 2400, Loss: 1.9137, LR: 0.000019
2025-10-22 00:16:51,184 - INFO - Batch 2500, Loss: 2.3245, LR: 0.000019
2025-10-22 00:17:07,515 - INFO - Batch 2600, Loss: 2.0106, LR: 0.000019
2025-10-22 00:17:23,806 - INFO - Batch 2700, Loss: 2.2035, LR: 0.000019
2025-10-22 00:17:40,036 - INFO - Batch 2800, Loss: 2.2290, LR: 0.000019
2025-10-22 00:17:56,035 - INFO - Batch 2900, Loss: 2.1814, LR: 0.000019
2025-10-22 00:18:12,057 - INFO - Batch 3000, Loss: 2.3765, LR: 0.000019
2025-10-22 00:18:28,111 - INFO - Batch 3100, Loss: 2.4465, LR: 0.000019
2025-10-22 00:18:44,341 - INFO - Batch 3200, Loss: 2.1170, LR: 0.000019
2025-10-22 00:19:00,585 - INFO - Batch 3300, Loss: 2.4224, LR: 0.000019
2025-10-22 00:19:16,787 - INFO - Batch 3400, Loss: 2.1707, LR: 0.000019
2025-10-22 00:19:33,038 - INFO - Batch 3500, Loss: 2.1594, LR: 0.000019
2025-10-22 00:19:49,232 - INFO - Batch 3600, Loss: 2.0582, LR: 0.000019
2025-10-22 00:20:05,485 - INFO - Batch 3700, Loss: 1.9758, LR: 0.000019
2025-10-22 00:20:21,618 - INFO - Batch 3800, Loss: 2.3299, LR: 0.000019
2025-10-22 00:20:37,668 - INFO - Batch 3900, Loss: 2.1291, LR: 0.000018
2025-10-22 00:20:53,603 - INFO - Batch 4000, Loss: 2.2549, LR: 0.000018
2025-10-22 00:21:09,716 - INFO - Batch 4100, Loss: 2.1261, LR: 0.000018
2025-10-22 00:21:25,921 - INFO - Batch 4200, Loss: 2.2536, LR: 0.000018
2025-10-22 00:21:42,170 - INFO - Batch 4300, Loss: 2.3187, LR: 0.000018
2025-10-22 00:21:58,377 - INFO - Batch 4400, Loss: 1.9258, LR: 0.000018
2025-10-22 00:22:14,607 - INFO - Batch 4500, Loss: 2.1332, LR: 0.000018
2025-10-22 00:22:30,856 - INFO - Batch 4600, Loss: 2.2052, LR: 0.000018
2025-10-22 00:22:47,099 - INFO - Batch 4700, Loss: 2.1386, LR: 0.000018
2025-10-22 00:23:03,224 - INFO - Batch 4800, Loss: 2.1454, LR: 0.000018
2025-10-22 00:23:19,220 - INFO - Batch 4900, Loss: 2.1038, LR: 0.000018
2025-10-22 00:23:35,251 - INFO - Batch 5000, Loss: 2.2809, LR: 0.000018
2025-10-22 00:23:51,389 - INFO - Batch 5100, Loss: 2.3738, LR: 0.000018
2025-10-22 00:24:07,654 - INFO - Batch 5200, Loss: 2.1567, LR: 0.000018
2025-10-22 00:24:23,886 - INFO - Batch 5300, Loss: 1.9467, LR: 0.000018
2025-10-22 00:24:40,109 - INFO - Batch 5400, Loss: 2.0432, LR: 0.000018
2025-10-22 00:24:56,329 - INFO - Batch 5500, Loss: 2.3805, LR: 0.000017
2025-10-22 00:25:12,565 - INFO - Batch 5600, Loss: 1.9137, LR: 0.000017
2025-10-22 00:25:28,809 - INFO - Batch 5700, Loss: 2.0548, LR: 0.000017
2025-10-22 00:25:44,909 - INFO - Batch 5800, Loss: 2.0924, LR: 0.000017
2025-10-22 00:26:00,916 - INFO - Batch 5900, Loss: 1.9540, LR: 0.000017
2025-10-22 00:26:16,922 - INFO - Batch 6000, Loss: 2.1137, LR: 0.000017
2025-10-22 00:26:33,032 - INFO - Batch 6100, Loss: 2.2498, LR: 0.000017
2025-10-22 00:26:49,273 - INFO - Batch 6200, Loss: 2.2789, LR: 0.000017
2025-10-22 00:27:05,481 - INFO - Batch 6300, Loss: 2.2799, LR: 0.000017
2025-10-22 00:27:21,701 - INFO - Batch 6400, Loss: 1.9588, LR: 0.000017
2025-10-22 00:27:29,802 - INFO - Epoch 22/30: Train Loss: 2.1472, Val Loss: 2.1004, LR: 0.000017
2025-10-22 00:27:30,075 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-22 00:27:30,252 - INFO - Batch 0, Loss: 2.0537, LR: 0.000017
2025-10-22 00:27:46,135 - INFO - Batch 100, Loss: 2.2789, LR: 0.000017
2025-10-22 00:28:02,467 - INFO - Batch 200, Loss: 2.1120, LR: 0.000017
2025-10-22 00:28:18,716 - INFO - Batch 300, Loss: 2.1006, LR: 0.000017
2025-10-22 00:28:34,722 - INFO - Batch 400, Loss: 2.3963, LR: 0.000017
2025-10-22 00:28:50,665 - INFO - Batch 500, Loss: 1.8243, LR: 0.000017
2025-10-22 00:29:06,636 - INFO - Batch 600, Loss: 2.2394, LR: 0.000017
2025-10-22 00:29:22,974 - INFO - Batch 700, Loss: 2.0660, LR: 0.000016
2025-10-22 00:29:39,329 - INFO - Batch 800, Loss: 2.0084, LR: 0.000016
2025-10-22 00:29:55,389 - INFO - Batch 900, Loss: 2.2199, LR: 0.000016
2025-10-22 00:30:11,723 - INFO - Batch 1000, Loss: 1.9508, LR: 0.000016
2025-10-22 00:30:28,024 - INFO - Batch 1100, Loss: 1.9966, LR: 0.000016
2025-10-22 00:30:44,352 - INFO - Batch 1200, Loss: 2.1039, LR: 0.000016
2025-10-22 00:31:00,598 - INFO - Batch 1300, Loss: 1.8040, LR: 0.000016
2025-10-22 00:31:16,470 - INFO - Batch 1400, Loss: 2.3294, LR: 0.000016
2025-10-22 00:31:32,309 - INFO - Batch 1500, Loss: 2.1819, LR: 0.000016
2025-10-22 00:31:48,167 - INFO - Batch 1600, Loss: 2.0886, LR: 0.000016
2025-10-22 00:32:04,485 - INFO - Batch 1700, Loss: 2.0984, LR: 0.000016
2025-10-22 00:32:20,819 - INFO - Batch 1800, Loss: 2.2302, LR: 0.000016
2025-10-22 00:32:37,135 - INFO - Batch 1900, Loss: 2.1250, LR: 0.000016
2025-10-22 00:32:53,434 - INFO - Batch 2000, Loss: 2.1277, LR: 0.000016
2025-10-22 00:33:09,775 - INFO - Batch 2100, Loss: 2.2300, LR: 0.000016
2025-10-22 00:33:26,129 - INFO - Batch 2200, Loss: 1.8276, LR: 0.000016
2025-10-22 00:33:42,385 - INFO - Batch 2300, Loss: 1.8497, LR: 0.000015
2025-10-22 00:33:58,497 - INFO - Batch 2400, Loss: 2.1629, LR: 0.000015
2025-10-22 00:34:14,434 - INFO - Batch 2500, Loss: 2.2772, LR: 0.000015
2025-10-22 00:34:30,249 - INFO - Batch 2600, Loss: 1.7291, LR: 0.000015
2025-10-22 00:34:46,574 - INFO - Batch 2700, Loss: 1.9822, LR: 0.000015
2025-10-22 00:35:02,917 - INFO - Batch 2800, Loss: 2.4436, LR: 0.000015
2025-10-22 00:35:19,209 - INFO - Batch 2900, Loss: 2.0068, LR: 0.000015
2025-10-22 00:35:35,496 - INFO - Batch 3000, Loss: 2.1300, LR: 0.000015
2025-10-22 00:35:51,811 - INFO - Batch 3100, Loss: 1.9589, LR: 0.000015
2025-10-22 00:36:08,143 - INFO - Batch 3200, Loss: 2.2634, LR: 0.000015
2025-10-22 00:36:24,422 - INFO - Batch 3300, Loss: 2.1314, LR: 0.000015
2025-10-22 00:36:40,565 - INFO - Batch 3400, Loss: 2.2474, LR: 0.000015
2025-10-22 00:36:56,653 - INFO - Batch 3500, Loss: 2.1177, LR: 0.000015
2025-10-22 00:37:12,815 - INFO - Batch 3600, Loss: 2.0937, LR: 0.000015
2025-10-22 00:37:29,065 - INFO - Batch 3700, Loss: 2.0838, LR: 0.000015
2025-10-22 00:37:45,355 - INFO - Batch 3800, Loss: 2.0695, LR: 0.000015
2025-10-22 00:38:01,660 - INFO - Batch 3900, Loss: 2.1724, LR: 0.000015
2025-10-22 00:38:17,971 - INFO - Batch 4000, Loss: 2.2150, LR: 0.000014
2025-10-22 00:38:34,278 - INFO - Batch 4100, Loss: 2.5187, LR: 0.000014
2025-10-22 00:38:50,612 - INFO - Batch 4200, Loss: 2.1102, LR: 0.000014
2025-10-22 00:39:06,829 - INFO - Batch 4300, Loss: 2.1758, LR: 0.000014
2025-10-22 00:39:22,842 - INFO - Batch 4400, Loss: 2.1056, LR: 0.000014
2025-10-22 00:39:38,963 - INFO - Batch 4500, Loss: 2.1635, LR: 0.000014
2025-10-22 00:39:55,152 - INFO - Batch 4600, Loss: 2.2326, LR: 0.000014
2025-10-22 00:40:11,455 - INFO - Batch 4700, Loss: 2.1221, LR: 0.000014
2025-10-22 00:40:27,771 - INFO - Batch 4800, Loss: 1.9590, LR: 0.000014
2025-10-22 00:40:43,942 - INFO - Batch 4900, Loss: 2.1384, LR: 0.000014
2025-10-22 00:41:00,193 - INFO - Batch 5000, Loss: 2.0911, LR: 0.000014
2025-10-22 00:41:16,432 - INFO - Batch 5100, Loss: 2.0966, LR: 0.000014
2025-10-22 00:41:32,559 - INFO - Batch 5200, Loss: 2.2384, LR: 0.000014
2025-10-22 00:41:48,772 - INFO - Batch 5300, Loss: 2.0828, LR: 0.000014
2025-10-22 00:42:04,709 - INFO - Batch 5400, Loss: 2.1223, LR: 0.000014
2025-10-22 00:42:20,784 - INFO - Batch 5500, Loss: 2.1213, LR: 0.000014
2025-10-22 00:42:36,824 - INFO - Batch 5600, Loss: 1.9108, LR: 0.000014
2025-10-22 00:42:53,026 - INFO - Batch 5700, Loss: 2.0387, LR: 0.000014
2025-10-22 00:43:09,275 - INFO - Batch 5800, Loss: 2.1278, LR: 0.000013
2025-10-22 00:43:25,526 - INFO - Batch 5900, Loss: 2.1030, LR: 0.000013
2025-10-22 00:43:41,796 - INFO - Batch 6000, Loss: 2.0652, LR: 0.000013
2025-10-22 00:43:57,992 - INFO - Batch 6100, Loss: 2.2147, LR: 0.000013
2025-10-22 00:44:14,277 - INFO - Batch 6200, Loss: 2.2513, LR: 0.000013
2025-10-22 00:44:30,340 - INFO - Batch 6300, Loss: 2.0654, LR: 0.000013
2025-10-22 00:44:46,159 - INFO - Batch 6400, Loss: 2.0857, LR: 0.000013
2025-10-22 00:44:54,098 - INFO - Epoch 23/30: Train Loss: 2.1359, Val Loss: 2.0890, LR: 0.000013
2025-10-22 00:44:54,371 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-22 00:44:54,539 - INFO - Batch 0, Loss: 2.1808, LR: 0.000013
2025-10-22 00:45:10,484 - INFO - Batch 100, Loss: 1.9224, LR: 0.000013
2025-10-22 00:45:26,640 - INFO - Batch 200, Loss: 1.9990, LR: 0.000013
2025-10-22 00:45:42,974 - INFO - Batch 300, Loss: 1.9726, LR: 0.000013
2025-10-22 00:45:59,275 - INFO - Batch 400, Loss: 2.1704, LR: 0.000013
2025-10-22 00:46:15,578 - INFO - Batch 500, Loss: 1.9098, LR: 0.000013
2025-10-22 00:46:31,819 - INFO - Batch 600, Loss: 2.1245, LR: 0.000013
2025-10-22 00:46:47,788 - INFO - Batch 700, Loss: 1.9335, LR: 0.000013
2025-10-22 00:47:04,100 - INFO - Batch 800, Loss: 2.1797, LR: 0.000013
2025-10-22 00:47:20,210 - INFO - Batch 900, Loss: 2.0589, LR: 0.000013
2025-10-22 00:47:36,016 - INFO - Batch 1000, Loss: 1.8989, LR: 0.000013
2025-10-22 00:47:51,754 - INFO - Batch 1100, Loss: 2.2952, LR: 0.000012
2025-10-22 00:48:07,720 - INFO - Batch 1200, Loss: 2.1916, LR: 0.000012
2025-10-22 00:48:23,968 - INFO - Batch 1300, Loss: 2.1405, LR: 0.000012
2025-10-22 00:48:40,253 - INFO - Batch 1400, Loss: 2.0916, LR: 0.000012
2025-10-22 00:48:56,544 - INFO - Batch 1500, Loss: 2.0370, LR: 0.000012
2025-10-22 00:49:12,770 - INFO - Batch 1600, Loss: 1.9740, LR: 0.000012
2025-10-22 00:49:28,977 - INFO - Batch 1700, Loss: 1.9983, LR: 0.000012
2025-10-22 00:49:45,202 - INFO - Batch 1800, Loss: 2.0642, LR: 0.000012
2025-10-22 00:50:01,411 - INFO - Batch 1900, Loss: 2.0521, LR: 0.000012
2025-10-22 00:50:17,506 - INFO - Batch 2000, Loss: 2.0344, LR: 0.000012
2025-10-22 00:50:33,571 - INFO - Batch 2100, Loss: 2.2534, LR: 0.000012
2025-10-22 00:50:49,748 - INFO - Batch 2200, Loss: 2.2611, LR: 0.000012
2025-10-22 00:51:05,914 - INFO - Batch 2300, Loss: 2.2639, LR: 0.000012
2025-10-22 00:51:22,093 - INFO - Batch 2400, Loss: 2.0073, LR: 0.000012
2025-10-22 00:51:38,301 - INFO - Batch 2500, Loss: 1.9228, LR: 0.000012
2025-10-22 00:51:54,581 - INFO - Batch 2600, Loss: 2.1441, LR: 0.000012
2025-10-22 00:52:10,778 - INFO - Batch 2700, Loss: 2.0826, LR: 0.000012
2025-10-22 00:52:27,004 - INFO - Batch 2800, Loss: 2.0005, LR: 0.000012
2025-10-22 00:52:43,207 - INFO - Batch 2900, Loss: 1.9255, LR: 0.000012
2025-10-22 00:52:59,195 - INFO - Batch 3000, Loss: 2.2258, LR: 0.000011
2025-10-22 00:53:15,255 - INFO - Batch 3100, Loss: 2.2064, LR: 0.000011
2025-10-22 00:53:31,321 - INFO - Batch 3200, Loss: 2.4033, LR: 0.000011
2025-10-22 00:53:47,592 - INFO - Batch 3300, Loss: 2.1395, LR: 0.000011
2025-10-22 00:54:03,854 - INFO - Batch 3400, Loss: 1.8905, LR: 0.000011
2025-10-22 00:54:20,156 - INFO - Batch 3500, Loss: 2.3219, LR: 0.000011
2025-10-22 00:54:36,338 - INFO - Batch 3600, Loss: 1.6881, LR: 0.000011
2025-10-22 00:54:52,565 - INFO - Batch 3700, Loss: 2.1393, LR: 0.000011
2025-10-22 00:55:08,750 - INFO - Batch 3800, Loss: 2.2786, LR: 0.000011
2025-10-22 00:55:24,701 - INFO - Batch 3900, Loss: 2.1092, LR: 0.000011
2025-10-22 00:55:40,478 - INFO - Batch 4000, Loss: 1.9386, LR: 0.000011
2025-10-22 00:55:56,253 - INFO - Batch 4100, Loss: 2.2182, LR: 0.000011
2025-10-22 00:56:12,083 - INFO - Batch 4200, Loss: 2.0694, LR: 0.000011
2025-10-22 00:56:28,229 - INFO - Batch 4300, Loss: 2.2235, LR: 0.000011
2025-10-22 00:56:44,431 - INFO - Batch 4400, Loss: 1.9907, LR: 0.000011
2025-10-22 00:57:00,619 - INFO - Batch 4500, Loss: 2.2264, LR: 0.000011
2025-10-22 00:57:16,890 - INFO - Batch 4600, Loss: 2.1159, LR: 0.000011
2025-10-22 00:57:33,139 - INFO - Batch 4700, Loss: 1.9475, LR: 0.000011
2025-10-22 00:57:49,384 - INFO - Batch 4800, Loss: 2.0100, LR: 0.000011
2025-10-22 00:58:05,548 - INFO - Batch 4900, Loss: 2.0553, LR: 0.000011
2025-10-22 00:58:21,308 - INFO - Batch 5000, Loss: 1.9677, LR: 0.000010
2025-10-22 00:58:37,069 - INFO - Batch 5100, Loss: 2.0847, LR: 0.000010
2025-10-22 00:58:52,817 - INFO - Batch 5200, Loss: 2.0496, LR: 0.000010
2025-10-22 00:59:08,770 - INFO - Batch 5300, Loss: 2.1627, LR: 0.000010
2025-10-22 00:59:24,975 - INFO - Batch 5400, Loss: 2.0144, LR: 0.000010
2025-10-22 00:59:41,284 - INFO - Batch 5500, Loss: 2.0423, LR: 0.000010
2025-10-22 00:59:57,535 - INFO - Batch 5600, Loss: 2.0251, LR: 0.000010
2025-10-22 01:00:13,797 - INFO - Batch 5700, Loss: 2.2277, LR: 0.000010
2025-10-22 01:00:30,000 - INFO - Batch 5800, Loss: 2.2530, LR: 0.000010
2025-10-22 01:00:46,229 - INFO - Batch 5900, Loss: 2.3655, LR: 0.000010
2025-10-22 01:01:02,255 - INFO - Batch 6000, Loss: 2.0086, LR: 0.000010
2025-10-22 01:01:18,300 - INFO - Batch 6100, Loss: 2.4643, LR: 0.000010
2025-10-22 01:01:34,376 - INFO - Batch 6200, Loss: 2.1778, LR: 0.000010
2025-10-22 01:01:50,528 - INFO - Batch 6300, Loss: 2.0586, LR: 0.000010
2025-10-22 01:02:06,756 - INFO - Batch 6400, Loss: 2.0414, LR: 0.000010
2025-10-22 01:02:14,837 - INFO - Epoch 24/30: Train Loss: 2.1251, Val Loss: 2.0887, LR: 0.000010
2025-10-22 01:02:15,103 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-22 01:02:15,270 - INFO - Batch 0, Loss: 2.1644, LR: 0.000010
2025-10-22 01:02:31,397 - INFO - Batch 100, Loss: 2.0121, LR: 0.000010
2025-10-22 01:02:47,542 - INFO - Batch 200, Loss: 2.1090, LR: 0.000010
2025-10-22 01:03:03,745 - INFO - Batch 300, Loss: 2.2068, LR: 0.000010
2025-10-22 01:03:19,948 - INFO - Batch 400, Loss: 2.0891, LR: 0.000010
2025-10-22 01:03:35,969 - INFO - Batch 500, Loss: 2.2009, LR: 0.000010
2025-10-22 01:03:51,894 - INFO - Batch 600, Loss: 2.1313, LR: 0.000009
2025-10-22 01:04:07,561 - INFO - Batch 700, Loss: 2.2137, LR: 0.000009
2025-10-22 01:04:23,213 - INFO - Batch 800, Loss: 2.1744, LR: 0.000009
2025-10-22 01:04:39,287 - INFO - Batch 900, Loss: 2.0292, LR: 0.000009
2025-10-22 01:04:55,386 - INFO - Batch 1000, Loss: 2.1357, LR: 0.000009
2025-10-22 01:05:11,509 - INFO - Batch 1100, Loss: 2.0759, LR: 0.000009
2025-10-22 01:05:27,667 - INFO - Batch 1200, Loss: 2.2485, LR: 0.000009
2025-10-22 01:05:43,822 - INFO - Batch 1300, Loss: 2.1837, LR: 0.000009
2025-10-22 01:05:59,949 - INFO - Batch 1400, Loss: 1.9336, LR: 0.000009
2025-10-22 01:06:16,015 - INFO - Batch 1500, Loss: 2.1169, LR: 0.000009
2025-10-22 01:06:31,873 - INFO - Batch 1600, Loss: 2.2122, LR: 0.000009
2025-10-22 01:06:47,597 - INFO - Batch 1700, Loss: 2.1536, LR: 0.000009
2025-10-22 01:07:03,252 - INFO - Batch 1800, Loss: 2.3133, LR: 0.000009
2025-10-22 01:07:19,087 - INFO - Batch 1900, Loss: 2.3193, LR: 0.000009
2025-10-22 01:07:35,193 - INFO - Batch 2000, Loss: 2.2530, LR: 0.000009
2025-10-22 01:07:51,247 - INFO - Batch 2100, Loss: 1.9149, LR: 0.000009
2025-10-22 01:08:07,415 - INFO - Batch 2200, Loss: 2.1890, LR: 0.000009
2025-10-22 01:08:23,577 - INFO - Batch 2300, Loss: 2.4618, LR: 0.000009
2025-10-22 01:08:39,726 - INFO - Batch 2400, Loss: 2.1512, LR: 0.000009
2025-10-22 01:08:55,885 - INFO - Batch 2500, Loss: 2.2784, LR: 0.000009
2025-10-22 01:09:12,005 - INFO - Batch 2600, Loss: 1.8882, LR: 0.000009
2025-10-22 01:09:28,019 - INFO - Batch 2700, Loss: 2.2655, LR: 0.000008
2025-10-22 01:09:44,013 - INFO - Batch 2800, Loss: 2.3073, LR: 0.000008
2025-10-22 01:10:00,083 - INFO - Batch 2900, Loss: 2.0160, LR: 0.000008
2025-10-22 01:10:16,258 - INFO - Batch 3000, Loss: 2.1306, LR: 0.000008
2025-10-22 01:10:32,472 - INFO - Batch 3100, Loss: 1.9639, LR: 0.000008
2025-10-22 01:10:48,587 - INFO - Batch 3200, Loss: 2.2611, LR: 0.000008
2025-10-22 01:11:04,749 - INFO - Batch 3300, Loss: 1.9105, LR: 0.000008
2025-10-22 01:11:20,813 - INFO - Batch 3400, Loss: 2.1816, LR: 0.000008
2025-10-22 01:11:36,871 - INFO - Batch 3500, Loss: 2.2996, LR: 0.000008
2025-10-22 01:11:52,799 - INFO - Batch 3600, Loss: 2.1823, LR: 0.000008
2025-10-22 01:12:08,617 - INFO - Batch 3700, Loss: 2.1710, LR: 0.000008
2025-10-22 01:12:24,405 - INFO - Batch 3800, Loss: 1.9555, LR: 0.000008
2025-10-22 01:12:40,286 - INFO - Batch 3900, Loss: 2.3462, LR: 0.000008
2025-10-22 01:12:56,306 - INFO - Batch 4000, Loss: 2.0608, LR: 0.000008
2025-10-22 01:13:12,440 - INFO - Batch 4100, Loss: 2.1225, LR: 0.000008
2025-10-22 01:13:28,309 - INFO - Batch 4200, Loss: 2.1553, LR: 0.000008
2025-10-22 01:13:44,491 - INFO - Batch 4300, Loss: 2.1904, LR: 0.000008
2025-10-22 01:14:00,746 - INFO - Batch 4400, Loss: 1.9744, LR: 0.000008
2025-10-22 01:14:16,946 - INFO - Batch 4500, Loss: 2.1195, LR: 0.000008
2025-10-22 01:14:32,950 - INFO - Batch 4600, Loss: 2.0719, LR: 0.000008
2025-10-22 01:14:48,823 - INFO - Batch 4700, Loss: 2.5066, LR: 0.000008
2025-10-22 01:15:04,586 - INFO - Batch 4800, Loss: 2.5554, LR: 0.000008
2025-10-22 01:15:20,316 - INFO - Batch 4900, Loss: 2.1401, LR: 0.000007
2025-10-22 01:15:36,348 - INFO - Batch 5000, Loss: 2.1581, LR: 0.000007
2025-10-22 01:15:52,394 - INFO - Batch 5100, Loss: 2.2412, LR: 0.000007
2025-10-22 01:16:08,480 - INFO - Batch 5200, Loss: 2.2787, LR: 0.000007
2025-10-22 01:16:24,708 - INFO - Batch 5300, Loss: 2.4043, LR: 0.000007
2025-10-22 01:16:40,824 - INFO - Batch 5400, Loss: 2.0268, LR: 0.000007
2025-10-22 01:16:57,007 - INFO - Batch 5500, Loss: 1.8710, LR: 0.000007
2025-10-22 01:17:13,129 - INFO - Batch 5600, Loss: 2.0726, LR: 0.000007
2025-10-22 01:17:28,868 - INFO - Batch 5700, Loss: 1.9280, LR: 0.000007
2025-10-22 01:17:44,615 - INFO - Batch 5800, Loss: 2.3050, LR: 0.000007
2025-10-22 01:18:00,457 - INFO - Batch 5900, Loss: 2.0751, LR: 0.000007
2025-10-22 01:18:16,554 - INFO - Batch 6000, Loss: 2.0305, LR: 0.000007
2025-10-22 01:18:32,777 - INFO - Batch 6100, Loss: 2.1700, LR: 0.000007
2025-10-22 01:18:49,008 - INFO - Batch 6200, Loss: 2.2633, LR: 0.000007
2025-10-22 01:19:05,242 - INFO - Batch 6300, Loss: 1.9377, LR: 0.000007
2025-10-22 01:19:21,461 - INFO - Batch 6400, Loss: 2.1265, LR: 0.000007
2025-10-22 01:19:29,519 - INFO - Epoch 25/30: Train Loss: 2.1154, Val Loss: 2.0806, LR: 0.000007
2025-10-22 01:19:29,782 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-22 01:19:29,949 - INFO - Batch 0, Loss: 2.0755, LR: 0.000007
2025-10-22 01:19:45,990 - INFO - Batch 100, Loss: 2.1175, LR: 0.000007
2025-10-22 01:20:02,029 - INFO - Batch 200, Loss: 2.0502, LR: 0.000007
2025-10-22 01:20:17,814 - INFO - Batch 300, Loss: 1.8430, LR: 0.000007
2025-10-22 01:20:33,607 - INFO - Batch 400, Loss: 2.2706, LR: 0.000007
2025-10-22 01:20:49,504 - INFO - Batch 500, Loss: 1.9236, LR: 0.000007
2025-10-22 01:21:05,846 - INFO - Batch 600, Loss: 2.2835, LR: 0.000007
2025-10-22 01:21:22,161 - INFO - Batch 700, Loss: 2.0803, LR: 0.000007
2025-10-22 01:21:38,458 - INFO - Batch 800, Loss: 1.9519, LR: 0.000007
2025-10-22 01:21:54,690 - INFO - Batch 900, Loss: 1.9631, LR: 0.000006
2025-10-22 01:22:10,964 - INFO - Batch 1000, Loss: 2.0301, LR: 0.000006
2025-10-22 01:22:27,271 - INFO - Batch 1100, Loss: 2.3138, LR: 0.000006
2025-10-22 01:22:43,526 - INFO - Batch 1200, Loss: 1.9881, LR: 0.000006
2025-10-22 01:22:59,443 - INFO - Batch 1300, Loss: 1.9530, LR: 0.000006
2025-10-22 01:23:15,378 - INFO - Batch 1400, Loss: 2.2963, LR: 0.000006
2025-10-22 01:23:31,287 - INFO - Batch 1500, Loss: 2.2168, LR: 0.000006
2025-10-22 01:23:47,578 - INFO - Batch 1600, Loss: 2.0674, LR: 0.000006
2025-10-22 01:24:03,821 - INFO - Batch 1700, Loss: 2.1704, LR: 0.000006
2025-10-22 01:24:20,008 - INFO - Batch 1800, Loss: 1.9433, LR: 0.000006
2025-10-22 01:24:36,356 - INFO - Batch 1900, Loss: 1.8983, LR: 0.000006
2025-10-22 01:24:52,675 - INFO - Batch 2000, Loss: 2.2528, LR: 0.000006
2025-10-22 01:25:09,009 - INFO - Batch 2100, Loss: 2.0523, LR: 0.000006
2025-10-22 01:25:25,314 - INFO - Batch 2200, Loss: 1.8992, LR: 0.000006
2025-10-22 01:25:41,263 - INFO - Batch 2300, Loss: 2.2122, LR: 0.000006
2025-10-22 01:25:57,195 - INFO - Batch 2400, Loss: 2.0298, LR: 0.000006
2025-10-22 01:26:13,106 - INFO - Batch 2500, Loss: 2.2796, LR: 0.000006
2025-10-22 01:26:29,342 - INFO - Batch 2600, Loss: 2.1749, LR: 0.000006
2025-10-22 01:26:45,619 - INFO - Batch 2700, Loss: 1.9093, LR: 0.000006
2025-10-22 01:27:01,836 - INFO - Batch 2800, Loss: 2.1030, LR: 0.000006
2025-10-22 01:27:18,144 - INFO - Batch 2900, Loss: 2.3233, LR: 0.000006
2025-10-22 01:27:34,312 - INFO - Batch 3000, Loss: 1.9284, LR: 0.000006
2025-10-22 01:27:50,589 - INFO - Batch 3100, Loss: 2.1850, LR: 0.000006
2025-10-22 01:28:06,915 - INFO - Batch 3200, Loss: 2.2959, LR: 0.000006
2025-10-22 01:28:22,875 - INFO - Batch 3300, Loss: 2.1524, LR: 0.000006
2025-10-22 01:28:38,826 - INFO - Batch 3400, Loss: 1.9182, LR: 0.000005
2025-10-22 01:28:54,754 - INFO - Batch 3500, Loss: 2.2634, LR: 0.000005
2025-10-22 01:29:11,028 - INFO - Batch 3600, Loss: 2.1758, LR: 0.000005
2025-10-22 01:29:27,351 - INFO - Batch 3700, Loss: 2.2477, LR: 0.000005
2025-10-22 01:29:43,672 - INFO - Batch 3800, Loss: 2.0602, LR: 0.000005
2025-10-22 01:29:59,971 - INFO - Batch 3900, Loss: 1.9813, LR: 0.000005
2025-10-22 01:30:16,302 - INFO - Batch 4000, Loss: 2.0753, LR: 0.000005
2025-10-22 01:30:32,571 - INFO - Batch 4100, Loss: 2.1025, LR: 0.000005
2025-10-22 01:30:48,867 - INFO - Batch 4200, Loss: 2.1563, LR: 0.000005
2025-10-22 01:31:04,837 - INFO - Batch 4300, Loss: 2.2252, LR: 0.000005
2025-10-22 01:31:20,770 - INFO - Batch 4400, Loss: 2.1414, LR: 0.000005
2025-10-22 01:31:36,695 - INFO - Batch 4500, Loss: 1.9901, LR: 0.000005
2025-10-22 01:31:52,954 - INFO - Batch 4600, Loss: 2.0287, LR: 0.000005
2025-10-22 01:32:09,261 - INFO - Batch 4700, Loss: 1.8219, LR: 0.000005
2025-10-22 01:32:25,561 - INFO - Batch 4800, Loss: 2.2121, LR: 0.000005
2025-10-22 01:32:41,870 - INFO - Batch 4900, Loss: 2.1687, LR: 0.000005
2025-10-22 01:32:58,207 - INFO - Batch 5000, Loss: 2.1982, LR: 0.000005
2025-10-22 01:33:14,501 - INFO - Batch 5100, Loss: 2.0063, LR: 0.000005
2025-10-22 01:33:30,812 - INFO - Batch 5200, Loss: 2.0625, LR: 0.000005
2025-10-22 01:33:46,821 - INFO - Batch 5300, Loss: 2.3821, LR: 0.000005
2025-10-22 01:34:02,753 - INFO - Batch 5400, Loss: 2.2743, LR: 0.000005
2025-10-22 01:34:18,662 - INFO - Batch 5500, Loss: 2.1784, LR: 0.000005
2025-10-22 01:34:34,894 - INFO - Batch 5600, Loss: 1.8632, LR: 0.000005
2025-10-22 01:34:51,207 - INFO - Batch 5700, Loss: 2.0384, LR: 0.000005
2025-10-22 01:35:07,514 - INFO - Batch 5800, Loss: 2.0748, LR: 0.000005
2025-10-22 01:35:23,813 - INFO - Batch 5900, Loss: 2.0212, LR: 0.000005
2025-10-22 01:35:40,123 - INFO - Batch 6000, Loss: 2.2198, LR: 0.000005
2025-10-22 01:35:56,364 - INFO - Batch 6100, Loss: 1.7606, LR: 0.000005
2025-10-22 01:36:12,649 - INFO - Batch 6200, Loss: 2.1095, LR: 0.000004
2025-10-22 01:36:28,628 - INFO - Batch 6300, Loss: 2.0165, LR: 0.000004
2025-10-22 01:36:44,559 - INFO - Batch 6400, Loss: 2.2353, LR: 0.000004
2025-10-22 01:36:52,496 - INFO - Epoch 26/30: Train Loss: 2.1071, Val Loss: 2.0808, LR: 0.000004
2025-10-22 01:36:52,662 - INFO - Batch 0, Loss: 1.8578, LR: 0.000004
2025-10-22 01:37:08,664 - INFO - Batch 100, Loss: 2.0500, LR: 0.000004
2025-10-22 01:37:24,933 - INFO - Batch 200, Loss: 2.1236, LR: 0.000004
2025-10-22 01:37:41,226 - INFO - Batch 300, Loss: 1.9230, LR: 0.000004
2025-10-22 01:37:57,539 - INFO - Batch 400, Loss: 2.3338, LR: 0.000004
2025-10-22 01:38:13,845 - INFO - Batch 500, Loss: 2.2078, LR: 0.000004
2025-10-22 01:38:30,150 - INFO - Batch 600, Loss: 2.0871, LR: 0.000004
2025-10-22 01:38:46,465 - INFO - Batch 700, Loss: 1.9790, LR: 0.000004
2025-10-22 01:39:02,736 - INFO - Batch 800, Loss: 2.2444, LR: 0.000004
2025-10-22 01:39:18,870 - INFO - Batch 900, Loss: 2.0640, LR: 0.000004
2025-10-22 01:39:34,806 - INFO - Batch 1000, Loss: 2.0972, LR: 0.000004
2025-10-22 01:39:50,892 - INFO - Batch 1100, Loss: 2.0344, LR: 0.000004
2025-10-22 01:40:07,232 - INFO - Batch 1200, Loss: 1.7183, LR: 0.000004
2025-10-22 01:40:23,491 - INFO - Batch 1300, Loss: 2.2590, LR: 0.000004
2025-10-22 01:40:39,729 - INFO - Batch 1400, Loss: 1.9912, LR: 0.000004
2025-10-22 01:40:56,043 - INFO - Batch 1500, Loss: 2.1564, LR: 0.000004
2025-10-22 01:41:12,326 - INFO - Batch 1600, Loss: 2.2332, LR: 0.000004
2025-10-22 01:41:28,650 - INFO - Batch 1700, Loss: 2.3906, LR: 0.000004
2025-10-22 01:41:44,778 - INFO - Batch 1800, Loss: 2.0600, LR: 0.000004
2025-10-22 01:42:00,726 - INFO - Batch 1900, Loss: 2.3225, LR: 0.000004
2025-10-22 01:42:16,706 - INFO - Batch 2000, Loss: 1.9300, LR: 0.000004
2025-10-22 01:42:32,739 - INFO - Batch 2100, Loss: 2.1179, LR: 0.000004
2025-10-22 01:42:49,022 - INFO - Batch 2200, Loss: 2.1298, LR: 0.000004
2025-10-22 01:43:05,346 - INFO - Batch 2300, Loss: 2.1324, LR: 0.000004
2025-10-22 01:43:21,663 - INFO - Batch 2400, Loss: 2.1746, LR: 0.000004
2025-10-22 01:43:37,957 - INFO - Batch 2500, Loss: 2.3388, LR: 0.000004
2025-10-22 01:43:54,261 - INFO - Batch 2600, Loss: 1.8761, LR: 0.000004
2025-10-22 01:44:10,570 - INFO - Batch 2700, Loss: 1.9768, LR: 0.000004
2025-10-22 01:44:26,773 - INFO - Batch 2800, Loss: 2.1267, LR: 0.000004
2025-10-22 01:44:42,674 - INFO - Batch 2900, Loss: 1.9537, LR: 0.000003
2025-10-22 01:44:58,584 - INFO - Batch 3000, Loss: 2.3826, LR: 0.000003
2025-10-22 01:45:14,524 - INFO - Batch 3100, Loss: 2.4172, LR: 0.000003
2025-10-22 01:45:30,781 - INFO - Batch 3200, Loss: 2.1965, LR: 0.000003
2025-10-22 01:45:47,118 - INFO - Batch 3300, Loss: 1.9351, LR: 0.000003
2025-10-22 01:46:03,399 - INFO - Batch 3400, Loss: 2.0700, LR: 0.000003
2025-10-22 01:46:19,656 - INFO - Batch 3500, Loss: 2.0584, LR: 0.000003
2025-10-22 01:46:35,892 - INFO - Batch 3600, Loss: 2.2457, LR: 0.000003
2025-10-22 01:46:52,067 - INFO - Batch 3700, Loss: 2.0590, LR: 0.000003
2025-10-22 01:47:08,307 - INFO - Batch 3800, Loss: 2.3136, LR: 0.000003
2025-10-22 01:47:24,271 - INFO - Batch 3900, Loss: 2.2871, LR: 0.000003
2025-10-22 01:47:40,200 - INFO - Batch 4000, Loss: 2.2117, LR: 0.000003
2025-10-22 01:47:56,115 - INFO - Batch 4100, Loss: 2.2213, LR: 0.000003
2025-10-22 01:48:12,435 - INFO - Batch 4200, Loss: 2.1795, LR: 0.000003
2025-10-22 01:48:28,752 - INFO - Batch 4300, Loss: 2.1348, LR: 0.000003
2025-10-22 01:48:45,078 - INFO - Batch 4400, Loss: 2.1165, LR: 0.000003
2025-10-22 01:49:01,418 - INFO - Batch 4500, Loss: 2.0943, LR: 0.000003
2025-10-22 01:49:17,736 - INFO - Batch 4600, Loss: 2.0983, LR: 0.000003
2025-10-22 01:49:34,042 - INFO - Batch 4700, Loss: 2.1006, LR: 0.000003
2025-10-22 01:49:50,303 - INFO - Batch 4800, Loss: 2.2410, LR: 0.000003
2025-10-22 01:50:06,253 - INFO - Batch 4900, Loss: 2.0006, LR: 0.000003
2025-10-22 01:50:22,163 - INFO - Batch 5000, Loss: 1.8892, LR: 0.000003
2025-10-22 01:50:38,090 - INFO - Batch 5100, Loss: 1.8932, LR: 0.000003
2025-10-22 01:50:54,391 - INFO - Batch 5200, Loss: 2.2237, LR: 0.000003
2025-10-22 01:51:10,687 - INFO - Batch 5300, Loss: 2.2459, LR: 0.000003
2025-10-22 01:51:26,998 - INFO - Batch 5400, Loss: 1.9050, LR: 0.000003
2025-10-22 01:51:43,286 - INFO - Batch 5500, Loss: 2.0056, LR: 0.000003
2025-10-22 01:51:59,605 - INFO - Batch 5600, Loss: 2.2074, LR: 0.000003
2025-10-22 01:52:15,922 - INFO - Batch 5700, Loss: 2.1159, LR: 0.000003
2025-10-22 01:52:32,131 - INFO - Batch 5800, Loss: 2.1530, LR: 0.000003
2025-10-22 01:52:48,040 - INFO - Batch 5900, Loss: 2.2627, LR: 0.000003
2025-10-22 01:53:03,954 - INFO - Batch 6000, Loss: 1.9804, LR: 0.000003
2025-10-22 01:53:19,875 - INFO - Batch 6100, Loss: 2.0462, LR: 0.000003
2025-10-22 01:53:36,168 - INFO - Batch 6200, Loss: 2.1811, LR: 0.000003
2025-10-22 01:53:52,468 - INFO - Batch 6300, Loss: 2.2446, LR: 0.000003
2025-10-22 01:54:08,778 - INFO - Batch 6400, Loss: 2.0616, LR: 0.000003
2025-10-22 01:54:16,883 - INFO - Epoch 27/30: Train Loss: 2.0994, Val Loss: 2.0759, LR: 0.000002
2025-10-22 01:54:17,145 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-22 01:54:17,314 - INFO - Batch 0, Loss: 1.9166, LR: 0.000002
2025-10-22 01:54:33,507 - INFO - Batch 100, Loss: 2.2270, LR: 0.000002
2025-10-22 01:54:49,784 - INFO - Batch 200, Loss: 2.1400, LR: 0.000002
2025-10-22 01:55:06,078 - INFO - Batch 300, Loss: 2.1823, LR: 0.000002
2025-10-22 01:55:22,163 - INFO - Batch 400, Loss: 2.1119, LR: 0.000002
2025-10-22 01:55:38,103 - INFO - Batch 500, Loss: 2.1321, LR: 0.000002
2025-10-22 01:55:54,039 - INFO - Batch 600, Loss: 2.1272, LR: 0.000002
2025-10-22 01:56:10,187 - INFO - Batch 700, Loss: 2.0167, LR: 0.000002
2025-10-22 01:56:26,520 - INFO - Batch 800, Loss: 2.1263, LR: 0.000002
2025-10-22 01:56:42,818 - INFO - Batch 900, Loss: 2.0902, LR: 0.000002
2025-10-22 01:56:59,145 - INFO - Batch 1000, Loss: 2.2317, LR: 0.000002
2025-10-22 01:57:15,475 - INFO - Batch 1100, Loss: 2.1648, LR: 0.000002
2025-10-22 01:57:31,786 - INFO - Batch 1200, Loss: 2.1388, LR: 0.000002
2025-10-22 01:57:48,087 - INFO - Batch 1300, Loss: 2.2693, LR: 0.000002
2025-10-22 01:58:04,237 - INFO - Batch 1400, Loss: 1.8808, LR: 0.000002
2025-10-22 01:58:20,263 - INFO - Batch 1500, Loss: 2.0744, LR: 0.000002
2025-10-22 01:58:36,314 - INFO - Batch 1600, Loss: 1.9383, LR: 0.000002
2025-10-22 01:58:52,446 - INFO - Batch 1700, Loss: 2.1298, LR: 0.000002
2025-10-22 01:59:08,715 - INFO - Batch 1800, Loss: 2.0194, LR: 0.000002
2025-10-22 01:59:25,049 - INFO - Batch 1900, Loss: 1.8014, LR: 0.000002
2025-10-22 01:59:41,390 - INFO - Batch 2000, Loss: 2.0683, LR: 0.000002
2025-10-22 01:59:57,657 - INFO - Batch 2100, Loss: 1.8759, LR: 0.000002
2025-10-22 02:00:13,967 - INFO - Batch 2200, Loss: 2.1768, LR: 0.000002
2025-10-22 02:00:30,229 - INFO - Batch 2300, Loss: 2.2034, LR: 0.000002
2025-10-22 02:00:46,329 - INFO - Batch 2400, Loss: 2.1674, LR: 0.000002
2025-10-22 02:01:02,258 - INFO - Batch 2500, Loss: 2.0670, LR: 0.000002
2025-10-22 02:01:18,206 - INFO - Batch 2600, Loss: 1.8362, LR: 0.000002
2025-10-22 02:01:34,259 - INFO - Batch 2700, Loss: 1.9203, LR: 0.000002
2025-10-22 02:01:50,421 - INFO - Batch 2800, Loss: 2.1638, LR: 0.000002
2025-10-22 02:02:06,745 - INFO - Batch 2900, Loss: 1.9269, LR: 0.000002
2025-10-22 02:02:23,036 - INFO - Batch 3000, Loss: 2.1404, LR: 0.000002
2025-10-22 02:02:39,335 - INFO - Batch 3100, Loss: 2.0103, LR: 0.000002
2025-10-22 02:02:55,615 - INFO - Batch 3200, Loss: 1.8312, LR: 0.000002
2025-10-22 02:03:11,918 - INFO - Batch 3300, Loss: 2.0041, LR: 0.000002
2025-10-22 02:03:28,037 - INFO - Batch 3400, Loss: 2.0080, LR: 0.000002
2025-10-22 02:03:43,973 - INFO - Batch 3500, Loss: 1.9084, LR: 0.000002
2025-10-22 02:03:59,906 - INFO - Batch 3600, Loss: 2.3120, LR: 0.000002
2025-10-22 02:04:15,968 - INFO - Batch 3700, Loss: 1.9611, LR: 0.000002
2025-10-22 02:04:32,275 - INFO - Batch 3800, Loss: 2.0069, LR: 0.000002
2025-10-22 02:04:48,561 - INFO - Batch 3900, Loss: 2.2545, LR: 0.000002
2025-10-22 02:05:04,794 - INFO - Batch 4000, Loss: 2.1436, LR: 0.000002
2025-10-22 02:05:20,730 - INFO - Batch 4100, Loss: 2.0210, LR: 0.000002
2025-10-22 02:05:36,681 - INFO - Batch 4200, Loss: 1.9220, LR: 0.000002
2025-10-22 02:05:52,690 - INFO - Batch 4300, Loss: 2.1052, LR: 0.000002
2025-10-22 02:06:08,826 - INFO - Batch 4400, Loss: 1.8028, LR: 0.000001
2025-10-22 02:06:24,737 - INFO - Batch 4500, Loss: 1.8452, LR: 0.000001
2025-10-22 02:06:40,650 - INFO - Batch 4600, Loss: 1.9424, LR: 0.000001
2025-10-22 02:06:56,646 - INFO - Batch 4700, Loss: 2.1991, LR: 0.000001
2025-10-22 02:07:12,961 - INFO - Batch 4800, Loss: 2.0294, LR: 0.000001
2025-10-22 02:07:29,239 - INFO - Batch 4900, Loss: 2.1027, LR: 0.000001
2025-10-22 02:07:45,565 - INFO - Batch 5000, Loss: 2.4128, LR: 0.000001
2025-10-22 02:08:01,871 - INFO - Batch 5100, Loss: 2.3708, LR: 0.000001
2025-10-22 02:08:18,149 - INFO - Batch 5200, Loss: 2.4456, LR: 0.000001
2025-10-22 02:08:34,428 - INFO - Batch 5300, Loss: 2.4334, LR: 0.000001
2025-10-22 02:08:50,617 - INFO - Batch 5400, Loss: 1.9777, LR: 0.000001
2025-10-22 02:09:06,572 - INFO - Batch 5500, Loss: 2.0028, LR: 0.000001
2025-10-22 02:09:22,521 - INFO - Batch 5600, Loss: 2.1344, LR: 0.000001
2025-10-22 02:09:38,538 - INFO - Batch 5700, Loss: 2.0942, LR: 0.000001
2025-10-22 02:09:54,853 - INFO - Batch 5800, Loss: 1.9215, LR: 0.000001
2025-10-22 02:10:11,139 - INFO - Batch 5900, Loss: 1.9073, LR: 0.000001
2025-10-22 02:10:27,421 - INFO - Batch 6000, Loss: 2.1249, LR: 0.000001
2025-10-22 02:10:43,708 - INFO - Batch 6100, Loss: 2.1891, LR: 0.000001
2025-10-22 02:11:00,006 - INFO - Batch 6200, Loss: 2.2008, LR: 0.000001
2025-10-22 02:11:16,264 - INFO - Batch 6300, Loss: 1.9974, LR: 0.000001
2025-10-22 02:11:32,417 - INFO - Batch 6400, Loss: 2.3079, LR: 0.000001
2025-10-22 02:11:40,349 - INFO - Epoch 28/30: Train Loss: 2.0944, Val Loss: 2.0751, LR: 0.000001
2025-10-22 02:11:40,949 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-22 02:11:41,116 - INFO - Batch 0, Loss: 1.8864, LR: 0.000001
2025-10-22 02:11:56,921 - INFO - Batch 100, Loss: 2.1695, LR: 0.000001
2025-10-22 02:12:12,859 - INFO - Batch 200, Loss: 2.0550, LR: 0.000001
2025-10-22 02:12:29,118 - INFO - Batch 300, Loss: 1.8510, LR: 0.000001
2025-10-22 02:12:45,375 - INFO - Batch 400, Loss: 2.1750, LR: 0.000001
2025-10-22 02:13:01,623 - INFO - Batch 500, Loss: 1.9967, LR: 0.000001
2025-10-22 02:13:17,892 - INFO - Batch 600, Loss: 2.0357, LR: 0.000001
2025-10-22 02:13:34,226 - INFO - Batch 700, Loss: 2.0813, LR: 0.000001
2025-10-22 02:13:50,443 - INFO - Batch 800, Loss: 2.0207, LR: 0.000001
2025-10-22 02:14:06,638 - INFO - Batch 900, Loss: 2.0100, LR: 0.000001
2025-10-22 02:14:22,547 - INFO - Batch 1000, Loss: 2.1282, LR: 0.000001
2025-10-22 02:14:38,459 - INFO - Batch 1100, Loss: 2.0466, LR: 0.000001
2025-10-22 02:14:54,360 - INFO - Batch 1200, Loss: 2.1252, LR: 0.000001
2025-10-22 02:15:10,637 - INFO - Batch 1300, Loss: 2.2287, LR: 0.000001
2025-10-22 02:15:26,960 - INFO - Batch 1400, Loss: 2.2517, LR: 0.000001
2025-10-22 02:15:43,100 - INFO - Batch 1500, Loss: 1.9957, LR: 0.000001
2025-10-22 02:15:59,266 - INFO - Batch 1600, Loss: 1.8885, LR: 0.000001
2025-10-22 02:16:15,242 - INFO - Batch 1700, Loss: 2.1821, LR: 0.000001
2025-10-22 02:16:31,197 - INFO - Batch 1800, Loss: 1.9957, LR: 0.000001
2025-10-22 02:16:47,183 - INFO - Batch 1900, Loss: 2.2401, LR: 0.000001
2025-10-22 02:17:02,976 - INFO - Batch 2000, Loss: 2.0312, LR: 0.000001
2025-10-22 02:17:18,839 - INFO - Batch 2100, Loss: 2.1223, LR: 0.000001
2025-10-22 02:17:34,653 - INFO - Batch 2200, Loss: 2.1131, LR: 0.000001
2025-10-22 02:17:50,539 - INFO - Batch 2300, Loss: 2.3448, LR: 0.000001
2025-10-22 02:18:06,514 - INFO - Batch 2400, Loss: 1.8738, LR: 0.000001
2025-10-22 02:18:22,525 - INFO - Batch 2500, Loss: 2.1983, LR: 0.000001
2025-10-22 02:18:38,625 - INFO - Batch 2600, Loss: 2.0524, LR: 0.000001
2025-10-22 02:18:54,693 - INFO - Batch 2700, Loss: 2.0050, LR: 0.000001
2025-10-22 02:19:10,667 - INFO - Batch 2800, Loss: 1.9200, LR: 0.000001
2025-10-22 02:19:26,626 - INFO - Batch 2900, Loss: 1.9000, LR: 0.000001
2025-10-22 02:19:42,515 - INFO - Batch 3000, Loss: 2.2370, LR: 0.000001
2025-10-22 02:19:58,278 - INFO - Batch 3100, Loss: 2.1116, LR: 0.000001
2025-10-22 02:20:14,010 - INFO - Batch 3200, Loss: 1.9714, LR: 0.000001
2025-10-22 02:20:29,769 - INFO - Batch 3300, Loss: 2.0098, LR: 0.000001
2025-10-22 02:20:45,719 - INFO - Batch 3400, Loss: 1.8144, LR: 0.000001
2025-10-22 02:21:01,676 - INFO - Batch 3500, Loss: 2.0004, LR: 0.000001
2025-10-22 02:21:17,668 - INFO - Batch 3600, Loss: 2.0406, LR: 0.000001
2025-10-22 02:21:33,643 - INFO - Batch 3700, Loss: 1.8318, LR: 0.000001
2025-10-22 02:21:49,673 - INFO - Batch 3800, Loss: 2.1854, LR: 0.000001
2025-10-22 02:22:05,945 - INFO - Batch 3900, Loss: 1.9926, LR: 0.000001
2025-10-22 02:22:22,113 - INFO - Batch 4000, Loss: 2.0845, LR: 0.000001
2025-10-22 02:22:37,981 - INFO - Batch 4100, Loss: 2.2167, LR: 0.000001
2025-10-22 02:22:53,804 - INFO - Batch 4200, Loss: 2.1343, LR: 0.000001
2025-10-22 02:23:09,654 - INFO - Batch 4300, Loss: 2.0742, LR: 0.000000
2025-10-22 02:23:25,589 - INFO - Batch 4400, Loss: 2.0629, LR: 0.000000
2025-10-22 02:23:41,612 - INFO - Batch 4500, Loss: 2.2472, LR: 0.000000
2025-10-22 02:23:57,597 - INFO - Batch 4600, Loss: 2.0381, LR: 0.000000
2025-10-22 02:24:13,577 - INFO - Batch 4700, Loss: 2.0415, LR: 0.000000
2025-10-22 02:24:29,572 - INFO - Batch 4800, Loss: 2.1374, LR: 0.000000
2025-10-22 02:24:45,692 - INFO - Batch 4900, Loss: 1.9453, LR: 0.000000
2025-10-22 02:25:01,797 - INFO - Batch 5000, Loss: 2.0934, LR: 0.000000
2025-10-22 02:25:17,746 - INFO - Batch 5100, Loss: 2.3067, LR: 0.000000
2025-10-22 02:25:33,499 - INFO - Batch 5200, Loss: 1.8688, LR: 0.000000
2025-10-22 02:25:49,277 - INFO - Batch 5300, Loss: 2.0142, LR: 0.000000
2025-10-22 02:26:05,062 - INFO - Batch 5400, Loss: 2.2485, LR: 0.000000
2025-10-22 02:26:21,032 - INFO - Batch 5500, Loss: 2.1903, LR: 0.000000
2025-10-22 02:26:37,028 - INFO - Batch 5600, Loss: 1.8350, LR: 0.000000
2025-10-22 02:26:53,020 - INFO - Batch 5700, Loss: 2.0706, LR: 0.000000
2025-10-22 02:27:09,044 - INFO - Batch 5800, Loss: 2.1888, LR: 0.000000
2025-10-22 02:27:25,130 - INFO - Batch 5900, Loss: 2.1498, LR: 0.000000
2025-10-22 02:27:41,068 - INFO - Batch 6000, Loss: 1.9178, LR: 0.000000
2025-10-22 02:27:57,044 - INFO - Batch 6100, Loss: 2.0611, LR: 0.000000
2025-10-22 02:28:13,190 - INFO - Batch 6200, Loss: 2.0877, LR: 0.000000
2025-10-22 02:28:29,126 - INFO - Batch 6300, Loss: 2.0851, LR: 0.000000
2025-10-22 02:28:44,908 - INFO - Batch 6400, Loss: 2.2036, LR: 0.000000
2025-10-22 02:28:52,790 - INFO - Epoch 29/30: Train Loss: 2.0904, Val Loss: 2.0754, LR: 0.000000
2025-10-22 02:28:52,954 - INFO - Batch 0, Loss: 2.0662, LR: 0.000000
2025-10-22 02:29:08,920 - INFO - Batch 100, Loss: 1.8683, LR: 0.000000
2025-10-22 02:29:24,880 - INFO - Batch 200, Loss: 2.2632, LR: 0.000000
2025-10-22 02:29:40,948 - INFO - Batch 300, Loss: 2.1256, LR: 0.000000
2025-10-22 02:29:57,253 - INFO - Batch 400, Loss: 2.3125, LR: 0.000000
2025-10-22 02:30:13,400 - INFO - Batch 500, Loss: 2.0791, LR: 0.000000
2025-10-22 02:30:29,499 - INFO - Batch 600, Loss: 2.2698, LR: 0.000000
2025-10-22 02:30:45,801 - INFO - Batch 700, Loss: 1.9506, LR: 0.000000
2025-10-22 02:31:01,765 - INFO - Batch 800, Loss: 2.0267, LR: 0.000000
2025-10-22 02:31:17,548 - INFO - Batch 900, Loss: 2.1749, LR: 0.000000
2025-10-22 02:31:33,386 - INFO - Batch 1000, Loss: 2.0859, LR: 0.000000
2025-10-22 02:31:49,331 - INFO - Batch 1100, Loss: 2.0270, LR: 0.000000
2025-10-22 02:32:05,578 - INFO - Batch 1200, Loss: 2.0628, LR: 0.000000
2025-10-22 02:32:21,888 - INFO - Batch 1300, Loss: 2.2249, LR: 0.000000
2025-10-22 02:32:38,194 - INFO - Batch 1400, Loss: 2.0576, LR: 0.000000
2025-10-22 02:32:54,538 - INFO - Batch 1500, Loss: 2.0561, LR: 0.000000
2025-10-22 02:33:10,869 - INFO - Batch 1600, Loss: 2.0212, LR: 0.000000
2025-10-22 02:33:27,083 - INFO - Batch 1700, Loss: 2.1137, LR: 0.000000
2025-10-22 02:33:43,054 - INFO - Batch 1800, Loss: 2.0818, LR: 0.000000
2025-10-22 02:33:58,919 - INFO - Batch 1900, Loss: 1.9659, LR: 0.000000
2025-10-22 02:34:14,835 - INFO - Batch 2000, Loss: 2.0528, LR: 0.000000
2025-10-22 02:34:30,999 - INFO - Batch 2100, Loss: 2.1896, LR: 0.000000
2025-10-22 02:34:47,210 - INFO - Batch 2200, Loss: 1.8750, LR: 0.000000
2025-10-22 02:35:03,335 - INFO - Batch 2300, Loss: 1.8293, LR: 0.000000
2025-10-22 02:35:19,544 - INFO - Batch 2400, Loss: 2.0037, LR: 0.000000
2025-10-22 02:35:35,843 - INFO - Batch 2500, Loss: 2.1326, LR: 0.000000
2025-10-22 02:35:52,137 - INFO - Batch 2600, Loss: 1.8416, LR: 0.000000
2025-10-22 02:36:08,435 - INFO - Batch 2700, Loss: 1.8840, LR: 0.000000
2025-10-22 02:36:24,573 - INFO - Batch 2800, Loss: 2.2475, LR: 0.000000
2025-10-22 02:36:40,526 - INFO - Batch 2900, Loss: 2.1220, LR: 0.000000
2025-10-22 02:36:56,530 - INFO - Batch 3000, Loss: 2.3177, LR: 0.000000
2025-10-22 02:37:12,651 - INFO - Batch 3100, Loss: 1.8864, LR: 0.000000
2025-10-22 02:37:28,688 - INFO - Batch 3200, Loss: 2.1057, LR: 0.000000
2025-10-22 02:37:44,649 - INFO - Batch 3300, Loss: 1.8883, LR: 0.000000
2025-10-22 02:38:00,631 - INFO - Batch 3400, Loss: 2.0760, LR: 0.000000
2025-10-22 02:38:16,598 - INFO - Batch 3500, Loss: 1.9765, LR: 0.000000
2025-10-22 02:38:32,581 - INFO - Batch 3600, Loss: 2.2807, LR: 0.000000
2025-10-22 02:38:48,491 - INFO - Batch 3700, Loss: 2.0128, LR: 0.000000
2025-10-22 02:39:04,359 - INFO - Batch 3800, Loss: 2.1166, LR: 0.000000
2025-10-22 02:39:20,143 - INFO - Batch 3900, Loss: 2.0761, LR: 0.000000
2025-10-22 02:39:35,886 - INFO - Batch 4000, Loss: 2.0199, LR: 0.000000
2025-10-22 02:39:51,657 - INFO - Batch 4100, Loss: 2.0041, LR: 0.000000
2025-10-22 02:40:07,590 - INFO - Batch 4200, Loss: 1.9872, LR: 0.000000
2025-10-22 02:40:23,551 - INFO - Batch 4300, Loss: 2.1441, LR: 0.000000
2025-10-22 02:40:39,515 - INFO - Batch 4400, Loss: 2.4596, LR: 0.000000
2025-10-22 02:40:55,384 - INFO - Batch 4500, Loss: 2.1301, LR: 0.000000
2025-10-22 02:41:11,305 - INFO - Batch 4600, Loss: 2.1980, LR: 0.000000
2025-10-22 02:41:27,177 - INFO - Batch 4700, Loss: 1.9684, LR: 0.000000
2025-10-22 02:41:43,038 - INFO - Batch 4800, Loss: 1.8986, LR: 0.000000
2025-10-22 02:41:58,895 - INFO - Batch 4900, Loss: 2.0549, LR: 0.000000
2025-10-22 02:42:14,809 - INFO - Batch 5000, Loss: 2.1814, LR: 0.000000
2025-10-22 02:42:30,554 - INFO - Batch 5100, Loss: 2.0918, LR: 0.000000
2025-10-22 02:42:46,320 - INFO - Batch 5200, Loss: 2.1315, LR: 0.000000
2025-10-22 02:43:02,255 - INFO - Batch 5300, Loss: 1.9777, LR: 0.000000
2025-10-22 02:43:18,436 - INFO - Batch 5400, Loss: 2.1298, LR: 0.000000
2025-10-22 02:43:34,569 - INFO - Batch 5500, Loss: 2.0175, LR: 0.000000
2025-10-22 02:43:50,688 - INFO - Batch 5600, Loss: 2.0390, LR: 0.000000
2025-10-22 02:44:06,899 - INFO - Batch 5700, Loss: 2.0815, LR: 0.000000
2025-10-22 02:44:23,062 - INFO - Batch 5800, Loss: 2.0833, LR: 0.000000
2025-10-22 02:44:39,021 - INFO - Batch 5900, Loss: 2.0850, LR: 0.000000
2025-10-22 02:44:54,934 - INFO - Batch 6000, Loss: 2.2519, LR: 0.000000
2025-10-22 02:45:10,781 - INFO - Batch 6100, Loss: 2.0444, LR: 0.000000
2025-10-22 02:45:26,757 - INFO - Batch 6200, Loss: 2.1876, LR: 0.000000
2025-10-22 02:45:42,877 - INFO - Batch 6300, Loss: 2.3101, LR: 0.000000
2025-10-22 02:45:58,810 - INFO - Batch 6400, Loss: 1.8662, LR: 0.000000
2025-10-22 02:46:06,757 - INFO - Epoch 30/30: Train Loss: 2.0881, Val Loss: 2.0747, LR: 0.000000
2025-10-22 02:46:07,005 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-22 02:46:07,199 - INFO - 模型已保存到: ./checkpoints/checkpoint_epoch_30.pth
2025-10-22 02:46:07,396 - INFO - 模型已保存到: ./checkpoints/final_model.pth
2025-10-22 02:46:07,970 - INFO - 训练完成!
2025-11-03 11:44:16,218 - INFO - 使用设备: cpu
2025-11-03 11:44:16,218 - INFO - 加载数据...
2025-11-03 11:44:16,218 - INFO - 加载模型: checkpoints/best_model.pth
2025-11-03 11:44:16,709 - INFO - 从检查点加载模型状态
2025-11-03 11:44:16,710 - INFO - 模型加载成功
2025-11-03 11:44:16,727 - INFO - 加载训练数据...
2025-11-03 11:44:22,126 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-11-03 11:44:22,126 - INFO - 训练数据结构:
2025-11-03 11:44:22,126 - INFO - 数据结构分析:
2025-11-03 11:44:22,126 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 11:44:22,126 - INFO - 
英语相关语言对:
2025-11-03 11:44:22,126 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 11:44:22,127 - INFO -   en->de: 206112 个样本
2025-11-03 11:44:22,127 - INFO - 
德语相关语言对:
2025-11-03 11:44:22,127 - INFO - 德语->其他语言: ['en']
2025-11-03 11:44:22,127 - INFO -   de->en: 206112 个样本
2025-11-03 11:44:22,127 - INFO - 提取 en->de 的翻译对
2025-11-03 11:44:22,127 - INFO - 找到 en->de: 206112 个样本
2025-11-03 11:44:22,185 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-11-03 11:44:22,185 - INFO - 加载验证数据...
2025-11-03 11:44:22,236 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-11-03 11:44:22,236 - INFO - 提取 en->de 的翻译对
2025-11-03 11:44:22,236 - INFO - 找到 en->de: 888 个样本
2025-11-03 11:44:22,237 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-11-03 11:44:22,237 - INFO - 加载测试数据...
2025-11-03 11:44:22,355 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-11-03 11:44:22,355 - INFO - 提取 en->de 的翻译对
2025-11-03 11:44:22,355 - INFO - 找到 en->de: 8079 个样本
2025-11-03 11:44:22,358 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-11-03 11:44:22,358 - INFO - 开始构建分词器...
2025-11-03 11:44:22,431 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-11-03 11:44:23,977 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-11-03 11:44:26,334 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-11-03 11:44:26,380 - INFO - 数据加载器创建完成
2025-11-03 11:44:26,380 - INFO - 训练集: 206112 个样本
2025-11-03 11:44:26,380 - INFO - 验证集: 888 个样本
2025-11-03 11:44:26,380 - INFO - 测试集: 8079 个样本
2025-11-03 11:44:27,526 - INFO - 计算训练集指标...
2025-11-03 11:49:11,182 - INFO - 使用设备: cpu
2025-11-03 11:49:11,182 - INFO - 加载数据...
2025-11-03 11:49:11,182 - INFO - 加载模型: checkpoints/best_model.pth
2025-11-03 11:49:11,446 - INFO - 从检查点加载模型状态
2025-11-03 11:49:11,446 - INFO - 模型加载成功
2025-11-03 11:49:11,455 - INFO - 加载训练数据...
2025-11-03 11:49:16,053 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-11-03 11:49:16,053 - INFO - 训练数据结构:
2025-11-03 11:49:16,053 - INFO - 数据结构分析:
2025-11-03 11:49:16,054 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 11:49:16,054 - INFO - 
英语相关语言对:
2025-11-03 11:49:16,054 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 11:49:16,054 - INFO -   en->de: 206112 个样本
2025-11-03 11:49:16,054 - INFO - 
德语相关语言对:
2025-11-03 11:49:16,054 - INFO - 德语->其他语言: ['en']
2025-11-03 11:49:16,054 - INFO -   de->en: 206112 个样本
2025-11-03 11:49:16,054 - INFO - 提取 en->de 的翻译对
2025-11-03 11:49:16,054 - INFO - 找到 en->de: 206112 个样本
2025-11-03 11:49:16,110 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-11-03 11:49:16,110 - INFO - 加载验证数据...
2025-11-03 11:49:16,154 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-11-03 11:49:16,154 - INFO - 提取 en->de 的翻译对
2025-11-03 11:49:16,154 - INFO - 找到 en->de: 888 个样本
2025-11-03 11:49:16,154 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-11-03 11:49:16,154 - INFO - 加载测试数据...
2025-11-03 11:49:16,256 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-11-03 11:49:16,257 - INFO - 提取 en->de 的翻译对
2025-11-03 11:49:16,257 - INFO - 找到 en->de: 8079 个样本
2025-11-03 11:49:16,259 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-11-03 11:49:16,259 - INFO - 开始构建分词器...
2025-11-03 11:49:16,314 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-11-03 11:49:17,574 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-11-03 11:49:19,600 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-11-03 11:49:19,639 - INFO - 数据加载器创建完成
2025-11-03 11:49:19,639 - INFO - 训练集: 206112 个样本
2025-11-03 11:49:19,639 - INFO - 验证集: 888 个样本
2025-11-03 11:49:19,639 - INFO - 测试集: 8079 个样本
2025-11-03 11:49:20,655 - INFO - 计算验证集指标...
2025-11-03 11:51:56,645 - INFO - 计算测试集指标...
2025-11-03 12:13:12,065 - INFO - 计算精确匹配率...
2025-11-03 12:23:38,329 - INFO - 使用设备: cpu
2025-11-03 12:23:38,329 - INFO - 加载数据和构建分词器...
2025-11-03 12:23:38,329 - INFO - 加载训练数据...
2025-11-03 12:23:42,993 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-11-03 12:23:42,993 - INFO - 训练数据结构:
2025-11-03 12:23:42,993 - INFO - 数据结构分析:
2025-11-03 12:23:42,993 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 12:23:42,994 - INFO - 
英语相关语言对:
2025-11-03 12:23:42,994 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 12:23:42,994 - INFO -   en->de: 206112 个样本
2025-11-03 12:23:42,994 - INFO - 
德语相关语言对:
2025-11-03 12:23:42,994 - INFO - 德语->其他语言: ['en']
2025-11-03 12:23:42,994 - INFO -   de->en: 206112 个样本
2025-11-03 12:23:42,994 - INFO - 提取 en->de 的翻译对
2025-11-03 12:23:42,994 - INFO - 找到 en->de: 206112 个样本
2025-11-03 12:23:43,050 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-11-03 12:23:43,050 - INFO - 加载验证数据...
2025-11-03 12:23:43,096 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-11-03 12:23:43,096 - INFO - 提取 en->de 的翻译对
2025-11-03 12:23:43,096 - INFO - 找到 en->de: 888 个样本
2025-11-03 12:23:43,096 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-11-03 12:23:43,096 - INFO - 加载测试数据...
2025-11-03 12:23:43,200 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-11-03 12:23:43,201 - INFO - 提取 en->de 的翻译对
2025-11-03 12:23:43,201 - INFO - 找到 en->de: 8079 个样本
2025-11-03 12:23:43,203 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-11-03 12:23:43,203 - INFO - 开始构建分词器...
2025-11-03 12:23:43,278 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-11-03 12:23:44,706 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-11-03 12:23:46,764 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-11-03 12:23:46,809 - INFO - 数据加载器创建完成
2025-11-03 12:23:46,809 - INFO - 训练集: 206112 个样本
2025-11-03 12:23:46,809 - INFO - 验证集: 888 个样本
2025-11-03 12:23:46,809 - INFO - 测试集: 8079 个样本
2025-11-03 12:23:47,839 - INFO - 加载模型: checkpoints/best_model.pth
2025-11-03 12:23:48,092 - INFO - 从检查点加载模型状态
2025-11-03 12:23:48,092 - INFO - 模型加载成功
2025-11-03 12:23:48,102 - INFO - 加载训练数据...
2025-11-03 12:23:52,582 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-11-03 12:23:52,582 - INFO - 训练数据结构:
2025-11-03 12:23:52,582 - INFO - 数据结构分析:
2025-11-03 12:23:52,582 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 12:23:52,582 - INFO - 
英语相关语言对:
2025-11-03 12:23:52,582 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 12:23:52,582 - INFO -   en->de: 206112 个样本
2025-11-03 12:23:52,582 - INFO - 
德语相关语言对:
2025-11-03 12:23:52,582 - INFO - 德语->其他语言: ['en']
2025-11-03 12:23:52,583 - INFO -   de->en: 206112 个样本
2025-11-03 12:23:52,583 - INFO - 提取 en->de 的翻译对
2025-11-03 12:23:52,583 - INFO - 找到 en->de: 206112 个样本
2025-11-03 12:23:52,639 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-11-03 12:23:52,639 - INFO - 加载验证数据...
2025-11-03 12:23:52,684 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-11-03 12:23:52,684 - INFO - 提取 en->de 的翻译对
2025-11-03 12:23:52,685 - INFO - 找到 en->de: 888 个样本
2025-11-03 12:23:52,685 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-11-03 12:23:52,685 - INFO - 加载测试数据...
2025-11-03 12:23:52,788 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-11-03 12:23:52,788 - INFO - 提取 en->de 的翻译对
2025-11-03 12:23:52,788 - INFO - 找到 en->de: 8079 个样本
2025-11-03 12:23:52,790 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-11-03 12:23:52,790 - INFO - 开始构建分词器...
2025-11-03 12:23:52,845 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-11-03 12:23:54,106 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-11-03 12:23:56,169 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-11-03 12:23:56,211 - INFO - 数据加载器创建完成
2025-11-03 12:23:56,211 - INFO - 训练集: 206112 个样本
2025-11-03 12:23:56,212 - INFO - 验证集: 888 个样本
2025-11-03 12:23:56,212 - INFO - 测试集: 8079 个样本
2025-11-03 12:23:57,253 - INFO - 计算验证集指标...
2025-11-03 12:26:28,149 - INFO - 计算精确匹配率...
2025-11-03 12:26:44,500 - INFO - 完整结果已保存到: results\validation_results.json
2025-11-03 12:26:44,500 - INFO - 简洁报告已保存到: results\validation_report.txt
2025-11-03 12:28:18,902 - INFO - 使用设备: cpu
2025-11-03 12:28:18,902 - INFO - 加载数据和构建分词器...
2025-11-03 12:28:18,902 - INFO - 加载训练数据...
2025-11-03 12:28:23,692 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-11-03 12:28:23,692 - INFO - 训练数据结构:
2025-11-03 12:28:23,693 - INFO - 数据结构分析:
2025-11-03 12:28:23,693 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 12:28:23,693 - INFO - 
英语相关语言对:
2025-11-03 12:28:23,693 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 12:28:23,693 - INFO -   en->de: 206112 个样本
2025-11-03 12:28:23,693 - INFO - 
德语相关语言对:
2025-11-03 12:28:23,693 - INFO - 德语->其他语言: ['en']
2025-11-03 12:28:23,693 - INFO -   de->en: 206112 个样本
2025-11-03 12:28:23,693 - INFO - 提取 en->de 的翻译对
2025-11-03 12:28:23,693 - INFO - 找到 en->de: 206112 个样本
2025-11-03 12:28:23,753 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-11-03 12:28:23,753 - INFO - 加载验证数据...
2025-11-03 12:28:23,814 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-11-03 12:28:23,814 - INFO - 提取 en->de 的翻译对
2025-11-03 12:28:23,815 - INFO - 找到 en->de: 888 个样本
2025-11-03 12:28:23,815 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-11-03 12:28:23,815 - INFO - 加载测试数据...
2025-11-03 12:28:23,932 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-11-03 12:28:23,932 - INFO - 提取 en->de 的翻译对
2025-11-03 12:28:23,932 - INFO - 找到 en->de: 8079 个样本
2025-11-03 12:28:23,935 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-11-03 12:28:23,935 - INFO - 开始构建分词器...
2025-11-03 12:28:23,993 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-11-03 12:28:25,416 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-11-03 12:28:27,631 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-11-03 12:28:27,673 - INFO - 数据加载器创建完成
2025-11-03 12:28:27,673 - INFO - 训练集: 206112 个样本
2025-11-03 12:28:27,673 - INFO - 验证集: 888 个样本
2025-11-03 12:28:27,673 - INFO - 测试集: 8079 个样本
2025-11-03 12:28:28,679 - INFO - 加载模型: checkpoints/best_model.pth
2025-11-03 12:28:28,939 - INFO - 从检查点加载模型状态
2025-11-03 12:28:28,939 - INFO - 模型加载成功
2025-11-03 12:28:28,950 - INFO - 加载训练数据...
2025-11-03 12:28:33,431 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-11-03 12:28:33,431 - INFO - 训练数据结构:
2025-11-03 12:28:33,431 - INFO - 数据结构分析:
2025-11-03 12:28:33,431 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 12:28:33,432 - INFO - 
英语相关语言对:
2025-11-03 12:28:33,432 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 12:28:33,432 - INFO -   en->de: 206112 个样本
2025-11-03 12:28:33,432 - INFO - 
德语相关语言对:
2025-11-03 12:28:33,432 - INFO - 德语->其他语言: ['en']
2025-11-03 12:28:33,432 - INFO -   de->en: 206112 个样本
2025-11-03 12:28:33,432 - INFO - 提取 en->de 的翻译对
2025-11-03 12:28:33,432 - INFO - 找到 en->de: 206112 个样本
2025-11-03 12:28:33,489 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-11-03 12:28:33,489 - INFO - 加载验证数据...
2025-11-03 12:28:33,535 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-11-03 12:28:33,535 - INFO - 提取 en->de 的翻译对
2025-11-03 12:28:33,535 - INFO - 找到 en->de: 888 个样本
2025-11-03 12:28:33,535 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-11-03 12:28:33,536 - INFO - 加载测试数据...
2025-11-03 12:28:33,638 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-11-03 12:28:33,638 - INFO - 提取 en->de 的翻译对
2025-11-03 12:28:33,638 - INFO - 找到 en->de: 8079 个样本
2025-11-03 12:28:33,641 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-11-03 12:28:33,642 - INFO - 开始构建分词器...
2025-11-03 12:28:33,693 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-11-03 12:28:34,921 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-11-03 12:28:37,024 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-11-03 12:28:37,063 - INFO - 数据加载器创建完成
2025-11-03 12:28:37,063 - INFO - 训练集: 206112 个样本
2025-11-03 12:28:37,063 - INFO - 验证集: 888 个样本
2025-11-03 12:28:37,063 - INFO - 测试集: 8079 个样本
2025-11-03 12:28:38,088 - INFO - 计算验证集指标...
2025-11-03 12:31:06,740 - INFO - 计算测试集指标...
2025-11-03 12:52:11,658 - INFO - 计算精确匹配率...
2025-11-03 12:52:27,261 - INFO - 完整结果已保存到: results\validation_results.json
2025-11-03 12:52:27,261 - INFO - 简洁报告已保存到: results\validation_report.txt
2025-11-03 12:57:36,513 - INFO - 使用设备: cpu
2025-11-03 12:57:36,513 - INFO - 加载数据和构建分词器...
2025-11-03 12:57:36,515 - INFO - 加载训练数据...
2025-11-03 12:57:41,161 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-11-03 12:57:41,161 - INFO - 训练数据结构:
2025-11-03 12:57:41,161 - INFO - 数据结构分析:
2025-11-03 12:57:41,161 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 12:57:41,161 - INFO - 
英语相关语言对:
2025-11-03 12:57:41,161 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 12:57:41,161 - INFO -   en->de: 206112 个样本
2025-11-03 12:57:41,161 - INFO - 
德语相关语言对:
2025-11-03 12:57:41,162 - INFO - 德语->其他语言: ['en']
2025-11-03 12:57:41,162 - INFO -   de->en: 206112 个样本
2025-11-03 12:57:41,162 - INFO - 提取 en->de 的翻译对
2025-11-03 12:57:41,162 - INFO - 找到 en->de: 206112 个样本
2025-11-03 12:57:41,217 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-11-03 12:57:41,218 - INFO - 加载验证数据...
2025-11-03 12:57:41,263 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-11-03 12:57:41,263 - INFO - 提取 en->de 的翻译对
2025-11-03 12:57:41,263 - INFO - 找到 en->de: 888 个样本
2025-11-03 12:57:41,263 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-11-03 12:57:41,263 - INFO - 加载测试数据...
2025-11-03 12:57:41,367 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-11-03 12:57:41,367 - INFO - 提取 en->de 的翻译对
2025-11-03 12:57:41,367 - INFO - 找到 en->de: 8079 个样本
2025-11-03 12:57:41,370 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-11-03 12:57:41,370 - INFO - 开始构建分词器...
2025-11-03 12:57:41,423 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-11-03 12:57:42,725 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-11-03 12:57:44,766 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-11-03 12:57:44,805 - INFO - 数据加载器创建完成
2025-11-03 12:57:44,805 - INFO - 训练集: 206112 个样本
2025-11-03 12:57:44,805 - INFO - 验证集: 888 个样本
2025-11-03 12:57:44,805 - INFO - 测试集: 8079 个样本
2025-11-03 12:57:45,811 - INFO - 加载模型: checkpoints/best_model_xiaorong.pth
2025-11-03 12:59:06,959 - INFO - 使用设备: cpu
2025-11-03 12:59:06,960 - INFO - 加载数据和构建分词器...
2025-11-03 12:59:06,960 - INFO - 加载训练数据...
2025-11-03 12:59:11,500 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-11-03 12:59:11,500 - INFO - 训练数据结构:
2025-11-03 12:59:11,500 - INFO - 数据结构分析:
2025-11-03 12:59:11,500 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 12:59:11,500 - INFO - 
英语相关语言对:
2025-11-03 12:59:11,500 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 12:59:11,500 - INFO -   en->de: 206112 个样本
2025-11-03 12:59:11,502 - INFO - 
德语相关语言对:
2025-11-03 12:59:11,502 - INFO - 德语->其他语言: ['en']
2025-11-03 12:59:11,502 - INFO -   de->en: 206112 个样本
2025-11-03 12:59:11,502 - INFO - 提取 en->de 的翻译对
2025-11-03 12:59:11,502 - INFO - 找到 en->de: 206112 个样本
2025-11-03 12:59:11,556 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-11-03 12:59:11,557 - INFO - 加载验证数据...
2025-11-03 12:59:11,602 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-11-03 12:59:11,602 - INFO - 提取 en->de 的翻译对
2025-11-03 12:59:11,602 - INFO - 找到 en->de: 888 个样本
2025-11-03 12:59:11,603 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-11-03 12:59:11,603 - INFO - 加载测试数据...
2025-11-03 12:59:11,705 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-11-03 12:59:11,705 - INFO - 提取 en->de 的翻译对
2025-11-03 12:59:11,705 - INFO - 找到 en->de: 8079 个样本
2025-11-03 12:59:11,708 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-11-03 12:59:11,708 - INFO - 开始构建分词器...
2025-11-03 12:59:11,758 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-11-03 12:59:13,027 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-11-03 12:59:15,094 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-11-03 12:59:15,133 - INFO - 数据加载器创建完成
2025-11-03 12:59:15,133 - INFO - 训练集: 206112 个样本
2025-11-03 12:59:15,133 - INFO - 验证集: 888 个样本
2025-11-03 12:59:15,133 - INFO - 测试集: 8079 个样本
2025-11-03 12:59:16,116 - INFO - 加载模型: checkpoints/best_model_xiaorong.pth
2025-11-03 13:01:55,215 - INFO - 使用设备: cpu
2025-11-03 13:01:55,215 - INFO - 加载数据和构建分词器...
2025-11-03 13:01:55,216 - INFO - 加载训练数据...
2025-11-03 13:01:59,846 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-11-03 13:01:59,846 - INFO - 训练数据结构:
2025-11-03 13:01:59,846 - INFO - 数据结构分析:
2025-11-03 13:01:59,846 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 13:01:59,846 - INFO - 
英语相关语言对:
2025-11-03 13:01:59,846 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 13:01:59,846 - INFO -   en->de: 206112 个样本
2025-11-03 13:01:59,846 - INFO - 
德语相关语言对:
2025-11-03 13:01:59,847 - INFO - 德语->其他语言: ['en']
2025-11-03 13:01:59,847 - INFO -   de->en: 206112 个样本
2025-11-03 13:01:59,847 - INFO - 提取 en->de 的翻译对
2025-11-03 13:01:59,847 - INFO - 找到 en->de: 206112 个样本
2025-11-03 13:01:59,902 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-11-03 13:01:59,903 - INFO - 加载验证数据...
2025-11-03 13:01:59,948 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-11-03 13:01:59,948 - INFO - 提取 en->de 的翻译对
2025-11-03 13:01:59,948 - INFO - 找到 en->de: 888 个样本
2025-11-03 13:01:59,948 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-11-03 13:01:59,948 - INFO - 加载测试数据...
2025-11-03 13:02:00,051 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-11-03 13:02:00,051 - INFO - 提取 en->de 的翻译对
2025-11-03 13:02:00,051 - INFO - 找到 en->de: 8079 个样本
2025-11-03 13:02:00,054 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-11-03 13:02:00,055 - INFO - 开始构建分词器...
2025-11-03 13:02:00,106 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-11-03 13:02:01,355 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-11-03 13:02:03,396 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-11-03 13:02:03,434 - INFO - 数据加载器创建完成
2025-11-03 13:02:03,434 - INFO - 训练集: 206112 个样本
2025-11-03 13:02:03,434 - INFO - 验证集: 888 个样本
2025-11-03 13:02:03,435 - INFO - 测试集: 8079 个样本
2025-11-03 13:02:04,438 - INFO - 加载模型: checkpoints/best_model.pth
2025-11-03 13:02:04,715 - INFO - 从检查点加载模型状态
2025-11-03 13:02:04,715 - INFO - 模型加载成功
2025-11-03 13:02:04,725 - INFO - 加载训练数据...
2025-11-03 13:02:09,260 - INFO - 成功加载 data\IWSLT2017\iwslt2017_train.pkl
2025-11-03 13:02:09,260 - INFO - 训练数据结构:
2025-11-03 13:02:09,260 - INFO - 数据结构分析:
2025-11-03 13:02:09,260 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 13:02:09,261 - INFO - 
英语相关语言对:
2025-11-03 13:02:09,261 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-11-03 13:02:09,261 - INFO -   en->de: 206112 个样本
2025-11-03 13:02:09,261 - INFO - 
德语相关语言对:
2025-11-03 13:02:09,261 - INFO - 德语->其他语言: ['en']
2025-11-03 13:02:09,261 - INFO -   de->en: 206112 个样本
2025-11-03 13:02:09,261 - INFO - 提取 en->de 的翻译对
2025-11-03 13:02:09,261 - INFO - 找到 en->de: 206112 个样本
2025-11-03 13:02:09,317 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-11-03 13:02:09,317 - INFO - 加载验证数据...
2025-11-03 13:02:09,370 - INFO - 成功加载 data\IWSLT2017\iwslt2017_validation.pkl
2025-11-03 13:02:09,370 - INFO - 提取 en->de 的翻译对
2025-11-03 13:02:09,370 - INFO - 找到 en->de: 888 个样本
2025-11-03 13:02:09,371 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-11-03 13:02:09,371 - INFO - 加载测试数据...
2025-11-03 13:02:09,480 - INFO - 成功加载 data\IWSLT2017\iwslt2017_test.pkl
2025-11-03 13:02:09,480 - INFO - 提取 en->de 的翻译对
2025-11-03 13:02:09,480 - INFO - 找到 en->de: 8079 个样本
2025-11-03 13:02:09,483 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-11-03 13:02:09,483 - INFO - 开始构建分词器...
2025-11-03 13:02:09,534 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子
2025-11-03 13:02:10,842 - INFO - 源语言分词器训练完成，词汇表大小: 5000
2025-11-03 13:02:12,979 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-11-03 13:02:13,025 - INFO - 数据加载器创建完成
2025-11-03 13:02:13,026 - INFO - 训练集: 206112 个样本
2025-11-03 13:02:13,026 - INFO - 验证集: 888 个样本
2025-11-03 13:02:13,026 - INFO - 测试集: 8079 个样本
2025-11-03 13:02:14,074 - INFO - 计算验证集指标...
2025-11-03 13:04:29,592 - INFO - 计算测试集指标...
2025-11-03 13:22:48,519 - INFO - 计算精确匹配率...
2025-11-03 13:23:01,166 - INFO - 完整结果已保存到: results\validation_results.json
2025-11-03 13:23:01,166 - INFO - 简洁报告已保存到: results\validation_report.txt
