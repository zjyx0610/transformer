nohup: 忽略输入
2025-10-23 10:01:05,199 - INFO - 使用设备: cuda:2
2025-10-23 10:01:05,199 - INFO - 加载数据...
2025-10-23 10:01:05,199 - INFO - 加载训练数据...
2025-10-23 10:01:09,002 - INFO - 成功加载 data/IWSLT2017/iwslt2017_train.pkl
2025-10-23 10:01:09,003 - INFO - 训练数据结构:
2025-10-23 10:01:09,003 - INFO - 数据结构分析:
2025-10-23 10:01:09,003 - INFO - 顶级键(源语言): ['ar', 'de', 'en', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-23 10:01:09,003 - INFO - 
英语相关语言对:
2025-10-23 10:01:09,003 - INFO - 英语->其他语言: ['ar', 'de', 'fr', 'it', 'ja', 'ko', 'nl', 'ro', 'zh']
2025-10-23 10:01:09,003 - INFO -   en->de: 206112 个样本
2025-10-23 10:01:09,003 - INFO - 
德语相关语言对:
2025-10-23 10:01:09,003 - INFO - 德语->其他语言: ['en']
2025-10-23 10:01:09,003 - INFO -   de->en: 206112 个样本
2025-10-23 10:01:09,003 - INFO - 提取 en->de 的翻译对
2025-10-23 10:01:09,003 - INFO - 找到 en->de: 206112 个样本
2025-10-23 10:01:09,037 - INFO - 最终提取到 206112 个en句子和 206112 个de句子
2025-10-23 10:01:09,037 - INFO - 加载验证数据...
2025-10-23 10:01:09,068 - INFO - 成功加载 data/IWSLT2017/iwslt2017_validation.pkl
2025-10-23 10:01:09,068 - INFO - 提取 en->de 的翻译对
2025-10-23 10:01:09,068 - INFO - 找到 en->de: 888 个样本
2025-10-23 10:01:09,068 - INFO - 最终提取到 888 个en句子和 888 个de句子
2025-10-23 10:01:09,068 - INFO - 加载测试数据...
2025-10-23 10:01:09,150 - INFO - 成功加载 data/IWSLT2017/iwslt2017_test.pkl
2025-10-23 10:01:09,151 - INFO - 提取 en->de 的翻译对
2025-10-23 10:01:09,151 - INFO - 找到 en->de: 8079 个样本
2025-10-23 10:01:09,152 - INFO - 最终提取到 8079 个en句子和 8079 个de句子
2025-10-23 10:01:09,152 - INFO - 开始构建分词器...
2025-10-23 10:01:09,183 - INFO - 过滤后: 206112 个源语言句子, 206112 个目标语言句子



2025-10-23 10:01:10,757 - INFO - 源语言分词器训练完成，词汇表大小: 5000



2025-10-23 10:01:13,752 - INFO - 目标语言分词器训练完成，词汇表大小: 5000
2025-10-23 10:01:13,814 - INFO - 数据加载器创建完成
2025-10-23 10:01:13,814 - INFO - 训练集: 206112 个样本
2025-10-23 10:01:13,814 - INFO - 验证集: 888 个样本
2025-10-23 10:01:13,814 - INFO - 测试集: 8079 个样本
2025-10-23 10:01:15,031 - INFO - 源语言词汇表大小: 5000
2025-10-23 10:01:15,031 - INFO - 目标语言词汇表大小: 5000
2025-10-23 10:01:15,031 - INFO - 创建模型...
2025-10-23 10:01:16,503 - INFO - 总参数数: 15,041,416
2025-10-23 10:01:16,503 - INFO - 可训练参数数: 15,041,416
2025-10-23 10:01:16,504 - INFO - 开始训练...
2025-10-23 10:01:17,520 - INFO - Batch 0, Loss: 8.5814, LR: 0.000000
2025-10-23 10:01:29,631 - INFO - Batch 100, Loss: 8.0524, LR: 0.000005
2025-10-23 10:01:41,737 - INFO - Batch 200, Loss: 7.5594, LR: 0.000010
2025-10-23 10:01:54,118 - INFO - Batch 300, Loss: 7.1740, LR: 0.000015
2025-10-23 10:02:06,501 - INFO - Batch 400, Loss: 6.8133, LR: 0.000020
2025-10-23 10:02:18,946 - INFO - Batch 500, Loss: 6.8655, LR: 0.000025
2025-10-23 10:02:31,156 - INFO - Batch 600, Loss: 6.7730, LR: 0.000030
2025-10-23 10:02:43,413 - INFO - Batch 700, Loss: 6.6172, LR: 0.000035
2025-10-23 10:02:55,637 - INFO - Batch 800, Loss: 6.3731, LR: 0.000040
2025-10-23 10:03:07,807 - INFO - Batch 900, Loss: 6.4287, LR: 0.000045
2025-10-23 10:03:20,242 - INFO - Batch 1000, Loss: 6.5323, LR: 0.000050
2025-10-23 10:03:32,573 - INFO - Batch 1100, Loss: 6.3293, LR: 0.000055
2025-10-23 10:03:44,805 - INFO - Batch 1200, Loss: 6.2985, LR: 0.000060
2025-10-23 10:03:57,049 - INFO - Batch 1300, Loss: 6.2636, LR: 0.000065
2025-10-23 10:04:09,280 - INFO - Batch 1400, Loss: 6.2506, LR: 0.000070
2025-10-23 10:04:21,522 - INFO - Batch 1500, Loss: 6.0095, LR: 0.000075
2025-10-23 10:04:33,790 - INFO - Batch 1600, Loss: 6.1107, LR: 0.000080
2025-10-23 10:04:46,241 - INFO - Batch 1700, Loss: 6.0156, LR: 0.000085
2025-10-23 10:04:58,637 - INFO - Batch 1800, Loss: 5.7954, LR: 0.000090
2025-10-23 10:05:11,035 - INFO - Batch 1900, Loss: 6.0070, LR: 0.000095
/home/jiahuanhe/.conda/envs/env_hjh/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-10-23 10:05:23,369 - INFO - Batch 2000, Loss: 5.7944, LR: 0.000100
2025-10-23 10:05:35,719 - INFO - Batch 2100, Loss: 5.7980, LR: 0.000100
2025-10-23 10:05:48,093 - INFO - Batch 2200, Loss: 5.6265, LR: 0.000100
2025-10-23 10:06:00,462 - INFO - Batch 2300, Loss: 5.5629, LR: 0.000100
2025-10-23 10:06:12,810 - INFO - Batch 2400, Loss: 5.6509, LR: 0.000100
2025-10-23 10:06:25,171 - INFO - Batch 2500, Loss: 5.4734, LR: 0.000100
2025-10-23 10:06:37,576 - INFO - Batch 2600, Loss: 5.5614, LR: 0.000100
2025-10-23 10:06:49,925 - INFO - Batch 2700, Loss: 5.4209, LR: 0.000100
2025-10-23 10:07:02,256 - INFO - Batch 2800, Loss: 5.2489, LR: 0.000100
2025-10-23 10:07:14,685 - INFO - Batch 2900, Loss: 5.3490, LR: 0.000100
2025-10-23 10:07:27,266 - INFO - Batch 3000, Loss: 5.3837, LR: 0.000100
2025-10-23 10:07:39,730 - INFO - Batch 3100, Loss: 5.4839, LR: 0.000100
2025-10-23 10:07:52,245 - INFO - Batch 3200, Loss: 5.4533, LR: 0.000100
2025-10-23 10:08:04,748 - INFO - Batch 3300, Loss: 5.4702, LR: 0.000100
2025-10-23 10:08:17,219 - INFO - Batch 3400, Loss: 5.2306, LR: 0.000100
2025-10-23 10:08:29,643 - INFO - Batch 3500, Loss: 5.1929, LR: 0.000100
2025-10-23 10:08:42,175 - INFO - Batch 3600, Loss: 5.0529, LR: 0.000100
2025-10-23 10:08:54,693 - INFO - Batch 3700, Loss: 5.3073, LR: 0.000100
2025-10-23 10:09:07,209 - INFO - Batch 3800, Loss: 5.0442, LR: 0.000100
2025-10-23 10:09:19,733 - INFO - Batch 3900, Loss: 5.1600, LR: 0.000100
2025-10-23 10:09:32,270 - INFO - Batch 4000, Loss: 5.1884, LR: 0.000100
2025-10-23 10:09:44,836 - INFO - Batch 4100, Loss: 5.0921, LR: 0.000100
2025-10-23 10:09:57,335 - INFO - Batch 4200, Loss: 5.0253, LR: 0.000100
2025-10-23 10:10:09,668 - INFO - Batch 4300, Loss: 5.0989, LR: 0.000100
2025-10-23 10:10:21,958 - INFO - Batch 4400, Loss: 5.1395, LR: 0.000100
2025-10-23 10:10:34,276 - INFO - Batch 4500, Loss: 5.0861, LR: 0.000100
2025-10-23 10:10:46,807 - INFO - Batch 4600, Loss: 5.1356, LR: 0.000100
2025-10-23 10:10:59,266 - INFO - Batch 4700, Loss: 4.9506, LR: 0.000100
2025-10-23 10:11:11,600 - INFO - Batch 4800, Loss: 5.0570, LR: 0.000100
2025-10-23 10:11:23,896 - INFO - Batch 4900, Loss: 5.0077, LR: 0.000100
2025-10-23 10:11:36,210 - INFO - Batch 5000, Loss: 5.1244, LR: 0.000100
2025-10-23 10:11:48,426 - INFO - Batch 5100, Loss: 4.9775, LR: 0.000100
2025-10-23 10:12:00,664 - INFO - Batch 5200, Loss: 5.1658, LR: 0.000100
2025-10-23 10:12:12,992 - INFO - Batch 5300, Loss: 4.7234, LR: 0.000100
2025-10-23 10:12:25,447 - INFO - Batch 5400, Loss: 4.6002, LR: 0.000100
2025-10-23 10:12:37,903 - INFO - Batch 5500, Loss: 4.7466, LR: 0.000100
2025-10-23 10:12:50,226 - INFO - Batch 5600, Loss: 4.8995, LR: 0.000100
2025-10-23 10:13:02,557 - INFO - Batch 5700, Loss: 4.6375, LR: 0.000100
2025-10-23 10:13:14,922 - INFO - Batch 5800, Loss: 4.8406, LR: 0.000100
2025-10-23 10:13:27,216 - INFO - Batch 5900, Loss: 5.0204, LR: 0.000100
2025-10-23 10:13:39,514 - INFO - Batch 6000, Loss: 4.8586, LR: 0.000100
2025-10-23 10:13:51,822 - INFO - Batch 6100, Loss: 4.9978, LR: 0.000100
2025-10-23 10:14:04,199 - INFO - Batch 6200, Loss: 5.0074, LR: 0.000100
2025-10-23 10:14:16,485 - INFO - Batch 6300, Loss: 4.7871, LR: 0.000100
2025-10-23 10:14:28,781 - INFO - Batch 6400, Loss: 4.8556, LR: 0.000100
2025-10-23 10:14:34,936 - INFO - Epoch 1/30: Train Loss: 5.6155, Val Loss: 4.7914, LR: 0.000100
2025-10-23 10:14:35,125 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 10:14:35,256 - INFO - Batch 0, Loss: 4.7619, LR: 0.000100
2025-10-23 10:14:47,596 - INFO - Batch 100, Loss: 4.4559, LR: 0.000100
2025-10-23 10:14:59,969 - INFO - Batch 200, Loss: 4.6318, LR: 0.000100
2025-10-23 10:15:12,364 - INFO - Batch 300, Loss: 4.8092, LR: 0.000100
2025-10-23 10:15:24,699 - INFO - Batch 400, Loss: 4.5485, LR: 0.000100
2025-10-23 10:15:37,024 - INFO - Batch 500, Loss: 4.8073, LR: 0.000100
2025-10-23 10:15:49,403 - INFO - Batch 600, Loss: 4.8656, LR: 0.000100
2025-10-23 10:16:01,823 - INFO - Batch 700, Loss: 4.8414, LR: 0.000100
2025-10-23 10:16:14,112 - INFO - Batch 800, Loss: 4.5939, LR: 0.000100
2025-10-23 10:16:26,461 - INFO - Batch 900, Loss: 4.7654, LR: 0.000100
2025-10-23 10:16:38,789 - INFO - Batch 1000, Loss: 4.8366, LR: 0.000100
2025-10-23 10:16:51,167 - INFO - Batch 1100, Loss: 4.7429, LR: 0.000100
2025-10-23 10:17:03,500 - INFO - Batch 1200, Loss: 4.6326, LR: 0.000100
2025-10-23 10:17:15,812 - INFO - Batch 1300, Loss: 4.7483, LR: 0.000100
2025-10-23 10:17:28,246 - INFO - Batch 1400, Loss: 4.6624, LR: 0.000100
2025-10-23 10:17:40,549 - INFO - Batch 1500, Loss: 4.4237, LR: 0.000100
2025-10-23 10:17:52,849 - INFO - Batch 1600, Loss: 4.5565, LR: 0.000100
2025-10-23 10:18:05,178 - INFO - Batch 1700, Loss: 4.4005, LR: 0.000100
2025-10-23 10:18:17,480 - INFO - Batch 1800, Loss: 4.6189, LR: 0.000100
2025-10-23 10:18:29,791 - INFO - Batch 1900, Loss: 4.5098, LR: 0.000100
2025-10-23 10:18:42,068 - INFO - Batch 2000, Loss: 4.6357, LR: 0.000100
2025-10-23 10:18:54,357 - INFO - Batch 2100, Loss: 4.3278, LR: 0.000100
2025-10-23 10:19:06,678 - INFO - Batch 2200, Loss: 4.4198, LR: 0.000100
2025-10-23 10:19:18,974 - INFO - Batch 2300, Loss: 4.5596, LR: 0.000100
2025-10-23 10:19:31,249 - INFO - Batch 2400, Loss: 4.7352, LR: 0.000100
2025-10-23 10:19:43,597 - INFO - Batch 2500, Loss: 4.3527, LR: 0.000100
2025-10-23 10:19:55,872 - INFO - Batch 2600, Loss: 4.4897, LR: 0.000100
2025-10-23 10:20:08,148 - INFO - Batch 2700, Loss: 4.5807, LR: 0.000100
2025-10-23 10:20:20,434 - INFO - Batch 2800, Loss: 4.4842, LR: 0.000100
2025-10-23 10:20:32,718 - INFO - Batch 2900, Loss: 4.3412, LR: 0.000100
2025-10-23 10:20:44,985 - INFO - Batch 3000, Loss: 4.5133, LR: 0.000100
2025-10-23 10:20:57,271 - INFO - Batch 3100, Loss: 4.5686, LR: 0.000100
2025-10-23 10:21:09,562 - INFO - Batch 3200, Loss: 4.4772, LR: 0.000100
2025-10-23 10:21:21,839 - INFO - Batch 3300, Loss: 4.5360, LR: 0.000100
2025-10-23 10:21:34,135 - INFO - Batch 3400, Loss: 4.4593, LR: 0.000100
2025-10-23 10:21:46,407 - INFO - Batch 3500, Loss: 4.5625, LR: 0.000100
2025-10-23 10:21:58,689 - INFO - Batch 3600, Loss: 4.4520, LR: 0.000100
2025-10-23 10:22:10,965 - INFO - Batch 3700, Loss: 4.4886, LR: 0.000100
2025-10-23 10:22:23,439 - INFO - Batch 3800, Loss: 4.3314, LR: 0.000100
2025-10-23 10:22:35,738 - INFO - Batch 3900, Loss: 4.3184, LR: 0.000100
2025-10-23 10:22:48,067 - INFO - Batch 4000, Loss: 4.4989, LR: 0.000100
2025-10-23 10:23:00,389 - INFO - Batch 4100, Loss: 4.2545, LR: 0.000100
2025-10-23 10:23:12,753 - INFO - Batch 4200, Loss: 4.3146, LR: 0.000099
2025-10-23 10:23:25,098 - INFO - Batch 4300, Loss: 4.5006, LR: 0.000099
2025-10-23 10:23:37,444 - INFO - Batch 4400, Loss: 3.9045, LR: 0.000099
2025-10-23 10:23:49,797 - INFO - Batch 4500, Loss: 3.9493, LR: 0.000099
2025-10-23 10:24:02,107 - INFO - Batch 4600, Loss: 4.3093, LR: 0.000099
2025-10-23 10:24:14,411 - INFO - Batch 4700, Loss: 4.4597, LR: 0.000099
2025-10-23 10:24:26,735 - INFO - Batch 4800, Loss: 4.2296, LR: 0.000099
2025-10-23 10:24:39,003 - INFO - Batch 4900, Loss: 4.3045, LR: 0.000099
2025-10-23 10:24:51,343 - INFO - Batch 5000, Loss: 3.9749, LR: 0.000099
2025-10-23 10:25:03,607 - INFO - Batch 5100, Loss: 3.9931, LR: 0.000099
2025-10-23 10:25:15,901 - INFO - Batch 5200, Loss: 4.4198, LR: 0.000099
2025-10-23 10:25:28,180 - INFO - Batch 5300, Loss: 3.9487, LR: 0.000099
2025-10-23 10:25:40,458 - INFO - Batch 5400, Loss: 4.2585, LR: 0.000099
2025-10-23 10:25:52,738 - INFO - Batch 5500, Loss: 4.1529, LR: 0.000099
2025-10-23 10:26:05,015 - INFO - Batch 5600, Loss: 4.0565, LR: 0.000099
2025-10-23 10:26:17,337 - INFO - Batch 5700, Loss: 3.9589, LR: 0.000099
2025-10-23 10:26:29,654 - INFO - Batch 5800, Loss: 4.2696, LR: 0.000099
2025-10-23 10:26:41,901 - INFO - Batch 5900, Loss: 3.7338, LR: 0.000099
2025-10-23 10:26:54,127 - INFO - Batch 6000, Loss: 4.0758, LR: 0.000099
2025-10-23 10:27:06,381 - INFO - Batch 6100, Loss: 4.1548, LR: 0.000099
2025-10-23 10:27:18,721 - INFO - Batch 6200, Loss: 3.8933, LR: 0.000099
2025-10-23 10:27:31,023 - INFO - Batch 6300, Loss: 4.0372, LR: 0.000099
2025-10-23 10:27:43,309 - INFO - Batch 6400, Loss: 3.8638, LR: 0.000099
2025-10-23 10:27:49,438 - INFO - Epoch 2/30: Train Loss: 4.4215, Val Loss: 3.9644, LR: 0.000099
2025-10-23 10:27:49,682 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 10:27:49,813 - INFO - Batch 0, Loss: 4.0136, LR: 0.000099
2025-10-23 10:28:02,138 - INFO - Batch 100, Loss: 4.0321, LR: 0.000099
2025-10-23 10:28:14,397 - INFO - Batch 200, Loss: 3.9468, LR: 0.000099
2025-10-23 10:28:26,770 - INFO - Batch 300, Loss: 3.9676, LR: 0.000099
2025-10-23 10:28:39,048 - INFO - Batch 400, Loss: 4.1216, LR: 0.000099
2025-10-23 10:28:51,336 - INFO - Batch 500, Loss: 3.9447, LR: 0.000099
2025-10-23 10:29:03,593 - INFO - Batch 600, Loss: 3.7278, LR: 0.000099
2025-10-23 10:29:15,874 - INFO - Batch 700, Loss: 3.9327, LR: 0.000099
2025-10-23 10:29:28,173 - INFO - Batch 800, Loss: 4.0894, LR: 0.000099
2025-10-23 10:29:40,476 - INFO - Batch 900, Loss: 3.9009, LR: 0.000099
2025-10-23 10:29:52,761 - INFO - Batch 1000, Loss: 3.8719, LR: 0.000099
2025-10-23 10:30:05,057 - INFO - Batch 1100, Loss: 3.9789, LR: 0.000099
2025-10-23 10:30:17,347 - INFO - Batch 1200, Loss: 3.8404, LR: 0.000099
2025-10-23 10:30:29,667 - INFO - Batch 1300, Loss: 3.7553, LR: 0.000099
2025-10-23 10:30:41,965 - INFO - Batch 1400, Loss: 3.9610, LR: 0.000099
2025-10-23 10:30:54,272 - INFO - Batch 1500, Loss: 3.9486, LR: 0.000099
2025-10-23 10:31:06,567 - INFO - Batch 1600, Loss: 3.9891, LR: 0.000099
2025-10-23 10:31:18,876 - INFO - Batch 1700, Loss: 3.7508, LR: 0.000099
2025-10-23 10:31:31,217 - INFO - Batch 1800, Loss: 3.7335, LR: 0.000099
2025-10-23 10:31:43,577 - INFO - Batch 1900, Loss: 3.4902, LR: 0.000099
2025-10-23 10:31:55,946 - INFO - Batch 2000, Loss: 3.8733, LR: 0.000099
2025-10-23 10:32:08,345 - INFO - Batch 2100, Loss: 3.7785, LR: 0.000099
2025-10-23 10:32:20,781 - INFO - Batch 2200, Loss: 3.6971, LR: 0.000099
2025-10-23 10:32:33,172 - INFO - Batch 2300, Loss: 3.8342, LR: 0.000099
2025-10-23 10:32:45,510 - INFO - Batch 2400, Loss: 3.7505, LR: 0.000099
2025-10-23 10:32:57,912 - INFO - Batch 2500, Loss: 3.7459, LR: 0.000099
2025-10-23 10:33:10,272 - INFO - Batch 2600, Loss: 3.6611, LR: 0.000099
2025-10-23 10:33:22,612 - INFO - Batch 2700, Loss: 3.9073, LR: 0.000099
2025-10-23 10:33:34,951 - INFO - Batch 2800, Loss: 3.6434, LR: 0.000099
2025-10-23 10:33:47,320 - INFO - Batch 2900, Loss: 4.0283, LR: 0.000099
2025-10-23 10:33:59,679 - INFO - Batch 3000, Loss: 3.5775, LR: 0.000099
2025-10-23 10:34:12,029 - INFO - Batch 3100, Loss: 3.8649, LR: 0.000099
2025-10-23 10:34:24,428 - INFO - Batch 3200, Loss: 3.5816, LR: 0.000099
2025-10-23 10:34:36,876 - INFO - Batch 3300, Loss: 3.5900, LR: 0.000099
2025-10-23 10:34:49,248 - INFO - Batch 3400, Loss: 3.4943, LR: 0.000099
2025-10-23 10:35:01,610 - INFO - Batch 3500, Loss: 3.7197, LR: 0.000099
2025-10-23 10:35:13,988 - INFO - Batch 3600, Loss: 3.5601, LR: 0.000099
2025-10-23 10:35:26,348 - INFO - Batch 3700, Loss: 3.4294, LR: 0.000099
2025-10-23 10:35:38,695 - INFO - Batch 3800, Loss: 3.6658, LR: 0.000099
2025-10-23 10:35:51,091 - INFO - Batch 3900, Loss: 3.6832, LR: 0.000099
2025-10-23 10:36:03,445 - INFO - Batch 4000, Loss: 3.7702, LR: 0.000099
2025-10-23 10:36:15,774 - INFO - Batch 4100, Loss: 3.6424, LR: 0.000098
2025-10-23 10:36:28,108 - INFO - Batch 4200, Loss: 3.5856, LR: 0.000098
2025-10-23 10:36:40,478 - INFO - Batch 4300, Loss: 3.8250, LR: 0.000098
2025-10-23 10:36:52,819 - INFO - Batch 4400, Loss: 3.5892, LR: 0.000098
2025-10-23 10:37:05,126 - INFO - Batch 4500, Loss: 3.7287, LR: 0.000098
2025-10-23 10:37:17,491 - INFO - Batch 4600, Loss: 3.5062, LR: 0.000098
2025-10-23 10:37:29,785 - INFO - Batch 4700, Loss: 3.6549, LR: 0.000098
2025-10-23 10:37:42,043 - INFO - Batch 4800, Loss: 3.2713, LR: 0.000098
2025-10-23 10:37:54,317 - INFO - Batch 4900, Loss: 3.5813, LR: 0.000098
2025-10-23 10:38:06,583 - INFO - Batch 5000, Loss: 3.4452, LR: 0.000098
2025-10-23 10:38:18,825 - INFO - Batch 5100, Loss: 3.7640, LR: 0.000098
2025-10-23 10:38:31,076 - INFO - Batch 5200, Loss: 3.3448, LR: 0.000098
2025-10-23 10:38:43,389 - INFO - Batch 5300, Loss: 3.4130, LR: 0.000098
2025-10-23 10:38:55,684 - INFO - Batch 5400, Loss: 3.7134, LR: 0.000098
2025-10-23 10:39:07,966 - INFO - Batch 5500, Loss: 3.7050, LR: 0.000098
2025-10-23 10:39:20,217 - INFO - Batch 5600, Loss: 3.8875, LR: 0.000098
2025-10-23 10:39:32,470 - INFO - Batch 5700, Loss: 3.3971, LR: 0.000098
2025-10-23 10:39:44,739 - INFO - Batch 5800, Loss: 3.5658, LR: 0.000098
2025-10-23 10:39:57,006 - INFO - Batch 5900, Loss: 3.5693, LR: 0.000098
2025-10-23 10:40:09,269 - INFO - Batch 6000, Loss: 3.2462, LR: 0.000098
2025-10-23 10:40:21,573 - INFO - Batch 6100, Loss: 3.5482, LR: 0.000098
2025-10-23 10:40:33,843 - INFO - Batch 6200, Loss: 3.3978, LR: 0.000098
2025-10-23 10:40:46,098 - INFO - Batch 6300, Loss: 3.4851, LR: 0.000098
2025-10-23 10:40:58,472 - INFO - Batch 6400, Loss: 3.4442, LR: 0.000098
2025-10-23 10:41:04,712 - INFO - Epoch 3/30: Train Loss: 3.6998, Val Loss: 3.3465, LR: 0.000098
2025-10-23 10:41:04,959 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 10:41:05,090 - INFO - Batch 0, Loss: 3.4564, LR: 0.000098
2025-10-23 10:41:17,622 - INFO - Batch 100, Loss: 3.1163, LR: 0.000098
2025-10-23 10:41:30,165 - INFO - Batch 200, Loss: 3.5620, LR: 0.000098
2025-10-23 10:41:42,698 - INFO - Batch 300, Loss: 3.3592, LR: 0.000098
2025-10-23 10:41:55,009 - INFO - Batch 400, Loss: 3.5562, LR: 0.000098
2025-10-23 10:42:07,236 - INFO - Batch 500, Loss: 3.4282, LR: 0.000098
2025-10-23 10:42:19,518 - INFO - Batch 600, Loss: 3.2579, LR: 0.000098
2025-10-23 10:42:31,751 - INFO - Batch 700, Loss: 3.1995, LR: 0.000098
2025-10-23 10:42:43,998 - INFO - Batch 800, Loss: 3.2977, LR: 0.000098
2025-10-23 10:42:56,220 - INFO - Batch 900, Loss: 3.3641, LR: 0.000098
2025-10-23 10:43:08,477 - INFO - Batch 1000, Loss: 3.3719, LR: 0.000098
2025-10-23 10:43:20,712 - INFO - Batch 1100, Loss: 3.5712, LR: 0.000098
2025-10-23 10:43:32,945 - INFO - Batch 1200, Loss: 3.4223, LR: 0.000098
2025-10-23 10:43:45,221 - INFO - Batch 1300, Loss: 3.2202, LR: 0.000098
2025-10-23 10:43:57,519 - INFO - Batch 1400, Loss: 3.3619, LR: 0.000098
2025-10-23 10:44:09,844 - INFO - Batch 1500, Loss: 3.1319, LR: 0.000098
2025-10-23 10:44:22,146 - INFO - Batch 1600, Loss: 3.5089, LR: 0.000098
2025-10-23 10:44:34,443 - INFO - Batch 1700, Loss: 3.3186, LR: 0.000098
2025-10-23 10:44:46,757 - INFO - Batch 1800, Loss: 3.2599, LR: 0.000098
2025-10-23 10:44:59,069 - INFO - Batch 1900, Loss: 3.2709, LR: 0.000098
2025-10-23 10:45:11,394 - INFO - Batch 2000, Loss: 3.2701, LR: 0.000098
2025-10-23 10:45:23,690 - INFO - Batch 2100, Loss: 3.1540, LR: 0.000097
2025-10-23 10:45:35,993 - INFO - Batch 2200, Loss: 3.3844, LR: 0.000097
2025-10-23 10:45:48,326 - INFO - Batch 2300, Loss: 2.8825, LR: 0.000097
2025-10-23 10:46:00,632 - INFO - Batch 2400, Loss: 3.1053, LR: 0.000097
2025-10-23 10:46:12,920 - INFO - Batch 2500, Loss: 3.0293, LR: 0.000097
2025-10-23 10:46:25,229 - INFO - Batch 2600, Loss: 3.0524, LR: 0.000097
2025-10-23 10:46:37,553 - INFO - Batch 2700, Loss: 3.3231, LR: 0.000097
2025-10-23 10:46:49,871 - INFO - Batch 2800, Loss: 3.4080, LR: 0.000097
2025-10-23 10:47:02,217 - INFO - Batch 2900, Loss: 3.1213, LR: 0.000097
2025-10-23 10:47:14,577 - INFO - Batch 3000, Loss: 2.9771, LR: 0.000097
2025-10-23 10:47:26,937 - INFO - Batch 3100, Loss: 3.0327, LR: 0.000097
2025-10-23 10:47:39,247 - INFO - Batch 3200, Loss: 3.2422, LR: 0.000097
2025-10-23 10:47:51,525 - INFO - Batch 3300, Loss: 3.0994, LR: 0.000097
2025-10-23 10:48:03,808 - INFO - Batch 3400, Loss: 3.3020, LR: 0.000097
2025-10-23 10:48:16,089 - INFO - Batch 3500, Loss: 3.2304, LR: 0.000097
2025-10-23 10:48:28,347 - INFO - Batch 3600, Loss: 3.1617, LR: 0.000097
2025-10-23 10:48:40,619 - INFO - Batch 3700, Loss: 3.0432, LR: 0.000097
2025-10-23 10:48:52,889 - INFO - Batch 3800, Loss: 3.0162, LR: 0.000097
2025-10-23 10:49:05,194 - INFO - Batch 3900, Loss: 3.0424, LR: 0.000097
2025-10-23 10:49:17,525 - INFO - Batch 4000, Loss: 3.2148, LR: 0.000097
2025-10-23 10:49:29,809 - INFO - Batch 4100, Loss: 3.3985, LR: 0.000097
2025-10-23 10:49:42,044 - INFO - Batch 4200, Loss: 3.2112, LR: 0.000097
2025-10-23 10:49:54,267 - INFO - Batch 4300, Loss: 3.2459, LR: 0.000097
2025-10-23 10:50:06,578 - INFO - Batch 4400, Loss: 3.0030, LR: 0.000097
2025-10-23 10:50:19,110 - INFO - Batch 4500, Loss: 2.9002, LR: 0.000097
2025-10-23 10:50:31,634 - INFO - Batch 4600, Loss: 3.1224, LR: 0.000097
2025-10-23 10:50:44,112 - INFO - Batch 4700, Loss: 3.1700, LR: 0.000097
2025-10-23 10:50:56,602 - INFO - Batch 4800, Loss: 2.9240, LR: 0.000097
2025-10-23 10:51:09,058 - INFO - Batch 4900, Loss: 3.1210, LR: 0.000097
2025-10-23 10:51:21,486 - INFO - Batch 5000, Loss: 3.0343, LR: 0.000097
2025-10-23 10:51:33,974 - INFO - Batch 5100, Loss: 3.0214, LR: 0.000097
2025-10-23 10:51:46,368 - INFO - Batch 5200, Loss: 2.9483, LR: 0.000097
2025-10-23 10:51:58,668 - INFO - Batch 5300, Loss: 2.9538, LR: 0.000097
2025-10-23 10:52:10,942 - INFO - Batch 5400, Loss: 3.0986, LR: 0.000097
2025-10-23 10:52:23,348 - INFO - Batch 5500, Loss: 3.1493, LR: 0.000097
2025-10-23 10:52:35,637 - INFO - Batch 5600, Loss: 2.9992, LR: 0.000096
2025-10-23 10:52:47,946 - INFO - Batch 5700, Loss: 3.1454, LR: 0.000096
2025-10-23 10:53:00,340 - INFO - Batch 5800, Loss: 3.1071, LR: 0.000096
2025-10-23 10:53:12,687 - INFO - Batch 5900, Loss: 3.0876, LR: 0.000096
2025-10-23 10:53:24,974 - INFO - Batch 6000, Loss: 2.7654, LR: 0.000096
2025-10-23 10:53:37,306 - INFO - Batch 6100, Loss: 2.9141, LR: 0.000096
2025-10-23 10:53:49,734 - INFO - Batch 6200, Loss: 3.1304, LR: 0.000096
2025-10-23 10:54:02,235 - INFO - Batch 6300, Loss: 3.0465, LR: 0.000096
2025-10-23 10:54:14,708 - INFO - Batch 6400, Loss: 2.8117, LR: 0.000096
2025-10-23 10:54:20,933 - INFO - Epoch 4/30: Train Loss: 3.2184, Val Loss: 2.9284, LR: 0.000096
2025-10-23 10:54:21,193 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 10:54:21,330 - INFO - Batch 0, Loss: 2.9107, LR: 0.000096
2025-10-23 10:54:33,738 - INFO - Batch 100, Loss: 2.9491, LR: 0.000096
2025-10-23 10:54:46,226 - INFO - Batch 200, Loss: 2.7992, LR: 0.000096
2025-10-23 10:54:58,647 - INFO - Batch 300, Loss: 2.9468, LR: 0.000096
2025-10-23 10:55:11,095 - INFO - Batch 400, Loss: 3.1984, LR: 0.000096
2025-10-23 10:55:23,500 - INFO - Batch 500, Loss: 2.7719, LR: 0.000096
2025-10-23 10:55:36,096 - INFO - Batch 600, Loss: 2.9372, LR: 0.000096
2025-10-23 10:55:48,624 - INFO - Batch 700, Loss: 2.9046, LR: 0.000096
2025-10-23 10:56:01,152 - INFO - Batch 800, Loss: 3.0447, LR: 0.000096
2025-10-23 10:56:13,688 - INFO - Batch 900, Loss: 2.9749, LR: 0.000096
2025-10-23 10:56:26,153 - INFO - Batch 1000, Loss: 2.8362, LR: 0.000096
2025-10-23 10:56:38,578 - INFO - Batch 1100, Loss: 3.1188, LR: 0.000096
2025-10-23 10:56:51,041 - INFO - Batch 1200, Loss: 2.7662, LR: 0.000096
2025-10-23 10:57:03,518 - INFO - Batch 1300, Loss: 2.7410, LR: 0.000096
2025-10-23 10:57:15,940 - INFO - Batch 1400, Loss: 3.1062, LR: 0.000096
2025-10-23 10:57:28,395 - INFO - Batch 1500, Loss: 2.8898, LR: 0.000096
2025-10-23 10:57:41,030 - INFO - Batch 1600, Loss: 3.1081, LR: 0.000096
2025-10-23 10:57:53,649 - INFO - Batch 1700, Loss: 2.9642, LR: 0.000096
2025-10-23 10:58:06,245 - INFO - Batch 1800, Loss: 2.9319, LR: 0.000096
2025-10-23 10:58:18,803 - INFO - Batch 1900, Loss: 3.0224, LR: 0.000096
2025-10-23 10:58:31,232 - INFO - Batch 2000, Loss: 3.1469, LR: 0.000096
2025-10-23 10:58:43,631 - INFO - Batch 2100, Loss: 2.8290, LR: 0.000096
2025-10-23 10:58:56,079 - INFO - Batch 2200, Loss: 2.8513, LR: 0.000096
2025-10-23 10:59:08,604 - INFO - Batch 2300, Loss: 3.2547, LR: 0.000095
2025-10-23 10:59:21,095 - INFO - Batch 2400, Loss: 2.9155, LR: 0.000095
2025-10-23 10:59:33,478 - INFO - Batch 2500, Loss: 3.0926, LR: 0.000095
2025-10-23 10:59:45,865 - INFO - Batch 2600, Loss: 2.8886, LR: 0.000095
2025-10-23 10:59:58,245 - INFO - Batch 2700, Loss: 3.1218, LR: 0.000095
2025-10-23 11:00:10,639 - INFO - Batch 2800, Loss: 2.8487, LR: 0.000095
2025-10-23 11:00:23,047 - INFO - Batch 2900, Loss: 2.8488, LR: 0.000095
2025-10-23 11:00:35,493 - INFO - Batch 3000, Loss: 2.8711, LR: 0.000095
2025-10-23 11:00:47,946 - INFO - Batch 3100, Loss: 3.0371, LR: 0.000095
2025-10-23 11:01:00,290 - INFO - Batch 3200, Loss: 2.9935, LR: 0.000095
2025-10-23 11:01:12,855 - INFO - Batch 3300, Loss: 2.9681, LR: 0.000095
2025-10-23 11:01:25,273 - INFO - Batch 3400, Loss: 2.9253, LR: 0.000095
2025-10-23 11:01:37,590 - INFO - Batch 3500, Loss: 3.2309, LR: 0.000095
2025-10-23 11:01:50,074 - INFO - Batch 3600, Loss: 2.8935, LR: 0.000095
2025-10-23 11:02:02,465 - INFO - Batch 3700, Loss: 2.9367, LR: 0.000095
2025-10-23 11:02:14,806 - INFO - Batch 3800, Loss: 3.0269, LR: 0.000095
2025-10-23 11:02:27,229 - INFO - Batch 3900, Loss: 2.6379, LR: 0.000095
2025-10-23 11:02:39,654 - INFO - Batch 4000, Loss: 3.0070, LR: 0.000095
2025-10-23 11:02:52,090 - INFO - Batch 4100, Loss: 3.0109, LR: 0.000095
2025-10-23 11:03:04,549 - INFO - Batch 4200, Loss: 2.8262, LR: 0.000095
2025-10-23 11:03:17,060 - INFO - Batch 4300, Loss: 2.9401, LR: 0.000095
2025-10-23 11:03:29,483 - INFO - Batch 4400, Loss: 2.9466, LR: 0.000095
2025-10-23 11:03:41,882 - INFO - Batch 4500, Loss: 2.7906, LR: 0.000095
2025-10-23 11:03:54,304 - INFO - Batch 4600, Loss: 2.8811, LR: 0.000095
2025-10-23 11:04:06,648 - INFO - Batch 4700, Loss: 2.8802, LR: 0.000095
2025-10-23 11:04:19,038 - INFO - Batch 4800, Loss: 2.9370, LR: 0.000095
2025-10-23 11:04:31,492 - INFO - Batch 4900, Loss: 2.9922, LR: 0.000095
2025-10-23 11:04:43,905 - INFO - Batch 5000, Loss: 2.9493, LR: 0.000095
2025-10-23 11:04:56,296 - INFO - Batch 5100, Loss: 2.8010, LR: 0.000094
2025-10-23 11:05:08,734 - INFO - Batch 5200, Loss: 2.8978, LR: 0.000094
2025-10-23 11:05:21,273 - INFO - Batch 5300, Loss: 2.7003, LR: 0.000094
2025-10-23 11:05:33,673 - INFO - Batch 5400, Loss: 2.9448, LR: 0.000094
2025-10-23 11:05:46,093 - INFO - Batch 5500, Loss: 2.8800, LR: 0.000094
2025-10-23 11:05:58,478 - INFO - Batch 5600, Loss: 2.7962, LR: 0.000094
2025-10-23 11:06:10,863 - INFO - Batch 5700, Loss: 2.8421, LR: 0.000094
2025-10-23 11:06:23,265 - INFO - Batch 5800, Loss: 2.8981, LR: 0.000094
2025-10-23 11:06:35,630 - INFO - Batch 5900, Loss: 2.9418, LR: 0.000094
2025-10-23 11:06:48,031 - INFO - Batch 6000, Loss: 2.7663, LR: 0.000094
2025-10-23 11:07:00,440 - INFO - Batch 6100, Loss: 2.9684, LR: 0.000094
2025-10-23 11:07:12,901 - INFO - Batch 6200, Loss: 3.0252, LR: 0.000094
2025-10-23 11:07:25,445 - INFO - Batch 6300, Loss: 2.7450, LR: 0.000094
2025-10-23 11:07:37,924 - INFO - Batch 6400, Loss: 2.7922, LR: 0.000094
2025-10-23 11:07:44,186 - INFO - Epoch 5/30: Train Loss: 2.9357, Val Loss: 2.7098, LR: 0.000094
2025-10-23 11:07:44,433 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 11:07:44,564 - INFO - Batch 0, Loss: 2.7367, LR: 0.000094
2025-10-23 11:07:56,946 - INFO - Batch 100, Loss: 2.8973, LR: 0.000094
2025-10-23 11:08:09,364 - INFO - Batch 200, Loss: 2.9912, LR: 0.000094
2025-10-23 11:08:21,946 - INFO - Batch 300, Loss: 2.9000, LR: 0.000094
2025-10-23 11:08:34,409 - INFO - Batch 400, Loss: 2.7816, LR: 0.000094
2025-10-23 11:08:46,982 - INFO - Batch 500, Loss: 2.8086, LR: 0.000094
2025-10-23 11:08:59,515 - INFO - Batch 600, Loss: 2.5029, LR: 0.000094
2025-10-23 11:09:12,057 - INFO - Batch 700, Loss: 2.6511, LR: 0.000094
2025-10-23 11:09:24,599 - INFO - Batch 800, Loss: 2.8137, LR: 0.000094
2025-10-23 11:09:37,080 - INFO - Batch 900, Loss: 2.8178, LR: 0.000094
2025-10-23 11:09:49,504 - INFO - Batch 1000, Loss: 2.6764, LR: 0.000094
2025-10-23 11:10:02,006 - INFO - Batch 1100, Loss: 2.8308, LR: 0.000094
2025-10-23 11:10:14,481 - INFO - Batch 1200, Loss: 2.8208, LR: 0.000093
2025-10-23 11:10:26,873 - INFO - Batch 1300, Loss: 2.9775, LR: 0.000093
2025-10-23 11:10:39,332 - INFO - Batch 1400, Loss: 2.7735, LR: 0.000093
2025-10-23 11:10:51,746 - INFO - Batch 1500, Loss: 2.8389, LR: 0.000093
2025-10-23 11:11:04,173 - INFO - Batch 1600, Loss: 2.8227, LR: 0.000093
2025-10-23 11:11:16,588 - INFO - Batch 1700, Loss: 2.5856, LR: 0.000093
2025-10-23 11:11:29,083 - INFO - Batch 1800, Loss: 2.8630, LR: 0.000093
2025-10-23 11:11:41,489 - INFO - Batch 1900, Loss: 2.6641, LR: 0.000093
2025-10-23 11:11:54,034 - INFO - Batch 2000, Loss: 3.0101, LR: 0.000093
2025-10-23 11:12:06,430 - INFO - Batch 2100, Loss: 2.7205, LR: 0.000093
2025-10-23 11:12:18,863 - INFO - Batch 2200, Loss: 2.5279, LR: 0.000093
2025-10-23 11:12:31,313 - INFO - Batch 2300, Loss: 2.8221, LR: 0.000093
2025-10-23 11:12:43,724 - INFO - Batch 2400, Loss: 2.8605, LR: 0.000093
2025-10-23 11:12:56,192 - INFO - Batch 2500, Loss: 2.8334, LR: 0.000093
2025-10-23 11:13:08,626 - INFO - Batch 2600, Loss: 2.7822, LR: 0.000093
2025-10-23 11:13:21,128 - INFO - Batch 2700, Loss: 2.7176, LR: 0.000093
2025-10-23 11:13:33,560 - INFO - Batch 2800, Loss: 2.7444, LR: 0.000093
2025-10-23 11:13:46,165 - INFO - Batch 2900, Loss: 2.6348, LR: 0.000093
2025-10-23 11:13:58,586 - INFO - Batch 3000, Loss: 2.9719, LR: 0.000093
2025-10-23 11:14:11,005 - INFO - Batch 3100, Loss: 2.9062, LR: 0.000093
2025-10-23 11:14:23,425 - INFO - Batch 3200, Loss: 3.0659, LR: 0.000093
2025-10-23 11:14:35,793 - INFO - Batch 3300, Loss: 2.4778, LR: 0.000093
2025-10-23 11:14:48,221 - INFO - Batch 3400, Loss: 2.4558, LR: 0.000093
2025-10-23 11:15:00,621 - INFO - Batch 3500, Loss: 2.5934, LR: 0.000093
2025-10-23 11:15:13,027 - INFO - Batch 3600, Loss: 2.8882, LR: 0.000092
2025-10-23 11:15:25,421 - INFO - Batch 3700, Loss: 2.5935, LR: 0.000092
2025-10-23 11:15:37,718 - INFO - Batch 3800, Loss: 2.7591, LR: 0.000092
2025-10-23 11:15:50,062 - INFO - Batch 3900, Loss: 3.1865, LR: 0.000092
2025-10-23 11:16:02,473 - INFO - Batch 4000, Loss: 2.9126, LR: 0.000092
2025-10-23 11:16:14,850 - INFO - Batch 4100, Loss: 2.8997, LR: 0.000092
2025-10-23 11:16:27,235 - INFO - Batch 4200, Loss: 2.6739, LR: 0.000092
2025-10-23 11:16:39,644 - INFO - Batch 4300, Loss: 2.8683, LR: 0.000092
2025-10-23 11:16:51,668 - INFO - Batch 4400, Loss: 2.6589, LR: 0.000092
2025-10-23 11:17:04,038 - INFO - Batch 4500, Loss: 2.7851, LR: 0.000092
2025-10-23 11:17:16,620 - INFO - Batch 4600, Loss: 2.9823, LR: 0.000092
2025-10-23 11:17:29,036 - INFO - Batch 4700, Loss: 2.7466, LR: 0.000092
2025-10-23 11:17:41,429 - INFO - Batch 4800, Loss: 3.0322, LR: 0.000092
2025-10-23 11:17:53,842 - INFO - Batch 4900, Loss: 2.7594, LR: 0.000092
2025-10-23 11:18:06,245 - INFO - Batch 5000, Loss: 2.8380, LR: 0.000092
2025-10-23 11:18:18,645 - INFO - Batch 5100, Loss: 2.7369, LR: 0.000092
2025-10-23 11:18:31,079 - INFO - Batch 5200, Loss: 2.5716, LR: 0.000092
2025-10-23 11:18:43,477 - INFO - Batch 5300, Loss: 2.6410, LR: 0.000092
2025-10-23 11:18:55,896 - INFO - Batch 5400, Loss: 2.9815, LR: 0.000092
2025-10-23 11:19:08,554 - INFO - Batch 5500, Loss: 2.8608, LR: 0.000092
2025-10-23 11:19:21,022 - INFO - Batch 5600, Loss: 2.5603, LR: 0.000092
2025-10-23 11:19:33,547 - INFO - Batch 5700, Loss: 2.7459, LR: 0.000092
2025-10-23 11:19:46,026 - INFO - Batch 5800, Loss: 2.7801, LR: 0.000092
2025-10-23 11:19:58,609 - INFO - Batch 5900, Loss: 2.7209, LR: 0.000091
2025-10-23 11:20:11,095 - INFO - Batch 6000, Loss: 2.8404, LR: 0.000091
2025-10-23 11:20:23,532 - INFO - Batch 6100, Loss: 2.8052, LR: 0.000091
2025-10-23 11:20:35,896 - INFO - Batch 6200, Loss: 2.6709, LR: 0.000091
2025-10-23 11:20:48,277 - INFO - Batch 6300, Loss: 2.5290, LR: 0.000091
2025-10-23 11:21:00,651 - INFO - Batch 6400, Loss: 2.5649, LR: 0.000091
2025-10-23 11:21:06,871 - INFO - Epoch 6/30: Train Loss: 2.7723, Val Loss: 2.5910, LR: 0.000091
2025-10-23 11:21:07,125 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 11:21:07,258 - INFO - Batch 0, Loss: 2.3821, LR: 0.000091
2025-10-23 11:21:19,651 - INFO - Batch 100, Loss: 2.8268, LR: 0.000091
2025-10-23 11:21:32,102 - INFO - Batch 200, Loss: 2.4490, LR: 0.000091
2025-10-23 11:21:44,601 - INFO - Batch 300, Loss: 2.0863, LR: 0.000091
2025-10-23 11:21:57,102 - INFO - Batch 400, Loss: 2.8257, LR: 0.000091
2025-10-23 11:22:09,688 - INFO - Batch 500, Loss: 2.8156, LR: 0.000091
2025-10-23 11:22:22,267 - INFO - Batch 600, Loss: 2.5703, LR: 0.000091
2025-10-23 11:22:34,804 - INFO - Batch 700, Loss: 2.5632, LR: 0.000091
2025-10-23 11:22:47,457 - INFO - Batch 800, Loss: 2.5585, LR: 0.000091
2025-10-23 11:23:00,037 - INFO - Batch 900, Loss: 2.5887, LR: 0.000091
2025-10-23 11:23:12,627 - INFO - Batch 1000, Loss: 2.3740, LR: 0.000091
2025-10-23 11:23:25,196 - INFO - Batch 1100, Loss: 2.9528, LR: 0.000091
2025-10-23 11:23:37,761 - INFO - Batch 1200, Loss: 2.4632, LR: 0.000091
2025-10-23 11:23:50,321 - INFO - Batch 1300, Loss: 2.6421, LR: 0.000091
2025-10-23 11:24:02,848 - INFO - Batch 1400, Loss: 2.5186, LR: 0.000091
2025-10-23 11:24:15,337 - INFO - Batch 1500, Loss: 2.5006, LR: 0.000090
2025-10-23 11:24:27,730 - INFO - Batch 1600, Loss: 2.4814, LR: 0.000090
2025-10-23 11:24:40,088 - INFO - Batch 1700, Loss: 2.7422, LR: 0.000090
2025-10-23 11:24:52,455 - INFO - Batch 1800, Loss: 2.7501, LR: 0.000090
2025-10-23 11:25:04,791 - INFO - Batch 1900, Loss: 2.6204, LR: 0.000090
2025-10-23 11:25:17,268 - INFO - Batch 2000, Loss: 2.8767, LR: 0.000090
2025-10-23 11:25:29,769 - INFO - Batch 2100, Loss: 2.6261, LR: 0.000090
2025-10-23 11:25:42,263 - INFO - Batch 2200, Loss: 2.6677, LR: 0.000090
2025-10-23 11:25:54,750 - INFO - Batch 2300, Loss: 2.9420, LR: 0.000090
2025-10-23 11:26:07,146 - INFO - Batch 2400, Loss: 2.6234, LR: 0.000090
2025-10-23 11:26:19,577 - INFO - Batch 2500, Loss: 2.9470, LR: 0.000090
2025-10-23 11:26:31,882 - INFO - Batch 2600, Loss: 2.5023, LR: 0.000090
2025-10-23 11:26:44,198 - INFO - Batch 2700, Loss: 2.8146, LR: 0.000090
2025-10-23 11:26:56,500 - INFO - Batch 2800, Loss: 2.7561, LR: 0.000090
2025-10-23 11:27:08,800 - INFO - Batch 2900, Loss: 2.5442, LR: 0.000090
2025-10-23 11:27:21,127 - INFO - Batch 3000, Loss: 2.9984, LR: 0.000090
2025-10-23 11:27:33,419 - INFO - Batch 3100, Loss: 2.8164, LR: 0.000090
2025-10-23 11:27:45,701 - INFO - Batch 3200, Loss: 2.6692, LR: 0.000090
2025-10-23 11:27:57,971 - INFO - Batch 3300, Loss: 2.7901, LR: 0.000090
2025-10-23 11:28:10,271 - INFO - Batch 3400, Loss: 2.5850, LR: 0.000090
2025-10-23 11:28:22,572 - INFO - Batch 3500, Loss: 2.5001, LR: 0.000090
2025-10-23 11:28:34,876 - INFO - Batch 3600, Loss: 2.6226, LR: 0.000089
2025-10-23 11:28:47,180 - INFO - Batch 3700, Loss: 2.7996, LR: 0.000089
2025-10-23 11:28:59,459 - INFO - Batch 3800, Loss: 2.7426, LR: 0.000089
2025-10-23 11:29:11,748 - INFO - Batch 3900, Loss: 2.9179, LR: 0.000089
2025-10-23 11:29:24,034 - INFO - Batch 4000, Loss: 2.4291, LR: 0.000089
2025-10-23 11:29:36,317 - INFO - Batch 4100, Loss: 2.4899, LR: 0.000089
2025-10-23 11:29:48,605 - INFO - Batch 4200, Loss: 2.5271, LR: 0.000089
2025-10-23 11:30:00,877 - INFO - Batch 4300, Loss: 2.9035, LR: 0.000089
2025-10-23 11:30:13,145 - INFO - Batch 4400, Loss: 2.6488, LR: 0.000089
2025-10-23 11:30:25,390 - INFO - Batch 4500, Loss: 2.4894, LR: 0.000089
2025-10-23 11:30:37,699 - INFO - Batch 4600, Loss: 2.6337, LR: 0.000089
2025-10-23 11:30:49,962 - INFO - Batch 4700, Loss: 2.3634, LR: 0.000089
2025-10-23 11:31:02,259 - INFO - Batch 4800, Loss: 2.6356, LR: 0.000089
2025-10-23 11:31:14,565 - INFO - Batch 4900, Loss: 2.7399, LR: 0.000089
2025-10-23 11:31:26,863 - INFO - Batch 5000, Loss: 2.4097, LR: 0.000089
2025-10-23 11:31:39,176 - INFO - Batch 5100, Loss: 2.8236, LR: 0.000089
2025-10-23 11:31:51,488 - INFO - Batch 5200, Loss: 2.5812, LR: 0.000089
2025-10-23 11:32:03,826 - INFO - Batch 5300, Loss: 2.9109, LR: 0.000089
2025-10-23 11:32:16,146 - INFO - Batch 5400, Loss: 2.4235, LR: 0.000089
2025-10-23 11:32:28,581 - INFO - Batch 5500, Loss: 2.2444, LR: 0.000088
2025-10-23 11:32:40,927 - INFO - Batch 5600, Loss: 2.7577, LR: 0.000088
2025-10-23 11:32:53,285 - INFO - Batch 5700, Loss: 2.9283, LR: 0.000088
2025-10-23 11:33:05,625 - INFO - Batch 5800, Loss: 2.5745, LR: 0.000088
2025-10-23 11:33:18,005 - INFO - Batch 5900, Loss: 2.4432, LR: 0.000088
2025-10-23 11:33:30,336 - INFO - Batch 6000, Loss: 2.6038, LR: 0.000088
2025-10-23 11:33:42,688 - INFO - Batch 6100, Loss: 2.7302, LR: 0.000088
2025-10-23 11:33:55,059 - INFO - Batch 6200, Loss: 2.5793, LR: 0.000088
2025-10-23 11:34:07,439 - INFO - Batch 6300, Loss: 2.6616, LR: 0.000088
2025-10-23 11:34:19,831 - INFO - Batch 6400, Loss: 2.8146, LR: 0.000088
2025-10-23 11:34:26,012 - INFO - Epoch 7/30: Train Loss: 2.6684, Val Loss: 2.5078, LR: 0.000088
2025-10-23 11:34:26,268 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 11:34:26,400 - INFO - Batch 0, Loss: 2.7339, LR: 0.000088
2025-10-23 11:34:38,792 - INFO - Batch 100, Loss: 2.4291, LR: 0.000088
2025-10-23 11:34:51,175 - INFO - Batch 200, Loss: 2.3498, LR: 0.000088
2025-10-23 11:35:03,589 - INFO - Batch 300, Loss: 2.7590, LR: 0.000088
2025-10-23 11:35:15,983 - INFO - Batch 400, Loss: 2.6674, LR: 0.000088
2025-10-23 11:35:28,364 - INFO - Batch 500, Loss: 2.6298, LR: 0.000088
2025-10-23 11:35:40,756 - INFO - Batch 600, Loss: 2.4952, LR: 0.000088
2025-10-23 11:35:53,122 - INFO - Batch 700, Loss: 2.5071, LR: 0.000088
2025-10-23 11:36:05,501 - INFO - Batch 800, Loss: 2.7378, LR: 0.000088
2025-10-23 11:36:17,850 - INFO - Batch 900, Loss: 2.7417, LR: 0.000088
2025-10-23 11:36:30,193 - INFO - Batch 1000, Loss: 2.6397, LR: 0.000087
2025-10-23 11:36:42,565 - INFO - Batch 1100, Loss: 2.7879, LR: 0.000087
2025-10-23 11:36:54,941 - INFO - Batch 1200, Loss: 2.5891, LR: 0.000087
2025-10-23 11:37:07,286 - INFO - Batch 1300, Loss: 2.8189, LR: 0.000087
2025-10-23 11:37:19,677 - INFO - Batch 1400, Loss: 2.6036, LR: 0.000087
2025-10-23 11:37:32,300 - INFO - Batch 1500, Loss: 2.6973, LR: 0.000087
2025-10-23 11:37:44,768 - INFO - Batch 1600, Loss: 2.5907, LR: 0.000087
2025-10-23 11:37:57,085 - INFO - Batch 1700, Loss: 2.4674, LR: 0.000087
2025-10-23 11:38:09,377 - INFO - Batch 1800, Loss: 2.6957, LR: 0.000087
2025-10-23 11:38:21,655 - INFO - Batch 1900, Loss: 2.4010, LR: 0.000087
2025-10-23 11:38:33,931 - INFO - Batch 2000, Loss: 2.4985, LR: 0.000087
2025-10-23 11:38:46,193 - INFO - Batch 2100, Loss: 2.3295, LR: 0.000087
2025-10-23 11:38:58,455 - INFO - Batch 2200, Loss: 2.4072, LR: 0.000087
2025-10-23 11:39:10,705 - INFO - Batch 2300, Loss: 2.5964, LR: 0.000087
2025-10-23 11:39:22,961 - INFO - Batch 2400, Loss: 2.3234, LR: 0.000087
2025-10-23 11:39:35,247 - INFO - Batch 2500, Loss: 2.5247, LR: 0.000087
2025-10-23 11:39:47,576 - INFO - Batch 2600, Loss: 2.5989, LR: 0.000087
2025-10-23 11:39:59,900 - INFO - Batch 2700, Loss: 2.5780, LR: 0.000087
2025-10-23 11:40:12,210 - INFO - Batch 2800, Loss: 2.3460, LR: 0.000086
2025-10-23 11:40:24,493 - INFO - Batch 2900, Loss: 2.5906, LR: 0.000086
2025-10-23 11:40:36,774 - INFO - Batch 3000, Loss: 2.2447, LR: 0.000086
2025-10-23 11:40:49,079 - INFO - Batch 3100, Loss: 2.7544, LR: 0.000086
2025-10-23 11:41:01,398 - INFO - Batch 3200, Loss: 2.5711, LR: 0.000086
2025-10-23 11:41:13,706 - INFO - Batch 3300, Loss: 2.6583, LR: 0.000086
2025-10-23 11:41:25,965 - INFO - Batch 3400, Loss: 2.7720, LR: 0.000086
2025-10-23 11:41:38,289 - INFO - Batch 3500, Loss: 2.4652, LR: 0.000086
2025-10-23 11:41:50,624 - INFO - Batch 3600, Loss: 2.7080, LR: 0.000086
2025-10-23 11:42:02,900 - INFO - Batch 3700, Loss: 2.7286, LR: 0.000086
2025-10-23 11:42:15,226 - INFO - Batch 3800, Loss: 2.3832, LR: 0.000086
2025-10-23 11:42:27,622 - INFO - Batch 3900, Loss: 2.7506, LR: 0.000086
2025-10-23 11:42:40,113 - INFO - Batch 4000, Loss: 2.6130, LR: 0.000086
2025-10-23 11:42:52,628 - INFO - Batch 4100, Loss: 2.3550, LR: 0.000086
2025-10-23 11:43:05,041 - INFO - Batch 4200, Loss: 2.7836, LR: 0.000086
2025-10-23 11:43:17,493 - INFO - Batch 4300, Loss: 2.5445, LR: 0.000086
2025-10-23 11:43:29,942 - INFO - Batch 4400, Loss: 2.4679, LR: 0.000086
2025-10-23 11:43:42,296 - INFO - Batch 4500, Loss: 2.6262, LR: 0.000085
2025-10-23 11:43:54,648 - INFO - Batch 4600, Loss: 2.4055, LR: 0.000085
2025-10-23 11:44:07,003 - INFO - Batch 4700, Loss: 2.6919, LR: 0.000085
2025-10-23 11:44:19,350 - INFO - Batch 4800, Loss: 2.8680, LR: 0.000085
2025-10-23 11:44:31,679 - INFO - Batch 4900, Loss: 2.4690, LR: 0.000085
2025-10-23 11:44:44,010 - INFO - Batch 5000, Loss: 2.7610, LR: 0.000085
2025-10-23 11:44:56,341 - INFO - Batch 5100, Loss: 2.6177, LR: 0.000085
2025-10-23 11:45:08,731 - INFO - Batch 5200, Loss: 2.6136, LR: 0.000085
2025-10-23 11:45:21,128 - INFO - Batch 5300, Loss: 2.7199, LR: 0.000085
2025-10-23 11:45:33,503 - INFO - Batch 5400, Loss: 2.3497, LR: 0.000085
2025-10-23 11:45:45,869 - INFO - Batch 5500, Loss: 2.4871, LR: 0.000085
2025-10-23 11:45:58,217 - INFO - Batch 5600, Loss: 2.4667, LR: 0.000085
2025-10-23 11:46:10,602 - INFO - Batch 5700, Loss: 2.6404, LR: 0.000085
2025-10-23 11:46:22,960 - INFO - Batch 5800, Loss: 2.8518, LR: 0.000085
2025-10-23 11:46:35,331 - INFO - Batch 5900, Loss: 2.7060, LR: 0.000085
2025-10-23 11:46:47,700 - INFO - Batch 6000, Loss: 2.1230, LR: 0.000085
2025-10-23 11:47:00,051 - INFO - Batch 6100, Loss: 2.7466, LR: 0.000085
2025-10-23 11:47:12,399 - INFO - Batch 6200, Loss: 2.3363, LR: 0.000084
2025-10-23 11:47:24,818 - INFO - Batch 6300, Loss: 2.5834, LR: 0.000084
2025-10-23 11:47:37,178 - INFO - Batch 6400, Loss: 2.6014, LR: 0.000084
2025-10-23 11:47:43,355 - INFO - Epoch 8/30: Train Loss: 2.5953, Val Loss: 2.4452, LR: 0.000084
2025-10-23 11:47:43,627 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 11:47:43,757 - INFO - Batch 0, Loss: 2.4626, LR: 0.000084
2025-10-23 11:47:56,182 - INFO - Batch 100, Loss: 2.6216, LR: 0.000084
2025-10-23 11:48:08,571 - INFO - Batch 200, Loss: 2.8970, LR: 0.000084
2025-10-23 11:48:20,970 - INFO - Batch 300, Loss: 2.5910, LR: 0.000084
2025-10-23 11:48:33,355 - INFO - Batch 400, Loss: 2.3874, LR: 0.000084
2025-10-23 11:48:45,728 - INFO - Batch 500, Loss: 2.4273, LR: 0.000084
2025-10-23 11:48:58,130 - INFO - Batch 600, Loss: 2.5851, LR: 0.000084
2025-10-23 11:49:10,553 - INFO - Batch 700, Loss: 2.5262, LR: 0.000084
2025-10-23 11:49:22,936 - INFO - Batch 800, Loss: 2.4731, LR: 0.000084
2025-10-23 11:49:35,331 - INFO - Batch 900, Loss: 2.4816, LR: 0.000084
2025-10-23 11:49:47,688 - INFO - Batch 1000, Loss: 2.3046, LR: 0.000084
2025-10-23 11:50:00,040 - INFO - Batch 1100, Loss: 2.5662, LR: 0.000084
2025-10-23 11:50:12,460 - INFO - Batch 1200, Loss: 2.7514, LR: 0.000084
2025-10-23 11:50:24,817 - INFO - Batch 1300, Loss: 2.6524, LR: 0.000084
2025-10-23 11:50:37,211 - INFO - Batch 1400, Loss: 2.2822, LR: 0.000083
2025-10-23 11:50:49,577 - INFO - Batch 1500, Loss: 2.6469, LR: 0.000083
2025-10-23 11:51:01,964 - INFO - Batch 1600, Loss: 2.6582, LR: 0.000083
2025-10-23 11:51:14,333 - INFO - Batch 1700, Loss: 2.5234, LR: 0.000083
2025-10-23 11:51:26,679 - INFO - Batch 1800, Loss: 2.4211, LR: 0.000083
2025-10-23 11:51:39,069 - INFO - Batch 1900, Loss: 2.6806, LR: 0.000083
2025-10-23 11:51:51,441 - INFO - Batch 2000, Loss: 2.7449, LR: 0.000083
2025-10-23 11:52:03,814 - INFO - Batch 2100, Loss: 2.4400, LR: 0.000083
2025-10-23 11:52:16,203 - INFO - Batch 2200, Loss: 2.5903, LR: 0.000083
2025-10-23 11:52:28,669 - INFO - Batch 2300, Loss: 2.4292, LR: 0.000083
2025-10-23 11:52:41,191 - INFO - Batch 2400, Loss: 2.5978, LR: 0.000083
2025-10-23 11:52:53,698 - INFO - Batch 2500, Loss: 2.6264, LR: 0.000083
2025-10-23 11:53:06,073 - INFO - Batch 2600, Loss: 2.4774, LR: 0.000083
2025-10-23 11:53:18,410 - INFO - Batch 2700, Loss: 2.6224, LR: 0.000083
2025-10-23 11:53:30,697 - INFO - Batch 2800, Loss: 2.3864, LR: 0.000083
2025-10-23 11:53:42,964 - INFO - Batch 2900, Loss: 2.4966, LR: 0.000083
2025-10-23 11:53:55,469 - INFO - Batch 3000, Loss: 2.5900, LR: 0.000083
2025-10-23 11:54:07,797 - INFO - Batch 3100, Loss: 2.3554, LR: 0.000082
2025-10-23 11:54:20,075 - INFO - Batch 3200, Loss: 2.7093, LR: 0.000082
2025-10-23 11:54:32,344 - INFO - Batch 3300, Loss: 2.5980, LR: 0.000082
2025-10-23 11:54:44,626 - INFO - Batch 3400, Loss: 2.4645, LR: 0.000082
2025-10-23 11:54:56,933 - INFO - Batch 3500, Loss: 2.6762, LR: 0.000082
2025-10-23 11:55:09,233 - INFO - Batch 3600, Loss: 2.5251, LR: 0.000082
2025-10-23 11:55:21,530 - INFO - Batch 3700, Loss: 2.6583, LR: 0.000082
2025-10-23 11:55:33,811 - INFO - Batch 3800, Loss: 2.6794, LR: 0.000082
2025-10-23 11:55:46,100 - INFO - Batch 3900, Loss: 2.4038, LR: 0.000082
2025-10-23 11:55:58,386 - INFO - Batch 4000, Loss: 2.7015, LR: 0.000082
2025-10-23 11:56:10,690 - INFO - Batch 4100, Loss: 2.5947, LR: 0.000082
2025-10-23 11:56:23,040 - INFO - Batch 4200, Loss: 2.4167, LR: 0.000082
2025-10-23 11:56:35,413 - INFO - Batch 4300, Loss: 2.3381, LR: 0.000082
2025-10-23 11:56:47,781 - INFO - Batch 4400, Loss: 2.6231, LR: 0.000082
2025-10-23 11:57:00,105 - INFO - Batch 4500, Loss: 2.5487, LR: 0.000082
2025-10-23 11:57:12,465 - INFO - Batch 4600, Loss: 2.7589, LR: 0.000081
2025-10-23 11:57:24,873 - INFO - Batch 4700, Loss: 2.8762, LR: 0.000081
2025-10-23 11:57:37,219 - INFO - Batch 4800, Loss: 2.5732, LR: 0.000081
2025-10-23 11:57:49,606 - INFO - Batch 4900, Loss: 2.3131, LR: 0.000081
2025-10-23 11:58:01,964 - INFO - Batch 5000, Loss: 2.7853, LR: 0.000081
2025-10-23 11:58:14,289 - INFO - Batch 5100, Loss: 2.3042, LR: 0.000081
2025-10-23 11:58:26,614 - INFO - Batch 5200, Loss: 2.2098, LR: 0.000081
2025-10-23 11:58:38,934 - INFO - Batch 5300, Loss: 2.4196, LR: 0.000081
2025-10-23 11:58:51,273 - INFO - Batch 5400, Loss: 2.5729, LR: 0.000081
2025-10-23 11:59:03,634 - INFO - Batch 5500, Loss: 2.7357, LR: 0.000081
2025-10-23 11:59:16,049 - INFO - Batch 5600, Loss: 2.6567, LR: 0.000081
2025-10-23 11:59:28,432 - INFO - Batch 5700, Loss: 2.6651, LR: 0.000081
2025-10-23 11:59:40,805 - INFO - Batch 5800, Loss: 2.6851, LR: 0.000081
2025-10-23 11:59:53,151 - INFO - Batch 5900, Loss: 2.6852, LR: 0.000081
2025-10-23 12:00:05,522 - INFO - Batch 6000, Loss: 2.3841, LR: 0.000081
2025-10-23 12:00:17,876 - INFO - Batch 6100, Loss: 2.2976, LR: 0.000081
2025-10-23 12:00:30,237 - INFO - Batch 6200, Loss: 2.6432, LR: 0.000080
2025-10-23 12:00:42,584 - INFO - Batch 6300, Loss: 2.6057, LR: 0.000080
2025-10-23 12:00:54,924 - INFO - Batch 6400, Loss: 2.5269, LR: 0.000080
2025-10-23 12:01:01,080 - INFO - Epoch 9/30: Train Loss: 2.5412, Val Loss: 2.3972, LR: 0.000080
2025-10-23 12:01:01,375 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 12:01:01,503 - INFO - Batch 0, Loss: 2.5745, LR: 0.000080
2025-10-23 12:01:13,804 - INFO - Batch 100, Loss: 2.4633, LR: 0.000080
2025-10-23 12:01:26,101 - INFO - Batch 200, Loss: 2.5067, LR: 0.000080
2025-10-23 12:01:38,406 - INFO - Batch 300, Loss: 2.6529, LR: 0.000080
2025-10-23 12:01:50,717 - INFO - Batch 400, Loss: 2.6311, LR: 0.000080
2025-10-23 12:02:03,035 - INFO - Batch 500, Loss: 2.5763, LR: 0.000080
2025-10-23 12:02:15,346 - INFO - Batch 600, Loss: 2.6787, LR: 0.000080
2025-10-23 12:02:27,765 - INFO - Batch 700, Loss: 2.2919, LR: 0.000080
2025-10-23 12:02:40,058 - INFO - Batch 800, Loss: 2.4028, LR: 0.000080
2025-10-23 12:02:52,361 - INFO - Batch 900, Loss: 2.6765, LR: 0.000080
2025-10-23 12:03:04,686 - INFO - Batch 1000, Loss: 2.3917, LR: 0.000080
2025-10-23 12:03:16,985 - INFO - Batch 1100, Loss: 2.3385, LR: 0.000080
2025-10-23 12:03:29,281 - INFO - Batch 1200, Loss: 2.5546, LR: 0.000080
2025-10-23 12:03:41,573 - INFO - Batch 1300, Loss: 2.5759, LR: 0.000079
2025-10-23 12:03:53,875 - INFO - Batch 1400, Loss: 2.5993, LR: 0.000079
2025-10-23 12:04:06,125 - INFO - Batch 1500, Loss: 2.4558, LR: 0.000079
2025-10-23 12:04:18,367 - INFO - Batch 1600, Loss: 2.3443, LR: 0.000079
2025-10-23 12:04:30,609 - INFO - Batch 1700, Loss: 2.5333, LR: 0.000079
2025-10-23 12:04:42,874 - INFO - Batch 1800, Loss: 2.4643, LR: 0.000079
2025-10-23 12:04:55,124 - INFO - Batch 1900, Loss: 2.6669, LR: 0.000079
2025-10-23 12:05:07,375 - INFO - Batch 2000, Loss: 2.6059, LR: 0.000079
2025-10-23 12:05:19,644 - INFO - Batch 2100, Loss: 2.4580, LR: 0.000079
2025-10-23 12:05:31,942 - INFO - Batch 2200, Loss: 2.3222, LR: 0.000079
2025-10-23 12:05:44,331 - INFO - Batch 2300, Loss: 2.3702, LR: 0.000079
2025-10-23 12:05:56,865 - INFO - Batch 2400, Loss: 2.3523, LR: 0.000079
2025-10-23 12:06:09,212 - INFO - Batch 2500, Loss: 2.5713, LR: 0.000079
2025-10-23 12:06:21,497 - INFO - Batch 2600, Loss: 2.5021, LR: 0.000079
2025-10-23 12:06:33,893 - INFO - Batch 2700, Loss: 2.4874, LR: 0.000079
2025-10-23 12:06:46,162 - INFO - Batch 2800, Loss: 2.5783, LR: 0.000078
2025-10-23 12:06:58,459 - INFO - Batch 2900, Loss: 2.3677, LR: 0.000078
2025-10-23 12:07:10,780 - INFO - Batch 3000, Loss: 2.4650, LR: 0.000078
2025-10-23 12:07:23,133 - INFO - Batch 3100, Loss: 2.6197, LR: 0.000078
2025-10-23 12:07:35,447 - INFO - Batch 3200, Loss: 2.3845, LR: 0.000078
2025-10-23 12:07:47,798 - INFO - Batch 3300, Loss: 2.6488, LR: 0.000078
2025-10-23 12:08:00,142 - INFO - Batch 3400, Loss: 2.6560, LR: 0.000078
2025-10-23 12:08:12,519 - INFO - Batch 3500, Loss: 2.5170, LR: 0.000078
2025-10-23 12:08:24,916 - INFO - Batch 3600, Loss: 2.4960, LR: 0.000078
2025-10-23 12:08:37,394 - INFO - Batch 3700, Loss: 2.3488, LR: 0.000078
2025-10-23 12:08:49,788 - INFO - Batch 3800, Loss: 2.4807, LR: 0.000078
2025-10-23 12:09:02,265 - INFO - Batch 3900, Loss: 2.4533, LR: 0.000078
2025-10-23 12:09:14,704 - INFO - Batch 4000, Loss: 2.4415, LR: 0.000078
2025-10-23 12:09:27,142 - INFO - Batch 4100, Loss: 2.4170, LR: 0.000078
2025-10-23 12:09:39,577 - INFO - Batch 4200, Loss: 2.6532, LR: 0.000077
2025-10-23 12:09:52,038 - INFO - Batch 4300, Loss: 2.3720, LR: 0.000077
2025-10-23 12:10:04,418 - INFO - Batch 4400, Loss: 2.2997, LR: 0.000077
2025-10-23 12:10:16,836 - INFO - Batch 4500, Loss: 2.5019, LR: 0.000077
2025-10-23 12:10:29,293 - INFO - Batch 4600, Loss: 2.4277, LR: 0.000077
2025-10-23 12:10:41,746 - INFO - Batch 4700, Loss: 2.4773, LR: 0.000077
2025-10-23 12:10:54,239 - INFO - Batch 4800, Loss: 2.5841, LR: 0.000077
2025-10-23 12:11:06,713 - INFO - Batch 4900, Loss: 2.5221, LR: 0.000077
2025-10-23 12:11:19,262 - INFO - Batch 5000, Loss: 2.4592, LR: 0.000077
2025-10-23 12:11:31,731 - INFO - Batch 5100, Loss: 2.7749, LR: 0.000077
2025-10-23 12:11:44,183 - INFO - Batch 5200, Loss: 2.5280, LR: 0.000077
2025-10-23 12:11:56,636 - INFO - Batch 5300, Loss: 2.3843, LR: 0.000077
2025-10-23 12:12:09,093 - INFO - Batch 5400, Loss: 2.6199, LR: 0.000077
2025-10-23 12:12:21,607 - INFO - Batch 5500, Loss: 2.5566, LR: 0.000077
2025-10-23 12:12:34,030 - INFO - Batch 5600, Loss: 2.6281, LR: 0.000077
2025-10-23 12:12:46,598 - INFO - Batch 5700, Loss: 2.3468, LR: 0.000076
2025-10-23 12:12:59,037 - INFO - Batch 5800, Loss: 2.5559, LR: 0.000076
2025-10-23 12:13:11,448 - INFO - Batch 5900, Loss: 2.4314, LR: 0.000076
2025-10-23 12:13:23,894 - INFO - Batch 6000, Loss: 2.3451, LR: 0.000076
2025-10-23 12:13:36,436 - INFO - Batch 6100, Loss: 2.3639, LR: 0.000076
2025-10-23 12:13:49,055 - INFO - Batch 6200, Loss: 2.5241, LR: 0.000076
2025-10-23 12:14:01,625 - INFO - Batch 6300, Loss: 2.3583, LR: 0.000076
2025-10-23 12:14:14,023 - INFO - Batch 6400, Loss: 2.5940, LR: 0.000076
2025-10-23 12:14:20,219 - INFO - Epoch 10/30: Train Loss: 2.4991, Val Loss: 2.3621, LR: 0.000076
2025-10-23 12:14:20,478 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 12:14:20,681 - INFO - 模型已保存到: ./checkpoints/checkpoint_epoch_10.pth
2025-10-23 12:14:20,810 - INFO - Batch 0, Loss: 2.3216, LR: 0.000076
2025-10-23 12:14:33,382 - INFO - Batch 100, Loss: 2.4449, LR: 0.000076
2025-10-23 12:14:45,924 - INFO - Batch 200, Loss: 2.5358, LR: 0.000076
2025-10-23 12:14:58,456 - INFO - Batch 300, Loss: 2.2983, LR: 0.000076
2025-10-23 12:15:11,025 - INFO - Batch 400, Loss: 2.5947, LR: 0.000076
2025-10-23 12:15:23,398 - INFO - Batch 500, Loss: 2.3828, LR: 0.000076
2025-10-23 12:15:35,817 - INFO - Batch 600, Loss: 2.0217, LR: 0.000076
2025-10-23 12:15:48,301 - INFO - Batch 700, Loss: 2.5493, LR: 0.000075
2025-10-23 12:16:00,755 - INFO - Batch 800, Loss: 2.6588, LR: 0.000075
2025-10-23 12:16:13,221 - INFO - Batch 900, Loss: 2.2074, LR: 0.000075
2025-10-23 12:16:25,702 - INFO - Batch 1000, Loss: 2.2606, LR: 0.000075
2025-10-23 12:16:38,163 - INFO - Batch 1100, Loss: 2.4182, LR: 0.000075
2025-10-23 12:16:50,633 - INFO - Batch 1200, Loss: 2.4593, LR: 0.000075
2025-10-23 12:17:03,084 - INFO - Batch 1300, Loss: 2.4782, LR: 0.000075
2025-10-23 12:17:15,516 - INFO - Batch 1400, Loss: 2.5033, LR: 0.000075
2025-10-23 12:17:27,961 - INFO - Batch 1500, Loss: 2.2496, LR: 0.000075
2025-10-23 12:17:40,312 - INFO - Batch 1600, Loss: 2.5360, LR: 0.000075
2025-10-23 12:17:52,702 - INFO - Batch 1700, Loss: 2.4156, LR: 0.000075
2025-10-23 12:18:05,057 - INFO - Batch 1800, Loss: 2.2223, LR: 0.000075
2025-10-23 12:18:17,449 - INFO - Batch 1900, Loss: 2.6659, LR: 0.000075
2025-10-23 12:18:29,821 - INFO - Batch 2000, Loss: 2.5782, LR: 0.000075
2025-10-23 12:18:42,381 - INFO - Batch 2100, Loss: 2.4507, LR: 0.000074
2025-10-23 12:18:54,888 - INFO - Batch 2200, Loss: 2.4595, LR: 0.000074
2025-10-23 12:19:07,456 - INFO - Batch 2300, Loss: 2.7485, LR: 0.000074
2025-10-23 12:19:20,069 - INFO - Batch 2400, Loss: 2.3436, LR: 0.000074
2025-10-23 12:19:32,689 - INFO - Batch 2500, Loss: 2.3521, LR: 0.000074
2025-10-23 12:19:45,300 - INFO - Batch 2600, Loss: 2.4951, LR: 0.000074
2025-10-23 12:19:57,912 - INFO - Batch 2700, Loss: 2.2008, LR: 0.000074
2025-10-23 12:20:10,546 - INFO - Batch 2800, Loss: 2.4991, LR: 0.000074
2025-10-23 12:20:23,164 - INFO - Batch 2900, Loss: 2.6593, LR: 0.000074
2025-10-23 12:20:35,781 - INFO - Batch 3000, Loss: 2.7465, LR: 0.000074
2025-10-23 12:20:48,423 - INFO - Batch 3100, Loss: 2.1868, LR: 0.000074
2025-10-23 12:21:01,066 - INFO - Batch 3200, Loss: 2.4239, LR: 0.000074
2025-10-23 12:21:13,621 - INFO - Batch 3300, Loss: 2.4744, LR: 0.000074
2025-10-23 12:21:26,113 - INFO - Batch 3400, Loss: 2.5581, LR: 0.000074
2025-10-23 12:21:38,570 - INFO - Batch 3500, Loss: 2.3855, LR: 0.000073
2025-10-23 12:21:51,045 - INFO - Batch 3600, Loss: 3.0465, LR: 0.000073
2025-10-23 12:22:03,547 - INFO - Batch 3700, Loss: 2.7145, LR: 0.000073
2025-10-23 12:22:16,048 - INFO - Batch 3800, Loss: 2.6871, LR: 0.000073
2025-10-23 12:22:28,554 - INFO - Batch 3900, Loss: 2.5003, LR: 0.000073
2025-10-23 12:22:40,988 - INFO - Batch 4000, Loss: 2.5915, LR: 0.000073
2025-10-23 12:22:53,401 - INFO - Batch 4100, Loss: 2.4414, LR: 0.000073
2025-10-23 12:23:05,810 - INFO - Batch 4200, Loss: 2.4816, LR: 0.000073
2025-10-23 12:23:18,228 - INFO - Batch 4300, Loss: 2.2137, LR: 0.000073
2025-10-23 12:23:30,613 - INFO - Batch 4400, Loss: 2.3143, LR: 0.000073
2025-10-23 12:23:43,000 - INFO - Batch 4500, Loss: 2.3810, LR: 0.000073
2025-10-23 12:23:55,375 - INFO - Batch 4600, Loss: 2.2121, LR: 0.000073
2025-10-23 12:24:07,767 - INFO - Batch 4700, Loss: 2.5402, LR: 0.000073
2025-10-23 12:24:20,150 - INFO - Batch 4800, Loss: 2.7406, LR: 0.000072
2025-10-23 12:24:32,509 - INFO - Batch 4900, Loss: 2.4263, LR: 0.000072
2025-10-23 12:24:44,927 - INFO - Batch 5000, Loss: 2.4163, LR: 0.000072
2025-10-23 12:24:57,395 - INFO - Batch 5100, Loss: 2.1900, LR: 0.000072
2025-10-23 12:25:09,840 - INFO - Batch 5200, Loss: 2.5177, LR: 0.000072
2025-10-23 12:25:22,237 - INFO - Batch 5300, Loss: 2.4595, LR: 0.000072
2025-10-23 12:25:34,622 - INFO - Batch 5400, Loss: 2.6874, LR: 0.000072
2025-10-23 12:25:46,977 - INFO - Batch 5500, Loss: 2.7101, LR: 0.000072
2025-10-23 12:25:59,322 - INFO - Batch 5600, Loss: 2.3800, LR: 0.000072
2025-10-23 12:26:11,669 - INFO - Batch 5700, Loss: 2.5765, LR: 0.000072
2025-10-23 12:26:24,029 - INFO - Batch 5800, Loss: 2.7524, LR: 0.000072
2025-10-23 12:26:36,506 - INFO - Batch 5900, Loss: 2.4853, LR: 0.000072
2025-10-23 12:26:49,084 - INFO - Batch 6000, Loss: 2.9458, LR: 0.000072
2025-10-23 12:27:01,664 - INFO - Batch 6100, Loss: 2.4932, LR: 0.000072
2025-10-23 12:27:14,236 - INFO - Batch 6200, Loss: 2.2492, LR: 0.000071
2025-10-23 12:27:26,825 - INFO - Batch 6300, Loss: 2.4895, LR: 0.000071
2025-10-23 12:27:39,417 - INFO - Batch 6400, Loss: 2.5089, LR: 0.000071
2025-10-23 12:27:45,676 - INFO - Epoch 11/30: Train Loss: 2.4628, Val Loss: 2.3323, LR: 0.000071
2025-10-23 12:27:45,936 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 12:27:46,066 - INFO - Batch 0, Loss: 1.9606, LR: 0.000071
2025-10-23 12:27:58,650 - INFO - Batch 100, Loss: 2.4287, LR: 0.000071
2025-10-23 12:28:11,234 - INFO - Batch 200, Loss: 2.4769, LR: 0.000071
2025-10-23 12:28:23,798 - INFO - Batch 300, Loss: 2.1858, LR: 0.000071
2025-10-23 12:28:36,205 - INFO - Batch 400, Loss: 2.3611, LR: 0.000071
2025-10-23 12:28:48,497 - INFO - Batch 500, Loss: 2.5072, LR: 0.000071
2025-10-23 12:29:00,780 - INFO - Batch 600, Loss: 2.3106, LR: 0.000071
2025-10-23 12:29:13,067 - INFO - Batch 700, Loss: 2.7275, LR: 0.000071
2025-10-23 12:29:25,349 - INFO - Batch 800, Loss: 2.3786, LR: 0.000071
2025-10-23 12:29:37,588 - INFO - Batch 900, Loss: 2.3043, LR: 0.000071
2025-10-23 12:29:49,832 - INFO - Batch 1000, Loss: 2.5324, LR: 0.000071
2025-10-23 12:30:02,085 - INFO - Batch 1100, Loss: 2.5219, LR: 0.000070
2025-10-23 12:30:14,519 - INFO - Batch 1200, Loss: 2.2144, LR: 0.000070
2025-10-23 12:30:27,050 - INFO - Batch 1300, Loss: 2.7419, LR: 0.000070
2025-10-23 12:30:39,418 - INFO - Batch 1400, Loss: 2.4982, LR: 0.000070
2025-10-23 12:30:51,744 - INFO - Batch 1500, Loss: 2.3811, LR: 0.000070
2025-10-23 12:31:04,058 - INFO - Batch 1600, Loss: 2.3543, LR: 0.000070
2025-10-23 12:31:16,357 - INFO - Batch 1700, Loss: 2.7755, LR: 0.000070
2025-10-23 12:31:28,711 - INFO - Batch 1800, Loss: 2.2438, LR: 0.000070
2025-10-23 12:31:41,069 - INFO - Batch 1900, Loss: 2.1903, LR: 0.000070
2025-10-23 12:31:53,407 - INFO - Batch 2000, Loss: 2.3206, LR: 0.000070
2025-10-23 12:32:05,793 - INFO - Batch 2100, Loss: 2.5429, LR: 0.000070
2025-10-23 12:32:18,167 - INFO - Batch 2200, Loss: 2.4793, LR: 0.000070
2025-10-23 12:32:30,535 - INFO - Batch 2300, Loss: 2.3526, LR: 0.000070
2025-10-23 12:32:42,908 - INFO - Batch 2400, Loss: 2.6382, LR: 0.000069
2025-10-23 12:32:55,273 - INFO - Batch 2500, Loss: 2.4694, LR: 0.000069
2025-10-23 12:33:07,646 - INFO - Batch 2600, Loss: 2.6442, LR: 0.000069
2025-10-23 12:33:20,048 - INFO - Batch 2700, Loss: 2.3892, LR: 0.000069
2025-10-23 12:33:32,441 - INFO - Batch 2800, Loss: 2.4024, LR: 0.000069
2025-10-23 12:33:44,841 - INFO - Batch 2900, Loss: 2.4489, LR: 0.000069
2025-10-23 12:33:57,245 - INFO - Batch 3000, Loss: 2.5572, LR: 0.000069
2025-10-23 12:34:09,640 - INFO - Batch 3100, Loss: 2.6472, LR: 0.000069
2025-10-23 12:34:22,006 - INFO - Batch 3200, Loss: 2.8740, LR: 0.000069
2025-10-23 12:34:34,397 - INFO - Batch 3300, Loss: 2.4979, LR: 0.000069
2025-10-23 12:34:46,805 - INFO - Batch 3400, Loss: 2.5289, LR: 0.000069
2025-10-23 12:34:59,251 - INFO - Batch 3500, Loss: 2.5076, LR: 0.000069
2025-10-23 12:35:11,910 - INFO - Batch 3600, Loss: 2.3881, LR: 0.000069
2025-10-23 12:35:24,248 - INFO - Batch 3700, Loss: 2.3462, LR: 0.000068
2025-10-23 12:35:36,633 - INFO - Batch 3800, Loss: 2.5897, LR: 0.000068
2025-10-23 12:35:49,029 - INFO - Batch 3900, Loss: 2.3166, LR: 0.000068
2025-10-23 12:36:01,380 - INFO - Batch 4000, Loss: 2.4537, LR: 0.000068
2025-10-23 12:36:13,822 - INFO - Batch 4100, Loss: 2.4880, LR: 0.000068
2025-10-23 12:36:26,203 - INFO - Batch 4200, Loss: 2.2980, LR: 0.000068
2025-10-23 12:36:38,597 - INFO - Batch 4300, Loss: 2.3305, LR: 0.000068
2025-10-23 12:36:51,001 - INFO - Batch 4400, Loss: 2.5555, LR: 0.000068
2025-10-23 12:37:03,373 - INFO - Batch 4500, Loss: 2.4962, LR: 0.000068
2025-10-23 12:37:15,847 - INFO - Batch 4600, Loss: 2.5053, LR: 0.000068
2025-10-23 12:37:28,333 - INFO - Batch 4700, Loss: 2.5621, LR: 0.000068
2025-10-23 12:37:40,756 - INFO - Batch 4800, Loss: 2.6577, LR: 0.000068
2025-10-23 12:37:53,153 - INFO - Batch 4900, Loss: 2.4425, LR: 0.000068
2025-10-23 12:38:05,562 - INFO - Batch 5000, Loss: 2.3389, LR: 0.000067
2025-10-23 12:38:18,023 - INFO - Batch 5100, Loss: 2.5850, LR: 0.000067
2025-10-23 12:38:30,442 - INFO - Batch 5200, Loss: 2.2334, LR: 0.000067
2025-10-23 12:38:42,858 - INFO - Batch 5300, Loss: 2.2326, LR: 0.000067
2025-10-23 12:38:55,270 - INFO - Batch 5400, Loss: 2.5846, LR: 0.000067
2025-10-23 12:39:07,676 - INFO - Batch 5500, Loss: 2.5485, LR: 0.000067
2025-10-23 12:39:20,073 - INFO - Batch 5600, Loss: 2.1697, LR: 0.000067
2025-10-23 12:39:32,441 - INFO - Batch 5700, Loss: 2.2301, LR: 0.000067
2025-10-23 12:39:44,795 - INFO - Batch 5800, Loss: 2.5657, LR: 0.000067
2025-10-23 12:39:57,142 - INFO - Batch 5900, Loss: 2.2930, LR: 0.000067
2025-10-23 12:40:09,484 - INFO - Batch 6000, Loss: 2.2353, LR: 0.000067
2025-10-23 12:40:21,816 - INFO - Batch 6100, Loss: 2.6336, LR: 0.000067
2025-10-23 12:40:34,133 - INFO - Batch 6200, Loss: 2.3746, LR: 0.000067
2025-10-23 12:40:46,444 - INFO - Batch 6300, Loss: 2.3952, LR: 0.000066
2025-10-23 12:40:58,791 - INFO - Batch 6400, Loss: 2.5638, LR: 0.000066
2025-10-23 12:41:04,944 - INFO - Epoch 12/30: Train Loss: 2.4334, Val Loss: 2.3069, LR: 0.000066
2025-10-23 12:41:05,206 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 12:41:05,336 - INFO - Batch 0, Loss: 2.4692, LR: 0.000066
2025-10-23 12:41:17,921 - INFO - Batch 100, Loss: 2.0926, LR: 0.000066
2025-10-23 12:41:30,475 - INFO - Batch 200, Loss: 2.4476, LR: 0.000066
2025-10-23 12:41:42,854 - INFO - Batch 300, Loss: 2.3405, LR: 0.000066
2025-10-23 12:41:55,122 - INFO - Batch 400, Loss: 2.2798, LR: 0.000066
2025-10-23 12:42:07,402 - INFO - Batch 500, Loss: 2.3889, LR: 0.000066
2025-10-23 12:42:19,768 - INFO - Batch 600, Loss: 2.5220, LR: 0.000066
2025-10-23 12:42:32,079 - INFO - Batch 700, Loss: 2.3115, LR: 0.000066
2025-10-23 12:42:44,387 - INFO - Batch 800, Loss: 2.0475, LR: 0.000066
2025-10-23 12:42:56,670 - INFO - Batch 900, Loss: 2.3590, LR: 0.000066
2025-10-23 12:43:09,081 - INFO - Batch 1000, Loss: 2.2931, LR: 0.000066
2025-10-23 12:43:21,379 - INFO - Batch 1100, Loss: 2.3602, LR: 0.000066
2025-10-23 12:43:33,661 - INFO - Batch 1200, Loss: 2.5436, LR: 0.000065
2025-10-23 12:43:45,984 - INFO - Batch 1300, Loss: 2.5758, LR: 0.000065
2025-10-23 12:43:58,286 - INFO - Batch 1400, Loss: 2.5541, LR: 0.000065
2025-10-23 12:44:10,603 - INFO - Batch 1500, Loss: 2.2015, LR: 0.000065
2025-10-23 12:44:22,928 - INFO - Batch 1600, Loss: 2.3991, LR: 0.000065
2025-10-23 12:44:35,234 - INFO - Batch 1700, Loss: 2.6625, LR: 0.000065
2025-10-23 12:44:47,567 - INFO - Batch 1800, Loss: 2.1137, LR: 0.000065
2025-10-23 12:44:59,936 - INFO - Batch 1900, Loss: 2.4321, LR: 0.000065
2025-10-23 12:45:12,300 - INFO - Batch 2000, Loss: 2.1548, LR: 0.000065
2025-10-23 12:45:24,802 - INFO - Batch 2100, Loss: 2.5385, LR: 0.000065
2025-10-23 12:45:37,324 - INFO - Batch 2200, Loss: 2.6240, LR: 0.000065
2025-10-23 12:45:49,863 - INFO - Batch 2300, Loss: 2.3887, LR: 0.000065
2025-10-23 12:46:02,394 - INFO - Batch 2400, Loss: 2.2768, LR: 0.000065
2025-10-23 12:46:14,941 - INFO - Batch 2500, Loss: 2.3828, LR: 0.000064
2025-10-23 12:46:27,574 - INFO - Batch 2600, Loss: 2.3265, LR: 0.000064
2025-10-23 12:46:40,217 - INFO - Batch 2700, Loss: 2.1405, LR: 0.000064
2025-10-23 12:46:52,804 - INFO - Batch 2800, Loss: 2.2570, LR: 0.000064
2025-10-23 12:47:05,301 - INFO - Batch 2900, Loss: 2.2129, LR: 0.000064
2025-10-23 12:47:17,818 - INFO - Batch 3000, Loss: 2.6241, LR: 0.000064
2025-10-23 12:47:30,351 - INFO - Batch 3100, Loss: 2.6178, LR: 0.000064
2025-10-23 12:47:42,874 - INFO - Batch 3200, Loss: 2.1216, LR: 0.000064
2025-10-23 12:47:55,430 - INFO - Batch 3300, Loss: 2.1530, LR: 0.000064
2025-10-23 12:48:07,976 - INFO - Batch 3400, Loss: 2.3796, LR: 0.000064
2025-10-23 12:48:20,515 - INFO - Batch 3500, Loss: 2.3989, LR: 0.000064
2025-10-23 12:48:33,053 - INFO - Batch 3600, Loss: 2.4380, LR: 0.000064
2025-10-23 12:48:45,606 - INFO - Batch 3700, Loss: 2.1684, LR: 0.000063
2025-10-23 12:48:58,145 - INFO - Batch 3800, Loss: 2.4012, LR: 0.000063
2025-10-23 12:49:10,678 - INFO - Batch 3900, Loss: 2.2154, LR: 0.000063
2025-10-23 12:49:23,221 - INFO - Batch 4000, Loss: 2.2950, LR: 0.000063
2025-10-23 12:49:35,759 - INFO - Batch 4100, Loss: 2.4063, LR: 0.000063
2025-10-23 12:49:48,255 - INFO - Batch 4200, Loss: 2.3034, LR: 0.000063
2025-10-23 12:50:00,729 - INFO - Batch 4300, Loss: 2.5090, LR: 0.000063
2025-10-23 12:50:13,204 - INFO - Batch 4400, Loss: 2.4536, LR: 0.000063
2025-10-23 12:50:25,675 - INFO - Batch 4500, Loss: 2.2617, LR: 0.000063
2025-10-23 12:50:38,151 - INFO - Batch 4600, Loss: 2.1682, LR: 0.000063
2025-10-23 12:50:50,619 - INFO - Batch 4700, Loss: 2.2676, LR: 0.000063
2025-10-23 12:51:03,114 - INFO - Batch 4800, Loss: 2.3904, LR: 0.000063
2025-10-23 12:51:15,589 - INFO - Batch 4900, Loss: 2.3057, LR: 0.000063
2025-10-23 12:51:28,091 - INFO - Batch 5000, Loss: 2.2389, LR: 0.000062
2025-10-23 12:51:40,414 - INFO - Batch 5100, Loss: 2.1614, LR: 0.000062
2025-10-23 12:51:52,729 - INFO - Batch 5200, Loss: 2.5235, LR: 0.000062
2025-10-23 12:52:05,335 - INFO - Batch 5300, Loss: 2.2205, LR: 0.000062
2025-10-23 12:52:17,817 - INFO - Batch 5400, Loss: 2.2010, LR: 0.000062
2025-10-23 12:52:30,291 - INFO - Batch 5500, Loss: 2.2034, LR: 0.000062
2025-10-23 12:52:42,587 - INFO - Batch 5600, Loss: 2.3015, LR: 0.000062
2025-10-23 12:52:54,979 - INFO - Batch 5700, Loss: 2.5245, LR: 0.000062
2025-10-23 12:53:07,353 - INFO - Batch 5800, Loss: 2.3361, LR: 0.000062
2025-10-23 12:53:19,645 - INFO - Batch 5900, Loss: 2.1024, LR: 0.000062
2025-10-23 12:53:32,002 - INFO - Batch 6000, Loss: 2.4396, LR: 0.000062
2025-10-23 12:53:44,421 - INFO - Batch 6100, Loss: 2.7522, LR: 0.000062
2025-10-23 12:53:56,905 - INFO - Batch 6200, Loss: 2.3165, LR: 0.000061
2025-10-23 12:54:09,215 - INFO - Batch 6300, Loss: 2.5605, LR: 0.000061
2025-10-23 12:54:21,508 - INFO - Batch 6400, Loss: 2.3814, LR: 0.000061
2025-10-23 12:54:27,641 - INFO - Epoch 13/30: Train Loss: 2.4082, Val Loss: 2.2914, LR: 0.000061
2025-10-23 12:54:27,899 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 12:54:28,027 - INFO - Batch 0, Loss: 2.4264, LR: 0.000061
2025-10-23 12:54:40,349 - INFO - Batch 100, Loss: 2.3280, LR: 0.000061
2025-10-23 12:54:52,600 - INFO - Batch 200, Loss: 2.1788, LR: 0.000061
2025-10-23 12:55:04,865 - INFO - Batch 300, Loss: 2.5453, LR: 0.000061
2025-10-23 12:55:17,373 - INFO - Batch 400, Loss: 2.3530, LR: 0.000061
2025-10-23 12:55:29,676 - INFO - Batch 500, Loss: 2.3852, LR: 0.000061
2025-10-23 12:55:41,946 - INFO - Batch 600, Loss: 2.4255, LR: 0.000061
2025-10-23 12:55:54,200 - INFO - Batch 700, Loss: 2.2728, LR: 0.000061
2025-10-23 12:56:06,458 - INFO - Batch 800, Loss: 2.1705, LR: 0.000061
2025-10-23 12:56:18,887 - INFO - Batch 900, Loss: 2.4283, LR: 0.000061
2025-10-23 12:56:31,344 - INFO - Batch 1000, Loss: 2.4834, LR: 0.000061
2025-10-23 12:56:43,629 - INFO - Batch 1100, Loss: 2.3098, LR: 0.000060
2025-10-23 12:56:55,949 - INFO - Batch 1200, Loss: 2.0673, LR: 0.000060
2025-10-23 12:57:08,313 - INFO - Batch 1300, Loss: 2.0514, LR: 0.000060
2025-10-23 12:57:20,749 - INFO - Batch 1400, Loss: 2.5855, LR: 0.000060
2025-10-23 12:57:33,164 - INFO - Batch 1500, Loss: 2.5372, LR: 0.000060
2025-10-23 12:57:45,627 - INFO - Batch 1600, Loss: 2.6310, LR: 0.000060
2025-10-23 12:57:58,198 - INFO - Batch 1700, Loss: 2.6134, LR: 0.000060
2025-10-23 12:58:10,779 - INFO - Batch 1800, Loss: 2.3501, LR: 0.000060
2025-10-23 12:58:23,313 - INFO - Batch 1900, Loss: 2.2964, LR: 0.000060
2025-10-23 12:58:35,814 - INFO - Batch 2000, Loss: 2.3879, LR: 0.000060
2025-10-23 12:58:48,428 - INFO - Batch 2100, Loss: 2.2913, LR: 0.000060
2025-10-23 12:59:00,879 - INFO - Batch 2200, Loss: 2.3674, LR: 0.000060
2025-10-23 12:59:13,358 - INFO - Batch 2300, Loss: 2.3556, LR: 0.000059
2025-10-23 12:59:26,036 - INFO - Batch 2400, Loss: 2.3833, LR: 0.000059
2025-10-23 12:59:38,473 - INFO - Batch 2500, Loss: 2.1956, LR: 0.000059
2025-10-23 12:59:50,952 - INFO - Batch 2600, Loss: 2.5788, LR: 0.000059
2025-10-23 13:00:03,393 - INFO - Batch 2700, Loss: 2.3686, LR: 0.000059
2025-10-23 13:00:15,793 - INFO - Batch 2800, Loss: 2.2476, LR: 0.000059
2025-10-23 13:00:28,171 - INFO - Batch 2900, Loss: 2.3274, LR: 0.000059
2025-10-23 13:00:40,569 - INFO - Batch 3000, Loss: 2.4081, LR: 0.000059
2025-10-23 13:00:52,984 - INFO - Batch 3100, Loss: 2.3594, LR: 0.000059
2025-10-23 13:01:05,387 - INFO - Batch 3200, Loss: 2.1931, LR: 0.000059
2025-10-23 13:01:17,791 - INFO - Batch 3300, Loss: 2.2856, LR: 0.000059
2025-10-23 13:01:30,186 - INFO - Batch 3400, Loss: 2.4243, LR: 0.000059
2025-10-23 13:01:42,541 - INFO - Batch 3500, Loss: 2.3280, LR: 0.000058
2025-10-23 13:01:54,907 - INFO - Batch 3600, Loss: 2.4385, LR: 0.000058
2025-10-23 13:02:07,280 - INFO - Batch 3700, Loss: 2.4016, LR: 0.000058
2025-10-23 13:02:19,774 - INFO - Batch 3800, Loss: 2.3632, LR: 0.000058
2025-10-23 13:02:32,275 - INFO - Batch 3900, Loss: 2.2341, LR: 0.000058
2025-10-23 13:02:44,867 - INFO - Batch 4000, Loss: 2.4967, LR: 0.000058
2025-10-23 13:02:57,334 - INFO - Batch 4100, Loss: 2.0825, LR: 0.000058
2025-10-23 13:03:09,871 - INFO - Batch 4200, Loss: 2.3870, LR: 0.000058
2025-10-23 13:03:22,441 - INFO - Batch 4300, Loss: 2.4099, LR: 0.000058
2025-10-23 13:03:34,946 - INFO - Batch 4400, Loss: 2.4878, LR: 0.000058
2025-10-23 13:03:47,552 - INFO - Batch 4500, Loss: 2.3907, LR: 0.000058
2025-10-23 13:04:00,032 - INFO - Batch 4600, Loss: 2.4500, LR: 0.000058
2025-10-23 13:04:12,542 - INFO - Batch 4700, Loss: 2.1781, LR: 0.000058
2025-10-23 13:04:25,122 - INFO - Batch 4800, Loss: 2.4104, LR: 0.000057
2025-10-23 13:04:37,629 - INFO - Batch 4900, Loss: 2.1978, LR: 0.000057
2025-10-23 13:04:50,162 - INFO - Batch 5000, Loss: 2.5893, LR: 0.000057
2025-10-23 13:05:02,700 - INFO - Batch 5100, Loss: 2.5518, LR: 0.000057
2025-10-23 13:05:15,267 - INFO - Batch 5200, Loss: 2.5877, LR: 0.000057
2025-10-23 13:05:27,769 - INFO - Batch 5300, Loss: 2.4109, LR: 0.000057
2025-10-23 13:05:40,274 - INFO - Batch 5400, Loss: 2.4084, LR: 0.000057
2025-10-23 13:05:52,773 - INFO - Batch 5500, Loss: 2.5240, LR: 0.000057
2025-10-23 13:06:05,287 - INFO - Batch 5600, Loss: 2.3808, LR: 0.000057
2025-10-23 13:06:17,788 - INFO - Batch 5700, Loss: 2.5674, LR: 0.000057
2025-10-23 13:06:30,297 - INFO - Batch 5800, Loss: 2.3912, LR: 0.000057
2025-10-23 13:06:42,823 - INFO - Batch 5900, Loss: 2.3884, LR: 0.000057
2025-10-23 13:06:55,354 - INFO - Batch 6000, Loss: 2.2239, LR: 0.000056
2025-10-23 13:07:07,835 - INFO - Batch 6100, Loss: 2.2470, LR: 0.000056
2025-10-23 13:07:20,316 - INFO - Batch 6200, Loss: 2.3161, LR: 0.000056
2025-10-23 13:07:32,599 - INFO - Batch 6300, Loss: 2.3584, LR: 0.000056
2025-10-23 13:07:44,926 - INFO - Batch 6400, Loss: 2.3077, LR: 0.000056
2025-10-23 13:07:51,095 - INFO - Epoch 14/30: Train Loss: 2.3865, Val Loss: 2.2700, LR: 0.000056
2025-10-23 13:07:51,366 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 13:07:51,494 - INFO - Batch 0, Loss: 2.4315, LR: 0.000056
2025-10-23 13:08:03,809 - INFO - Batch 100, Loss: 2.2387, LR: 0.000056
2025-10-23 13:08:16,111 - INFO - Batch 200, Loss: 2.4088, LR: 0.000056
2025-10-23 13:08:28,422 - INFO - Batch 300, Loss: 2.3241, LR: 0.000056
2025-10-23 13:08:40,805 - INFO - Batch 400, Loss: 2.2233, LR: 0.000056
2025-10-23 13:08:53,272 - INFO - Batch 500, Loss: 2.5027, LR: 0.000056
2025-10-23 13:09:05,801 - INFO - Batch 600, Loss: 2.4946, LR: 0.000056
2025-10-23 13:09:18,340 - INFO - Batch 700, Loss: 2.7176, LR: 0.000056
2025-10-23 13:09:30,703 - INFO - Batch 800, Loss: 2.4159, LR: 0.000055
2025-10-23 13:09:43,074 - INFO - Batch 900, Loss: 2.2833, LR: 0.000055
2025-10-23 13:09:55,446 - INFO - Batch 1000, Loss: 2.5956, LR: 0.000055
2025-10-23 13:10:07,798 - INFO - Batch 1100, Loss: 2.5033, LR: 0.000055
2025-10-23 13:10:20,225 - INFO - Batch 1200, Loss: 2.3640, LR: 0.000055
2025-10-23 13:10:32,706 - INFO - Batch 1300, Loss: 2.2466, LR: 0.000055
2025-10-23 13:10:45,274 - INFO - Batch 1400, Loss: 2.4657, LR: 0.000055
2025-10-23 13:10:57,926 - INFO - Batch 1500, Loss: 2.3870, LR: 0.000055
2025-10-23 13:11:10,491 - INFO - Batch 1600, Loss: 2.4190, LR: 0.000055
2025-10-23 13:11:23,037 - INFO - Batch 1700, Loss: 2.2463, LR: 0.000055
2025-10-23 13:11:35,556 - INFO - Batch 1800, Loss: 1.9784, LR: 0.000055
2025-10-23 13:11:48,053 - INFO - Batch 1900, Loss: 2.5701, LR: 0.000055
2025-10-23 13:12:00,558 - INFO - Batch 2000, Loss: 2.3402, LR: 0.000054
2025-10-23 13:12:13,059 - INFO - Batch 2100, Loss: 2.2402, LR: 0.000054
2025-10-23 13:12:25,578 - INFO - Batch 2200, Loss: 2.4340, LR: 0.000054
2025-10-23 13:12:38,098 - INFO - Batch 2300, Loss: 2.5587, LR: 0.000054
2025-10-23 13:12:50,602 - INFO - Batch 2400, Loss: 2.2494, LR: 0.000054
2025-10-23 13:13:03,096 - INFO - Batch 2500, Loss: 2.4730, LR: 0.000054
2025-10-23 13:13:15,562 - INFO - Batch 2600, Loss: 2.3462, LR: 0.000054
2025-10-23 13:13:28,041 - INFO - Batch 2700, Loss: 2.3541, LR: 0.000054
2025-10-23 13:13:40,545 - INFO - Batch 2800, Loss: 1.9472, LR: 0.000054
2025-10-23 13:13:53,125 - INFO - Batch 2900, Loss: 2.2059, LR: 0.000054
2025-10-23 13:14:05,634 - INFO - Batch 3000, Loss: 2.3172, LR: 0.000054
2025-10-23 13:14:18,069 - INFO - Batch 3100, Loss: 2.3978, LR: 0.000054
2025-10-23 13:14:30,510 - INFO - Batch 3200, Loss: 2.3732, LR: 0.000053
2025-10-23 13:14:42,920 - INFO - Batch 3300, Loss: 2.5745, LR: 0.000053
2025-10-23 13:14:55,327 - INFO - Batch 3400, Loss: 2.4406, LR: 0.000053
2025-10-23 13:15:07,756 - INFO - Batch 3500, Loss: 2.3059, LR: 0.000053
2025-10-23 13:15:20,183 - INFO - Batch 3600, Loss: 2.2325, LR: 0.000053
2025-10-23 13:15:32,608 - INFO - Batch 3700, Loss: 2.3232, LR: 0.000053
2025-10-23 13:15:45,097 - INFO - Batch 3800, Loss: 2.4494, LR: 0.000053
2025-10-23 13:15:57,482 - INFO - Batch 3900, Loss: 2.4660, LR: 0.000053
2025-10-23 13:16:09,861 - INFO - Batch 4000, Loss: 2.4616, LR: 0.000053
2025-10-23 13:16:22,273 - INFO - Batch 4100, Loss: 2.2922, LR: 0.000053
2025-10-23 13:16:34,753 - INFO - Batch 4200, Loss: 2.4232, LR: 0.000053
2025-10-23 13:16:47,165 - INFO - Batch 4300, Loss: 2.2885, LR: 0.000053
2025-10-23 13:16:59,545 - INFO - Batch 4400, Loss: 2.4833, LR: 0.000052
2025-10-23 13:17:11,953 - INFO - Batch 4500, Loss: 2.3676, LR: 0.000052
2025-10-23 13:17:24,383 - INFO - Batch 4600, Loss: 2.2432, LR: 0.000052
2025-10-23 13:17:36,724 - INFO - Batch 4700, Loss: 2.3032, LR: 0.000052
2025-10-23 13:17:49,095 - INFO - Batch 4800, Loss: 2.6451, LR: 0.000052
2025-10-23 13:18:01,474 - INFO - Batch 4900, Loss: 2.5764, LR: 0.000052
2025-10-23 13:18:13,874 - INFO - Batch 5000, Loss: 2.4413, LR: 0.000052
2025-10-23 13:18:26,322 - INFO - Batch 5100, Loss: 2.1953, LR: 0.000052
2025-10-23 13:18:38,675 - INFO - Batch 5200, Loss: 2.3382, LR: 0.000052
2025-10-23 13:18:51,037 - INFO - Batch 5300, Loss: 2.5240, LR: 0.000052
2025-10-23 13:19:03,401 - INFO - Batch 5400, Loss: 2.2065, LR: 0.000052
2025-10-23 13:19:15,790 - INFO - Batch 5500, Loss: 2.3287, LR: 0.000052
2025-10-23 13:19:28,199 - INFO - Batch 5600, Loss: 2.3643, LR: 0.000052
2025-10-23 13:19:40,603 - INFO - Batch 5700, Loss: 2.2369, LR: 0.000051
2025-10-23 13:19:52,976 - INFO - Batch 5800, Loss: 2.4492, LR: 0.000051
2025-10-23 13:20:05,365 - INFO - Batch 5900, Loss: 2.3636, LR: 0.000051
2025-10-23 13:20:17,742 - INFO - Batch 6000, Loss: 2.4114, LR: 0.000051
2025-10-23 13:20:30,164 - INFO - Batch 6100, Loss: 2.6013, LR: 0.000051
2025-10-23 13:20:42,657 - INFO - Batch 6200, Loss: 2.5391, LR: 0.000051
2025-10-23 13:20:55,053 - INFO - Batch 6300, Loss: 2.4100, LR: 0.000051
2025-10-23 13:21:07,595 - INFO - Batch 6400, Loss: 2.1107, LR: 0.000051
2025-10-23 13:21:13,844 - INFO - Epoch 15/30: Train Loss: 2.3658, Val Loss: 2.2593, LR: 0.000051
2025-10-23 13:21:14,105 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 13:21:14,235 - INFO - Batch 0, Loss: 1.9959, LR: 0.000051
2025-10-23 13:21:26,657 - INFO - Batch 100, Loss: 2.1640, LR: 0.000051
2025-10-23 13:21:39,088 - INFO - Batch 200, Loss: 2.3764, LR: 0.000051
2025-10-23 13:21:51,534 - INFO - Batch 300, Loss: 2.2975, LR: 0.000051
2025-10-23 13:22:03,955 - INFO - Batch 400, Loss: 2.4920, LR: 0.000050
2025-10-23 13:22:16,431 - INFO - Batch 500, Loss: 2.2250, LR: 0.000050
2025-10-23 13:22:28,942 - INFO - Batch 600, Loss: 2.5937, LR: 0.000050
2025-10-23 13:22:41,404 - INFO - Batch 700, Loss: 2.3307, LR: 0.000050
2025-10-23 13:22:53,865 - INFO - Batch 800, Loss: 2.1201, LR: 0.000050
2025-10-23 13:23:06,290 - INFO - Batch 900, Loss: 2.3009, LR: 0.000050
2025-10-23 13:23:18,737 - INFO - Batch 1000, Loss: 2.5924, LR: 0.000050
2025-10-23 13:23:31,165 - INFO - Batch 1100, Loss: 2.2876, LR: 0.000050
2025-10-23 13:23:43,604 - INFO - Batch 1200, Loss: 2.2623, LR: 0.000050
2025-10-23 13:23:56,130 - INFO - Batch 1300, Loss: 2.2851, LR: 0.000050
2025-10-23 13:24:08,622 - INFO - Batch 1400, Loss: 2.0907, LR: 0.000050
2025-10-23 13:24:21,101 - INFO - Batch 1500, Loss: 2.2218, LR: 0.000050
2025-10-23 13:24:33,589 - INFO - Batch 1600, Loss: 2.2341, LR: 0.000050
2025-10-23 13:24:46,045 - INFO - Batch 1700, Loss: 2.3285, LR: 0.000049
2025-10-23 13:24:58,518 - INFO - Batch 1800, Loss: 2.3805, LR: 0.000049
2025-10-23 13:25:11,040 - INFO - Batch 1900, Loss: 2.4667, LR: 0.000049
2025-10-23 13:25:23,570 - INFO - Batch 2000, Loss: 2.0052, LR: 0.000049
2025-10-23 13:25:36,094 - INFO - Batch 2100, Loss: 2.3017, LR: 0.000049
2025-10-23 13:25:48,728 - INFO - Batch 2200, Loss: 2.2736, LR: 0.000049
2025-10-23 13:26:01,287 - INFO - Batch 2300, Loss: 2.2444, LR: 0.000049
2025-10-23 13:26:13,826 - INFO - Batch 2400, Loss: 2.5302, LR: 0.000049
2025-10-23 13:26:26,290 - INFO - Batch 2500, Loss: 2.2471, LR: 0.000049
2025-10-23 13:26:38,777 - INFO - Batch 2600, Loss: 2.4359, LR: 0.000049
2025-10-23 13:26:51,269 - INFO - Batch 2700, Loss: 2.4039, LR: 0.000049
2025-10-23 13:27:03,766 - INFO - Batch 2800, Loss: 2.4056, LR: 0.000049
2025-10-23 13:27:16,265 - INFO - Batch 2900, Loss: 2.5523, LR: 0.000048
2025-10-23 13:27:28,816 - INFO - Batch 3000, Loss: 2.2786, LR: 0.000048
2025-10-23 13:27:41,328 - INFO - Batch 3100, Loss: 2.2404, LR: 0.000048
2025-10-23 13:27:53,854 - INFO - Batch 3200, Loss: 2.3917, LR: 0.000048
2025-10-23 13:28:06,378 - INFO - Batch 3300, Loss: 2.2251, LR: 0.000048
2025-10-23 13:28:18,862 - INFO - Batch 3400, Loss: 2.4622, LR: 0.000048
2025-10-23 13:28:31,340 - INFO - Batch 3500, Loss: 2.4167, LR: 0.000048
2025-10-23 13:28:43,839 - INFO - Batch 3600, Loss: 2.5271, LR: 0.000048
2025-10-23 13:28:56,418 - INFO - Batch 3700, Loss: 2.3851, LR: 0.000048
2025-10-23 13:29:09,023 - INFO - Batch 3800, Loss: 2.2591, LR: 0.000048
2025-10-23 13:29:21,581 - INFO - Batch 3900, Loss: 2.1860, LR: 0.000048
2025-10-23 13:29:34,164 - INFO - Batch 4000, Loss: 2.3197, LR: 0.000048
2025-10-23 13:29:46,678 - INFO - Batch 4100, Loss: 2.5493, LR: 0.000047
2025-10-23 13:29:59,188 - INFO - Batch 4200, Loss: 2.5356, LR: 0.000047
2025-10-23 13:30:11,689 - INFO - Batch 4300, Loss: 2.4617, LR: 0.000047
2025-10-23 13:30:24,148 - INFO - Batch 4400, Loss: 2.4787, LR: 0.000047
2025-10-23 13:30:36,634 - INFO - Batch 4500, Loss: 2.5811, LR: 0.000047
2025-10-23 13:30:49,100 - INFO - Batch 4600, Loss: 2.5438, LR: 0.000047
2025-10-23 13:31:01,582 - INFO - Batch 4700, Loss: 2.3468, LR: 0.000047
2025-10-23 13:31:14,062 - INFO - Batch 4800, Loss: 2.8617, LR: 0.000047
2025-10-23 13:31:26,522 - INFO - Batch 4900, Loss: 2.4738, LR: 0.000047
2025-10-23 13:31:38,960 - INFO - Batch 5000, Loss: 2.2410, LR: 0.000047
2025-10-23 13:31:51,403 - INFO - Batch 5100, Loss: 2.1789, LR: 0.000047
2025-10-23 13:32:03,958 - INFO - Batch 5200, Loss: 2.5147, LR: 0.000047
2025-10-23 13:32:16,401 - INFO - Batch 5300, Loss: 2.0340, LR: 0.000046
2025-10-23 13:32:28,976 - INFO - Batch 5400, Loss: 2.7121, LR: 0.000046
2025-10-23 13:32:41,402 - INFO - Batch 5500, Loss: 2.0971, LR: 0.000046
2025-10-23 13:32:53,857 - INFO - Batch 5600, Loss: 2.3182, LR: 0.000046
2025-10-23 13:33:06,295 - INFO - Batch 5700, Loss: 2.2437, LR: 0.000046
2025-10-23 13:33:18,705 - INFO - Batch 5800, Loss: 2.1277, LR: 0.000046
2025-10-23 13:33:31,061 - INFO - Batch 5900, Loss: 2.4910, LR: 0.000046
2025-10-23 13:33:43,538 - INFO - Batch 6000, Loss: 2.3454, LR: 0.000046
2025-10-23 13:33:55,951 - INFO - Batch 6100, Loss: 2.2635, LR: 0.000046
2025-10-23 13:34:08,538 - INFO - Batch 6200, Loss: 2.1430, LR: 0.000046
2025-10-23 13:34:21,066 - INFO - Batch 6300, Loss: 2.4287, LR: 0.000046
2025-10-23 13:34:33,531 - INFO - Batch 6400, Loss: 2.6795, LR: 0.000046
2025-10-23 13:34:39,752 - INFO - Epoch 16/30: Train Loss: 2.3489, Val Loss: 2.2463, LR: 0.000046
2025-10-23 13:34:40,028 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 13:34:40,159 - INFO - Batch 0, Loss: 2.2297, LR: 0.000046
2025-10-23 13:34:52,555 - INFO - Batch 100, Loss: 2.2778, LR: 0.000045
2025-10-23 13:35:04,930 - INFO - Batch 200, Loss: 2.3683, LR: 0.000045
2025-10-23 13:35:17,289 - INFO - Batch 300, Loss: 2.2504, LR: 0.000045
2025-10-23 13:35:29,667 - INFO - Batch 400, Loss: 2.2213, LR: 0.000045
2025-10-23 13:35:42,042 - INFO - Batch 500, Loss: 2.4340, LR: 0.000045
2025-10-23 13:35:54,404 - INFO - Batch 600, Loss: 2.3662, LR: 0.000045
2025-10-23 13:36:06,770 - INFO - Batch 700, Loss: 2.1571, LR: 0.000045
2025-10-23 13:36:19,147 - INFO - Batch 800, Loss: 2.3436, LR: 0.000045
2025-10-23 13:36:31,497 - INFO - Batch 900, Loss: 2.3911, LR: 0.000045
2025-10-23 13:36:43,959 - INFO - Batch 1000, Loss: 2.5703, LR: 0.000045
2025-10-23 13:36:56,386 - INFO - Batch 1100, Loss: 2.4036, LR: 0.000045
2025-10-23 13:37:09,030 - INFO - Batch 1200, Loss: 2.3603, LR: 0.000045
2025-10-23 13:37:21,689 - INFO - Batch 1300, Loss: 2.1608, LR: 0.000044
2025-10-23 13:37:34,189 - INFO - Batch 1400, Loss: 2.4150, LR: 0.000044
2025-10-23 13:37:46,698 - INFO - Batch 1500, Loss: 2.2778, LR: 0.000044
2025-10-23 13:37:59,220 - INFO - Batch 1600, Loss: 2.4070, LR: 0.000044
2025-10-23 13:38:11,696 - INFO - Batch 1700, Loss: 2.1365, LR: 0.000044
2025-10-23 13:38:24,174 - INFO - Batch 1800, Loss: 2.5450, LR: 0.000044
2025-10-23 13:38:36,693 - INFO - Batch 1900, Loss: 2.0891, LR: 0.000044
2025-10-23 13:38:49,193 - INFO - Batch 2000, Loss: 2.6355, LR: 0.000044
2025-10-23 13:39:01,644 - INFO - Batch 2100, Loss: 2.2809, LR: 0.000044
2025-10-23 13:39:14,101 - INFO - Batch 2200, Loss: 2.4945, LR: 0.000044
2025-10-23 13:39:26,563 - INFO - Batch 2300, Loss: 2.3899, LR: 0.000044
2025-10-23 13:39:39,081 - INFO - Batch 2400, Loss: 2.5129, LR: 0.000044
2025-10-23 13:39:51,616 - INFO - Batch 2500, Loss: 2.6980, LR: 0.000043
2025-10-23 13:40:04,177 - INFO - Batch 2600, Loss: 2.4475, LR: 0.000043
2025-10-23 13:40:16,735 - INFO - Batch 2700, Loss: 2.1888, LR: 0.000043
2025-10-23 13:40:29,233 - INFO - Batch 2800, Loss: 2.3949, LR: 0.000043
2025-10-23 13:40:41,703 - INFO - Batch 2900, Loss: 2.5899, LR: 0.000043
2025-10-23 13:40:54,158 - INFO - Batch 3000, Loss: 2.4567, LR: 0.000043
2025-10-23 13:41:06,605 - INFO - Batch 3100, Loss: 2.3232, LR: 0.000043
2025-10-23 13:41:19,058 - INFO - Batch 3200, Loss: 2.1774, LR: 0.000043
2025-10-23 13:41:31,485 - INFO - Batch 3300, Loss: 2.3948, LR: 0.000043
2025-10-23 13:41:43,915 - INFO - Batch 3400, Loss: 2.1714, LR: 0.000043
2025-10-23 13:41:56,301 - INFO - Batch 3500, Loss: 2.5723, LR: 0.000043
2025-10-23 13:42:08,709 - INFO - Batch 3600, Loss: 2.3880, LR: 0.000043
2025-10-23 13:42:21,261 - INFO - Batch 3700, Loss: 2.3640, LR: 0.000043
2025-10-23 13:42:33,699 - INFO - Batch 3800, Loss: 2.5883, LR: 0.000042
2025-10-23 13:42:46,133 - INFO - Batch 3900, Loss: 2.5063, LR: 0.000042
2025-10-23 13:42:58,632 - INFO - Batch 4000, Loss: 2.2853, LR: 0.000042
2025-10-23 13:43:11,094 - INFO - Batch 4100, Loss: 2.3859, LR: 0.000042
2025-10-23 13:43:23,497 - INFO - Batch 4200, Loss: 2.0488, LR: 0.000042
2025-10-23 13:43:35,889 - INFO - Batch 4300, Loss: 2.2838, LR: 0.000042
2025-10-23 13:43:48,237 - INFO - Batch 4400, Loss: 2.5157, LR: 0.000042
2025-10-23 13:44:00,605 - INFO - Batch 4500, Loss: 2.4526, LR: 0.000042
2025-10-23 13:44:12,953 - INFO - Batch 4600, Loss: 2.6271, LR: 0.000042
2025-10-23 13:44:25,316 - INFO - Batch 4700, Loss: 2.5089, LR: 0.000042
2025-10-23 13:44:37,728 - INFO - Batch 4800, Loss: 2.1705, LR: 0.000042
2025-10-23 13:44:50,134 - INFO - Batch 4900, Loss: 2.3219, LR: 0.000042
2025-10-23 13:45:02,521 - INFO - Batch 5000, Loss: 2.2548, LR: 0.000041
2025-10-23 13:45:14,914 - INFO - Batch 5100, Loss: 2.3383, LR: 0.000041
2025-10-23 13:45:27,481 - INFO - Batch 5200, Loss: 2.3224, LR: 0.000041
2025-10-23 13:45:39,945 - INFO - Batch 5300, Loss: 2.3564, LR: 0.000041
2025-10-23 13:45:52,344 - INFO - Batch 5400, Loss: 2.5331, LR: 0.000041
2025-10-23 13:46:04,734 - INFO - Batch 5500, Loss: 2.1612, LR: 0.000041
2025-10-23 13:46:17,186 - INFO - Batch 5600, Loss: 2.3238, LR: 0.000041
2025-10-23 13:46:29,646 - INFO - Batch 5700, Loss: 2.1939, LR: 0.000041
2025-10-23 13:46:42,057 - INFO - Batch 5800, Loss: 2.5936, LR: 0.000041
2025-10-23 13:46:54,439 - INFO - Batch 5900, Loss: 2.3881, LR: 0.000041
2025-10-23 13:47:06,890 - INFO - Batch 6000, Loss: 2.2653, LR: 0.000041
2025-10-23 13:47:19,358 - INFO - Batch 6100, Loss: 2.1693, LR: 0.000041
2025-10-23 13:47:31,730 - INFO - Batch 6200, Loss: 2.6238, LR: 0.000040
2025-10-23 13:47:44,087 - INFO - Batch 6300, Loss: 2.3744, LR: 0.000040
2025-10-23 13:47:56,441 - INFO - Batch 6400, Loss: 2.2684, LR: 0.000040
2025-10-23 13:48:02,607 - INFO - Epoch 17/30: Train Loss: 2.3319, Val Loss: 2.2305, LR: 0.000040
2025-10-23 13:48:02,971 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 13:48:03,102 - INFO - Batch 0, Loss: 2.4208, LR: 0.000040
2025-10-23 13:48:15,545 - INFO - Batch 100, Loss: 2.5041, LR: 0.000040
2025-10-23 13:48:27,934 - INFO - Batch 200, Loss: 2.5778, LR: 0.000040
2025-10-23 13:48:40,329 - INFO - Batch 300, Loss: 2.2946, LR: 0.000040
2025-10-23 13:48:52,738 - INFO - Batch 400, Loss: 2.2401, LR: 0.000040
2025-10-23 13:49:05,149 - INFO - Batch 500, Loss: 2.3114, LR: 0.000040
2025-10-23 13:49:17,589 - INFO - Batch 600, Loss: 2.5440, LR: 0.000040
2025-10-23 13:49:30,055 - INFO - Batch 700, Loss: 2.5164, LR: 0.000040
2025-10-23 13:49:42,583 - INFO - Batch 800, Loss: 2.2020, LR: 0.000040
2025-10-23 13:49:55,032 - INFO - Batch 900, Loss: 2.1066, LR: 0.000040
2025-10-23 13:50:07,555 - INFO - Batch 1000, Loss: 2.4697, LR: 0.000039
2025-10-23 13:50:20,082 - INFO - Batch 1100, Loss: 2.5499, LR: 0.000039
2025-10-23 13:50:32,589 - INFO - Batch 1200, Loss: 2.2288, LR: 0.000039
2025-10-23 13:50:45,100 - INFO - Batch 1300, Loss: 2.2922, LR: 0.000039
2025-10-23 13:50:57,609 - INFO - Batch 1400, Loss: 2.4181, LR: 0.000039
2025-10-23 13:51:10,121 - INFO - Batch 1500, Loss: 2.4120, LR: 0.000039
2025-10-23 13:51:22,649 - INFO - Batch 1600, Loss: 2.2847, LR: 0.000039
2025-10-23 13:51:35,166 - INFO - Batch 1700, Loss: 2.3057, LR: 0.000039
2025-10-23 13:51:47,655 - INFO - Batch 1800, Loss: 2.4159, LR: 0.000039
2025-10-23 13:52:00,174 - INFO - Batch 1900, Loss: 2.3739, LR: 0.000039
2025-10-23 13:52:12,848 - INFO - Batch 2000, Loss: 2.5169, LR: 0.000039
2025-10-23 13:52:25,461 - INFO - Batch 2100, Loss: 2.2214, LR: 0.000039
2025-10-23 13:52:38,069 - INFO - Batch 2200, Loss: 2.3429, LR: 0.000039
2025-10-23 13:52:50,595 - INFO - Batch 2300, Loss: 2.1596, LR: 0.000038
2025-10-23 13:53:03,092 - INFO - Batch 2400, Loss: 2.2438, LR: 0.000038
2025-10-23 13:53:15,590 - INFO - Batch 2500, Loss: 2.3926, LR: 0.000038
2025-10-23 13:53:28,124 - INFO - Batch 2600, Loss: 2.3878, LR: 0.000038
2025-10-23 13:53:40,745 - INFO - Batch 2700, Loss: 2.3115, LR: 0.000038
2025-10-23 13:53:53,425 - INFO - Batch 2800, Loss: 2.2619, LR: 0.000038
2025-10-23 13:54:05,904 - INFO - Batch 2900, Loss: 2.5816, LR: 0.000038
2025-10-23 13:54:18,409 - INFO - Batch 3000, Loss: 2.3440, LR: 0.000038
2025-10-23 13:54:30,910 - INFO - Batch 3100, Loss: 2.4227, LR: 0.000038
2025-10-23 13:54:43,390 - INFO - Batch 3200, Loss: 2.0055, LR: 0.000038
2025-10-23 13:54:55,868 - INFO - Batch 3300, Loss: 2.6618, LR: 0.000038
2025-10-23 13:55:08,334 - INFO - Batch 3400, Loss: 2.3551, LR: 0.000038
2025-10-23 13:55:20,813 - INFO - Batch 3500, Loss: 2.3877, LR: 0.000037
2025-10-23 13:55:33,277 - INFO - Batch 3600, Loss: 2.4549, LR: 0.000037
2025-10-23 13:55:45,762 - INFO - Batch 3700, Loss: 2.4379, LR: 0.000037
2025-10-23 13:55:58,245 - INFO - Batch 3800, Loss: 2.2246, LR: 0.000037
2025-10-23 13:56:10,805 - INFO - Batch 3900, Loss: 2.2390, LR: 0.000037
2025-10-23 13:56:23,404 - INFO - Batch 4000, Loss: 2.1761, LR: 0.000037
2025-10-23 13:56:36,026 - INFO - Batch 4100, Loss: 2.2570, LR: 0.000037
2025-10-23 13:56:48,593 - INFO - Batch 4200, Loss: 2.3071, LR: 0.000037
2025-10-23 13:57:01,221 - INFO - Batch 4300, Loss: 2.3449, LR: 0.000037
2025-10-23 13:57:13,864 - INFO - Batch 4400, Loss: 2.3383, LR: 0.000037
2025-10-23 13:57:26,513 - INFO - Batch 4500, Loss: 2.3781, LR: 0.000037
2025-10-23 13:57:39,204 - INFO - Batch 4600, Loss: 2.3531, LR: 0.000037
2025-10-23 13:57:51,808 - INFO - Batch 4700, Loss: 2.2832, LR: 0.000037
2025-10-23 13:58:04,386 - INFO - Batch 4800, Loss: 2.4278, LR: 0.000036
2025-10-23 13:58:16,845 - INFO - Batch 4900, Loss: 2.4828, LR: 0.000036
2025-10-23 13:58:29,493 - INFO - Batch 5000, Loss: 2.3553, LR: 0.000036
2025-10-23 13:58:42,085 - INFO - Batch 5100, Loss: 2.1585, LR: 0.000036
2025-10-23 13:58:54,562 - INFO - Batch 5200, Loss: 2.3302, LR: 0.000036
2025-10-23 13:59:06,977 - INFO - Batch 5300, Loss: 2.2755, LR: 0.000036
2025-10-23 13:59:19,401 - INFO - Batch 5400, Loss: 2.2075, LR: 0.000036
2025-10-23 13:59:31,810 - INFO - Batch 5500, Loss: 2.1678, LR: 0.000036
2025-10-23 13:59:44,297 - INFO - Batch 5600, Loss: 2.3404, LR: 0.000036
2025-10-23 13:59:56,849 - INFO - Batch 5700, Loss: 2.3560, LR: 0.000036
2025-10-23 14:00:09,278 - INFO - Batch 5800, Loss: 2.3149, LR: 0.000036
2025-10-23 14:00:21,744 - INFO - Batch 5900, Loss: 1.9731, LR: 0.000036
2025-10-23 14:00:34,241 - INFO - Batch 6000, Loss: 2.4129, LR: 0.000036
2025-10-23 14:00:46,697 - INFO - Batch 6100, Loss: 2.4217, LR: 0.000035
2025-10-23 14:00:59,219 - INFO - Batch 6200, Loss: 2.4563, LR: 0.000035
2025-10-23 14:01:11,662 - INFO - Batch 6300, Loss: 2.3570, LR: 0.000035
2025-10-23 14:01:24,135 - INFO - Batch 6400, Loss: 2.4219, LR: 0.000035
2025-10-23 14:01:30,380 - INFO - Epoch 18/30: Train Loss: 2.3164, Val Loss: 2.2242, LR: 0.000035
2025-10-23 14:01:30,656 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 14:01:30,793 - INFO - Batch 0, Loss: 2.3145, LR: 0.000035
2025-10-23 14:01:43,306 - INFO - Batch 100, Loss: 2.4678, LR: 0.000035
2025-10-23 14:01:55,809 - INFO - Batch 200, Loss: 2.3265, LR: 0.000035
2025-10-23 14:02:08,374 - INFO - Batch 300, Loss: 2.2651, LR: 0.000035
2025-10-23 14:02:20,917 - INFO - Batch 400, Loss: 2.2979, LR: 0.000035
2025-10-23 14:02:33,438 - INFO - Batch 500, Loss: 2.3716, LR: 0.000035
2025-10-23 14:02:46,033 - INFO - Batch 600, Loss: 2.3295, LR: 0.000035
2025-10-23 14:02:58,596 - INFO - Batch 700, Loss: 2.0116, LR: 0.000035
2025-10-23 14:03:11,207 - INFO - Batch 800, Loss: 2.2738, LR: 0.000035
2025-10-23 14:03:23,839 - INFO - Batch 900, Loss: 2.3879, LR: 0.000034
2025-10-23 14:03:36,468 - INFO - Batch 1000, Loss: 2.1078, LR: 0.000034
2025-10-23 14:03:49,071 - INFO - Batch 1100, Loss: 2.3907, LR: 0.000034
2025-10-23 14:04:01,641 - INFO - Batch 1200, Loss: 2.0586, LR: 0.000034
2025-10-23 14:04:14,347 - INFO - Batch 1300, Loss: 2.1178, LR: 0.000034
2025-10-23 14:04:27,023 - INFO - Batch 1400, Loss: 2.1235, LR: 0.000034
2025-10-23 14:04:39,709 - INFO - Batch 1500, Loss: 2.3993, LR: 0.000034
2025-10-23 14:04:52,543 - INFO - Batch 1600, Loss: 2.4967, LR: 0.000034
2025-10-23 14:05:05,270 - INFO - Batch 1700, Loss: 2.1616, LR: 0.000034
2025-10-23 14:05:17,994 - INFO - Batch 1800, Loss: 2.1420, LR: 0.000034
2025-10-23 14:05:30,656 - INFO - Batch 1900, Loss: 2.1655, LR: 0.000034
2025-10-23 14:05:43,329 - INFO - Batch 2000, Loss: 2.0994, LR: 0.000034
2025-10-23 14:05:56,025 - INFO - Batch 2100, Loss: 2.2667, LR: 0.000034
2025-10-23 14:06:08,711 - INFO - Batch 2200, Loss: 2.0946, LR: 0.000033
2025-10-23 14:06:21,448 - INFO - Batch 2300, Loss: 2.3102, LR: 0.000033
2025-10-23 14:06:34,044 - INFO - Batch 2400, Loss: 2.2324, LR: 0.000033
2025-10-23 14:06:46,661 - INFO - Batch 2500, Loss: 2.1639, LR: 0.000033
2025-10-23 14:06:59,265 - INFO - Batch 2600, Loss: 2.2064, LR: 0.000033
2025-10-23 14:07:11,857 - INFO - Batch 2700, Loss: 2.4546, LR: 0.000033
2025-10-23 14:07:24,538 - INFO - Batch 2800, Loss: 1.9641, LR: 0.000033
2025-10-23 14:07:37,205 - INFO - Batch 2900, Loss: 2.2885, LR: 0.000033
2025-10-23 14:07:49,737 - INFO - Batch 3000, Loss: 2.2252, LR: 0.000033
2025-10-23 14:08:02,299 - INFO - Batch 3100, Loss: 2.4576, LR: 0.000033
2025-10-23 14:08:14,827 - INFO - Batch 3200, Loss: 2.5004, LR: 0.000033
2025-10-23 14:08:27,335 - INFO - Batch 3300, Loss: 2.3863, LR: 0.000033
2025-10-23 14:08:39,881 - INFO - Batch 3400, Loss: 2.3302, LR: 0.000033
2025-10-23 14:08:52,417 - INFO - Batch 3500, Loss: 2.2000, LR: 0.000032
2025-10-23 14:09:04,968 - INFO - Batch 3600, Loss: 2.2288, LR: 0.000032
2025-10-23 14:09:17,522 - INFO - Batch 3700, Loss: 2.1980, LR: 0.000032
2025-10-23 14:09:30,248 - INFO - Batch 3800, Loss: 2.1927, LR: 0.000032
2025-10-23 14:09:42,941 - INFO - Batch 3900, Loss: 2.2346, LR: 0.000032
2025-10-23 14:09:55,660 - INFO - Batch 4000, Loss: 2.1622, LR: 0.000032
2025-10-23 14:10:08,366 - INFO - Batch 4100, Loss: 2.3137, LR: 0.000032
2025-10-23 14:10:20,914 - INFO - Batch 4200, Loss: 2.3309, LR: 0.000032
2025-10-23 14:10:33,428 - INFO - Batch 4300, Loss: 2.5989, LR: 0.000032
2025-10-23 14:10:45,953 - INFO - Batch 4400, Loss: 2.3507, LR: 0.000032
2025-10-23 14:10:58,433 - INFO - Batch 4500, Loss: 2.3632, LR: 0.000032
2025-10-23 14:11:10,922 - INFO - Batch 4600, Loss: 2.2362, LR: 0.000032
2025-10-23 14:11:23,410 - INFO - Batch 4700, Loss: 2.1367, LR: 0.000032
2025-10-23 14:11:35,964 - INFO - Batch 4800, Loss: 2.2780, LR: 0.000031
2025-10-23 14:11:48,486 - INFO - Batch 4900, Loss: 2.2130, LR: 0.000031
2025-10-23 14:12:01,018 - INFO - Batch 5000, Loss: 2.2435, LR: 0.000031
2025-10-23 14:12:13,533 - INFO - Batch 5100, Loss: 2.1881, LR: 0.000031
2025-10-23 14:12:26,093 - INFO - Batch 5200, Loss: 2.3463, LR: 0.000031
2025-10-23 14:12:38,555 - INFO - Batch 5300, Loss: 2.2853, LR: 0.000031
2025-10-23 14:12:51,051 - INFO - Batch 5400, Loss: 2.5501, LR: 0.000031
2025-10-23 14:13:03,515 - INFO - Batch 5500, Loss: 1.8393, LR: 0.000031
2025-10-23 14:13:15,973 - INFO - Batch 5600, Loss: 2.2261, LR: 0.000031
2025-10-23 14:13:28,445 - INFO - Batch 5700, Loss: 2.3393, LR: 0.000031
2025-10-23 14:13:40,910 - INFO - Batch 5800, Loss: 2.1810, LR: 0.000031
2025-10-23 14:13:53,498 - INFO - Batch 5900, Loss: 2.1630, LR: 0.000031
2025-10-23 14:14:05,975 - INFO - Batch 6000, Loss: 2.1057, LR: 0.000031
2025-10-23 14:14:18,538 - INFO - Batch 6100, Loss: 2.0492, LR: 0.000030
2025-10-23 14:14:31,104 - INFO - Batch 6200, Loss: 2.6097, LR: 0.000030
2025-10-23 14:14:43,696 - INFO - Batch 6300, Loss: 2.3493, LR: 0.000030
2025-10-23 14:14:56,276 - INFO - Batch 6400, Loss: 2.3325, LR: 0.000030
2025-10-23 14:15:02,554 - INFO - Epoch 19/30: Train Loss: 2.3013, Val Loss: 2.2106, LR: 0.000030
2025-10-23 14:15:02,832 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 14:15:02,965 - INFO - Batch 0, Loss: 2.3517, LR: 0.000030
2025-10-23 14:15:15,598 - INFO - Batch 100, Loss: 2.2054, LR: 0.000030
2025-10-23 14:15:28,221 - INFO - Batch 200, Loss: 2.2103, LR: 0.000030
2025-10-23 14:15:40,923 - INFO - Batch 300, Loss: 2.2756, LR: 0.000030
2025-10-23 14:15:53,660 - INFO - Batch 400, Loss: 2.3504, LR: 0.000030
2025-10-23 14:16:06,239 - INFO - Batch 500, Loss: 2.0440, LR: 0.000030
2025-10-23 14:16:18,926 - INFO - Batch 600, Loss: 2.2325, LR: 0.000030
2025-10-23 14:16:31,708 - INFO - Batch 700, Loss: 2.2787, LR: 0.000030
2025-10-23 14:16:44,466 - INFO - Batch 800, Loss: 2.3808, LR: 0.000030
2025-10-23 14:16:57,263 - INFO - Batch 900, Loss: 2.3333, LR: 0.000030
2025-10-23 14:17:10,110 - INFO - Batch 1000, Loss: 2.1820, LR: 0.000029
2025-10-23 14:17:22,928 - INFO - Batch 1100, Loss: 2.5716, LR: 0.000029
2025-10-23 14:17:35,623 - INFO - Batch 1200, Loss: 2.2727, LR: 0.000029
2025-10-23 14:17:48,303 - INFO - Batch 1300, Loss: 2.2516, LR: 0.000029
2025-10-23 14:18:01,129 - INFO - Batch 1400, Loss: 2.1258, LR: 0.000029
2025-10-23 14:18:13,917 - INFO - Batch 1500, Loss: 2.1315, LR: 0.000029
2025-10-23 14:18:26,728 - INFO - Batch 1600, Loss: 2.1777, LR: 0.000029
2025-10-23 14:18:39,437 - INFO - Batch 1700, Loss: 2.4408, LR: 0.000029
2025-10-23 14:18:52,270 - INFO - Batch 1800, Loss: 2.1630, LR: 0.000029
2025-10-23 14:19:04,998 - INFO - Batch 1900, Loss: 2.2543, LR: 0.000029
2025-10-23 14:19:17,741 - INFO - Batch 2000, Loss: 2.1447, LR: 0.000029
2025-10-23 14:19:30,488 - INFO - Batch 2100, Loss: 2.2942, LR: 0.000029
2025-10-23 14:19:43,191 - INFO - Batch 2200, Loss: 2.4185, LR: 0.000029
2025-10-23 14:19:55,834 - INFO - Batch 2300, Loss: 2.2351, LR: 0.000028
2025-10-23 14:20:08,574 - INFO - Batch 2400, Loss: 2.2076, LR: 0.000028
2025-10-23 14:20:21,297 - INFO - Batch 2500, Loss: 2.3840, LR: 0.000028
2025-10-23 14:20:33,921 - INFO - Batch 2600, Loss: 2.5106, LR: 0.000028
2025-10-23 14:20:46,558 - INFO - Batch 2700, Loss: 2.7100, LR: 0.000028
2025-10-23 14:20:59,218 - INFO - Batch 2800, Loss: 2.3909, LR: 0.000028
2025-10-23 14:21:11,849 - INFO - Batch 2900, Loss: 2.0385, LR: 0.000028
2025-10-23 14:21:24,610 - INFO - Batch 3000, Loss: 2.4317, LR: 0.000028
2025-10-23 14:21:37,362 - INFO - Batch 3100, Loss: 2.3802, LR: 0.000028
2025-10-23 14:21:50,117 - INFO - Batch 3200, Loss: 2.2824, LR: 0.000028
2025-10-23 14:22:02,882 - INFO - Batch 3300, Loss: 2.5130, LR: 0.000028
2025-10-23 14:22:15,486 - INFO - Batch 3400, Loss: 2.3291, LR: 0.000028
2025-10-23 14:22:28,166 - INFO - Batch 3500, Loss: 2.5178, LR: 0.000028
2025-10-23 14:22:40,750 - INFO - Batch 3600, Loss: 2.5687, LR: 0.000028
2025-10-23 14:22:53,318 - INFO - Batch 3700, Loss: 2.0695, LR: 0.000027
2025-10-23 14:23:05,904 - INFO - Batch 3800, Loss: 2.1255, LR: 0.000027
2025-10-23 14:23:18,637 - INFO - Batch 3900, Loss: 2.2460, LR: 0.000027
2025-10-23 14:23:31,357 - INFO - Batch 4000, Loss: 2.3014, LR: 0.000027
2025-10-23 14:23:44,104 - INFO - Batch 4100, Loss: 2.3320, LR: 0.000027
2025-10-23 14:23:56,798 - INFO - Batch 4200, Loss: 2.3122, LR: 0.000027
2025-10-23 14:24:09,453 - INFO - Batch 4300, Loss: 2.5127, LR: 0.000027
2025-10-23 14:24:22,147 - INFO - Batch 4400, Loss: 2.5078, LR: 0.000027
2025-10-23 14:24:34,789 - INFO - Batch 4500, Loss: 2.2520, LR: 0.000027
2025-10-23 14:24:47,435 - INFO - Batch 4600, Loss: 2.5411, LR: 0.000027
2025-10-23 14:25:00,103 - INFO - Batch 4700, Loss: 2.4899, LR: 0.000027
2025-10-23 14:25:12,747 - INFO - Batch 4800, Loss: 2.0483, LR: 0.000027
2025-10-23 14:25:25,345 - INFO - Batch 4900, Loss: 2.2381, LR: 0.000027
2025-10-23 14:25:37,890 - INFO - Batch 5000, Loss: 2.2793, LR: 0.000027
2025-10-23 14:25:50,441 - INFO - Batch 5100, Loss: 2.2059, LR: 0.000026
2025-10-23 14:26:03,098 - INFO - Batch 5200, Loss: 2.2883, LR: 0.000026
2025-10-23 14:26:15,807 - INFO - Batch 5300, Loss: 2.1575, LR: 0.000026
2025-10-23 14:26:28,494 - INFO - Batch 5400, Loss: 2.2814, LR: 0.000026
2025-10-23 14:26:41,123 - INFO - Batch 5500, Loss: 2.4293, LR: 0.000026
2025-10-23 14:26:53,694 - INFO - Batch 5600, Loss: 2.1655, LR: 0.000026
2025-10-23 14:27:06,293 - INFO - Batch 5700, Loss: 2.2079, LR: 0.000026
2025-10-23 14:27:19,038 - INFO - Batch 5800, Loss: 2.2883, LR: 0.000026
2025-10-23 14:27:31,662 - INFO - Batch 5900, Loss: 2.3608, LR: 0.000026
2025-10-23 14:27:44,188 - INFO - Batch 6000, Loss: 2.3017, LR: 0.000026
2025-10-23 14:27:56,800 - INFO - Batch 6100, Loss: 2.0048, LR: 0.000026
2025-10-23 14:28:09,392 - INFO - Batch 6200, Loss: 2.4348, LR: 0.000026
2025-10-23 14:28:22,000 - INFO - Batch 6300, Loss: 2.2987, LR: 0.000026
2025-10-23 14:28:34,609 - INFO - Batch 6400, Loss: 2.3149, LR: 0.000026
2025-10-23 14:28:40,873 - INFO - Epoch 20/30: Train Loss: 2.2884, Val Loss: 2.1964, LR: 0.000025
2025-10-23 14:28:41,142 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 14:28:41,349 - INFO - 模型已保存到: ./checkpoints/checkpoint_epoch_20.pth
2025-10-23 14:28:41,479 - INFO - Batch 0, Loss: 2.1600, LR: 0.000025
2025-10-23 14:28:54,051 - INFO - Batch 100, Loss: 2.2338, LR: 0.000025
2025-10-23 14:29:06,629 - INFO - Batch 200, Loss: 2.3268, LR: 0.000025
2025-10-23 14:29:19,290 - INFO - Batch 300, Loss: 2.0668, LR: 0.000025
2025-10-23 14:29:32,000 - INFO - Batch 400, Loss: 2.3599, LR: 0.000025
2025-10-23 14:29:44,713 - INFO - Batch 500, Loss: 2.4106, LR: 0.000025
2025-10-23 14:29:57,433 - INFO - Batch 600, Loss: 2.2488, LR: 0.000025
2025-10-23 14:30:10,157 - INFO - Batch 700, Loss: 2.3696, LR: 0.000025
2025-10-23 14:30:22,863 - INFO - Batch 800, Loss: 2.2922, LR: 0.000025
2025-10-23 14:30:35,484 - INFO - Batch 900, Loss: 2.3259, LR: 0.000025
2025-10-23 14:30:48,113 - INFO - Batch 1000, Loss: 2.2266, LR: 0.000025
2025-10-23 14:31:00,727 - INFO - Batch 1100, Loss: 2.1930, LR: 0.000025
2025-10-23 14:31:13,324 - INFO - Batch 1200, Loss: 2.2850, LR: 0.000025
2025-10-23 14:31:26,105 - INFO - Batch 1300, Loss: 2.2979, LR: 0.000025
2025-10-23 14:31:38,885 - INFO - Batch 1400, Loss: 2.0080, LR: 0.000024
2025-10-23 14:31:51,635 - INFO - Batch 1500, Loss: 2.1402, LR: 0.000024
2025-10-23 14:32:04,354 - INFO - Batch 1600, Loss: 2.0728, LR: 0.000024
2025-10-23 14:32:17,080 - INFO - Batch 1700, Loss: 2.0809, LR: 0.000024
2025-10-23 14:32:29,893 - INFO - Batch 1800, Loss: 2.4075, LR: 0.000024
2025-10-23 14:32:42,735 - INFO - Batch 1900, Loss: 2.1327, LR: 0.000024
2025-10-23 14:32:55,516 - INFO - Batch 2000, Loss: 2.1997, LR: 0.000024
2025-10-23 14:33:08,334 - INFO - Batch 2100, Loss: 2.4137, LR: 0.000024
2025-10-23 14:33:21,069 - INFO - Batch 2200, Loss: 2.1902, LR: 0.000024
2025-10-23 14:33:33,894 - INFO - Batch 2300, Loss: 2.2738, LR: 0.000024
2025-10-23 14:33:46,806 - INFO - Batch 2400, Loss: 1.9863, LR: 0.000024
2025-10-23 14:33:59,523 - INFO - Batch 2500, Loss: 2.1635, LR: 0.000024
2025-10-23 14:34:12,232 - INFO - Batch 2600, Loss: 2.2476, LR: 0.000024
2025-10-23 14:34:25,002 - INFO - Batch 2700, Loss: 2.0480, LR: 0.000024
2025-10-23 14:34:37,743 - INFO - Batch 2800, Loss: 2.3327, LR: 0.000023
2025-10-23 14:34:50,414 - INFO - Batch 2900, Loss: 2.3698, LR: 0.000023
2025-10-23 14:35:03,126 - INFO - Batch 3000, Loss: 2.1449, LR: 0.000023
2025-10-23 14:35:15,848 - INFO - Batch 3100, Loss: 2.3776, LR: 0.000023
2025-10-23 14:35:28,585 - INFO - Batch 3200, Loss: 2.1276, LR: 0.000023
2025-10-23 14:35:41,257 - INFO - Batch 3300, Loss: 2.4103, LR: 0.000023
2025-10-23 14:35:53,994 - INFO - Batch 3400, Loss: 2.4004, LR: 0.000023
2025-10-23 14:36:06,824 - INFO - Batch 3500, Loss: 2.3777, LR: 0.000023
2025-10-23 14:36:19,496 - INFO - Batch 3600, Loss: 2.3434, LR: 0.000023
2025-10-23 14:36:32,187 - INFO - Batch 3700, Loss: 2.1197, LR: 0.000023
2025-10-23 14:36:44,845 - INFO - Batch 3800, Loss: 2.4471, LR: 0.000023
2025-10-23 14:36:57,609 - INFO - Batch 3900, Loss: 2.1436, LR: 0.000023
2025-10-23 14:37:10,324 - INFO - Batch 4000, Loss: 2.2526, LR: 0.000023
2025-10-23 14:37:23,071 - INFO - Batch 4100, Loss: 2.3539, LR: 0.000023
2025-10-23 14:37:35,902 - INFO - Batch 4200, Loss: 2.0691, LR: 0.000023
2025-10-23 14:37:48,712 - INFO - Batch 4300, Loss: 2.2688, LR: 0.000022
2025-10-23 14:38:01,445 - INFO - Batch 4400, Loss: 2.1035, LR: 0.000022
2025-10-23 14:38:14,112 - INFO - Batch 4500, Loss: 2.1767, LR: 0.000022
2025-10-23 14:38:26,746 - INFO - Batch 4600, Loss: 2.1383, LR: 0.000022
2025-10-23 14:38:39,392 - INFO - Batch 4700, Loss: 2.3149, LR: 0.000022
2025-10-23 14:38:52,183 - INFO - Batch 4800, Loss: 2.0096, LR: 0.000022
2025-10-23 14:39:04,735 - INFO - Batch 4900, Loss: 2.0139, LR: 0.000022
2025-10-23 14:39:17,164 - INFO - Batch 5000, Loss: 2.1889, LR: 0.000022
2025-10-23 14:39:29,594 - INFO - Batch 5100, Loss: 2.3154, LR: 0.000022
2025-10-23 14:39:42,044 - INFO - Batch 5200, Loss: 2.2445, LR: 0.000022
2025-10-23 14:39:54,575 - INFO - Batch 5300, Loss: 2.3530, LR: 0.000022
2025-10-23 14:40:07,040 - INFO - Batch 5400, Loss: 2.3760, LR: 0.000022
2025-10-23 14:40:19,498 - INFO - Batch 5500, Loss: 2.3466, LR: 0.000022
2025-10-23 14:40:31,953 - INFO - Batch 5600, Loss: 2.1129, LR: 0.000022
2025-10-23 14:40:44,515 - INFO - Batch 5700, Loss: 2.2985, LR: 0.000022
2025-10-23 14:40:57,006 - INFO - Batch 5800, Loss: 2.2697, LR: 0.000021
2025-10-23 14:41:09,554 - INFO - Batch 5900, Loss: 2.3935, LR: 0.000021
2025-10-23 14:41:22,123 - INFO - Batch 6000, Loss: 2.3559, LR: 0.000021
2025-10-23 14:41:34,715 - INFO - Batch 6100, Loss: 2.1945, LR: 0.000021
2025-10-23 14:41:47,306 - INFO - Batch 6200, Loss: 2.1535, LR: 0.000021
2025-10-23 14:41:59,882 - INFO - Batch 6300, Loss: 2.6446, LR: 0.000021
2025-10-23 14:42:12,449 - INFO - Batch 6400, Loss: 2.4477, LR: 0.000021
2025-10-23 14:42:18,777 - INFO - Epoch 21/30: Train Loss: 2.2756, Val Loss: 2.1894, LR: 0.000021
2025-10-23 14:42:19,076 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 14:42:19,219 - INFO - Batch 0, Loss: 2.3501, LR: 0.000021
2025-10-23 14:42:31,691 - INFO - Batch 100, Loss: 2.4608, LR: 0.000021
2025-10-23 14:42:44,120 - INFO - Batch 200, Loss: 2.2286, LR: 0.000021
2025-10-23 14:42:56,595 - INFO - Batch 300, Loss: 2.1727, LR: 0.000021
2025-10-23 14:43:09,001 - INFO - Batch 400, Loss: 2.2329, LR: 0.000021
2025-10-23 14:43:21,362 - INFO - Batch 500, Loss: 2.4664, LR: 0.000021
2025-10-23 14:43:33,701 - INFO - Batch 600, Loss: 2.1597, LR: 0.000021
2025-10-23 14:43:46,152 - INFO - Batch 700, Loss: 2.1850, LR: 0.000021
2025-10-23 14:43:58,637 - INFO - Batch 800, Loss: 2.2489, LR: 0.000020
2025-10-23 14:44:11,158 - INFO - Batch 900, Loss: 2.2140, LR: 0.000020
2025-10-23 14:44:23,619 - INFO - Batch 1000, Loss: 2.3596, LR: 0.000020
2025-10-23 14:44:36,017 - INFO - Batch 1100, Loss: 2.3774, LR: 0.000020
2025-10-23 14:44:48,394 - INFO - Batch 1200, Loss: 2.2626, LR: 0.000020
2025-10-23 14:45:00,720 - INFO - Batch 1300, Loss: 2.2455, LR: 0.000020
2025-10-23 14:45:13,114 - INFO - Batch 1400, Loss: 2.2460, LR: 0.000020
2025-10-23 14:45:25,483 - INFO - Batch 1500, Loss: 2.3148, LR: 0.000020
2025-10-23 14:45:37,857 - INFO - Batch 1600, Loss: 2.0202, LR: 0.000020
2025-10-23 14:45:50,341 - INFO - Batch 1700, Loss: 2.2024, LR: 0.000020
2025-10-23 14:46:02,854 - INFO - Batch 1800, Loss: 2.3421, LR: 0.000020
2025-10-23 14:46:15,321 - INFO - Batch 1900, Loss: 2.0160, LR: 0.000020
2025-10-23 14:46:27,711 - INFO - Batch 2000, Loss: 2.3005, LR: 0.000020
2025-10-23 14:46:40,091 - INFO - Batch 2100, Loss: 2.2631, LR: 0.000020
2025-10-23 14:46:52,105 - INFO - Batch 2200, Loss: 1.9131, LR: 0.000020
2025-10-23 14:47:04,124 - INFO - Batch 2300, Loss: 2.3610, LR: 0.000019
2025-10-23 14:47:16,183 - INFO - Batch 2400, Loss: 2.0554, LR: 0.000019
2025-10-23 14:47:28,653 - INFO - Batch 2500, Loss: 2.4940, LR: 0.000019
2025-10-23 14:47:41,117 - INFO - Batch 2600, Loss: 2.1692, LR: 0.000019
2025-10-23 14:47:53,686 - INFO - Batch 2700, Loss: 2.2881, LR: 0.000019
2025-10-23 14:48:06,069 - INFO - Batch 2800, Loss: 2.3859, LR: 0.000019
2025-10-23 14:48:18,443 - INFO - Batch 2900, Loss: 2.2409, LR: 0.000019
2025-10-23 14:48:31,038 - INFO - Batch 3000, Loss: 2.4556, LR: 0.000019
2025-10-23 14:48:43,539 - INFO - Batch 3100, Loss: 2.5306, LR: 0.000019
2025-10-23 14:48:55,993 - INFO - Batch 3200, Loss: 2.2210, LR: 0.000019
2025-10-23 14:49:08,317 - INFO - Batch 3300, Loss: 2.5151, LR: 0.000019
2025-10-23 14:49:20,886 - INFO - Batch 3400, Loss: 2.3130, LR: 0.000019
2025-10-23 14:49:33,635 - INFO - Batch 3500, Loss: 2.2852, LR: 0.000019
2025-10-23 14:49:46,338 - INFO - Batch 3600, Loss: 2.1898, LR: 0.000019
2025-10-23 14:49:59,033 - INFO - Batch 3700, Loss: 2.0490, LR: 0.000019
2025-10-23 14:50:11,635 - INFO - Batch 3800, Loss: 2.3922, LR: 0.000019
2025-10-23 14:50:24,216 - INFO - Batch 3900, Loss: 2.2386, LR: 0.000018
2025-10-23 14:50:36,778 - INFO - Batch 4000, Loss: 2.4213, LR: 0.000018
2025-10-23 14:50:49,265 - INFO - Batch 4100, Loss: 2.2712, LR: 0.000018
2025-10-23 14:51:01,804 - INFO - Batch 4200, Loss: 2.4225, LR: 0.000018
2025-10-23 14:51:14,414 - INFO - Batch 4300, Loss: 2.4570, LR: 0.000018
2025-10-23 14:51:26,917 - INFO - Batch 4400, Loss: 2.0708, LR: 0.000018
2025-10-23 14:51:39,382 - INFO - Batch 4500, Loss: 2.1554, LR: 0.000018
2025-10-23 14:51:51,898 - INFO - Batch 4600, Loss: 2.2933, LR: 0.000018
2025-10-23 14:52:04,548 - INFO - Batch 4700, Loss: 2.2261, LR: 0.000018
2025-10-23 14:52:17,238 - INFO - Batch 4800, Loss: 2.3255, LR: 0.000018
2025-10-23 14:52:29,826 - INFO - Batch 4900, Loss: 2.1690, LR: 0.000018
2025-10-23 14:52:42,318 - INFO - Batch 5000, Loss: 2.4482, LR: 0.000018
2025-10-23 14:52:54,919 - INFO - Batch 5100, Loss: 2.5167, LR: 0.000018
2025-10-23 14:53:07,601 - INFO - Batch 5200, Loss: 2.2450, LR: 0.000018
2025-10-23 14:53:20,213 - INFO - Batch 5300, Loss: 2.0727, LR: 0.000018
2025-10-23 14:53:32,806 - INFO - Batch 5400, Loss: 2.1756, LR: 0.000018
2025-10-23 14:53:45,341 - INFO - Batch 5500, Loss: 2.5654, LR: 0.000017
2025-10-23 14:53:57,954 - INFO - Batch 5600, Loss: 2.1146, LR: 0.000017
2025-10-23 14:54:10,533 - INFO - Batch 5700, Loss: 2.2342, LR: 0.000017
2025-10-23 14:54:23,184 - INFO - Batch 5800, Loss: 2.2586, LR: 0.000017
2025-10-23 14:54:35,842 - INFO - Batch 5900, Loss: 2.0010, LR: 0.000017
2025-10-23 14:54:48,377 - INFO - Batch 6000, Loss: 2.2441, LR: 0.000017
2025-10-23 14:55:00,942 - INFO - Batch 6100, Loss: 2.3320, LR: 0.000017
2025-10-23 14:55:13,556 - INFO - Batch 6200, Loss: 2.4095, LR: 0.000017
2025-10-23 14:55:27,316 - INFO - Batch 6300, Loss: 2.2953, LR: 0.000017
2025-10-23 14:55:41,778 - INFO - Batch 6400, Loss: 2.0585, LR: 0.000017
2025-10-23 14:55:48,995 - INFO - Epoch 22/30: Train Loss: 2.2637, Val Loss: 2.1872, LR: 0.000017
2025-10-23 14:55:49,276 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 14:55:49,426 - INFO - Batch 0, Loss: 2.1907, LR: 0.000017
2025-10-23 14:56:03,321 - INFO - Batch 100, Loss: 2.4301, LR: 0.000017
2025-10-23 14:56:18,115 - INFO - Batch 200, Loss: 2.1877, LR: 0.000017
2025-10-23 14:56:32,568 - INFO - Batch 300, Loss: 2.2491, LR: 0.000017
2025-10-23 14:56:47,862 - INFO - Batch 400, Loss: 2.4665, LR: 0.000017
2025-10-23 14:57:02,692 - INFO - Batch 500, Loss: 1.9682, LR: 0.000017
2025-10-23 14:57:17,288 - INFO - Batch 600, Loss: 2.2566, LR: 0.000017
2025-10-23 14:57:32,250 - INFO - Batch 700, Loss: 2.2306, LR: 0.000016
2025-10-23 14:57:46,842 - INFO - Batch 800, Loss: 2.1587, LR: 0.000016
2025-10-23 14:58:01,649 - INFO - Batch 900, Loss: 2.3584, LR: 0.000016
2025-10-23 14:58:16,179 - INFO - Batch 1000, Loss: 2.1067, LR: 0.000016
2025-10-23 14:58:31,191 - INFO - Batch 1100, Loss: 2.1028, LR: 0.000016
2025-10-23 14:58:45,939 - INFO - Batch 1200, Loss: 2.2900, LR: 0.000016
2025-10-23 14:59:00,626 - INFO - Batch 1300, Loss: 1.9527, LR: 0.000016
2025-10-23 14:59:15,022 - INFO - Batch 1400, Loss: 2.4614, LR: 0.000016
2025-10-23 14:59:30,057 - INFO - Batch 1500, Loss: 2.3283, LR: 0.000016
2025-10-23 14:59:44,679 - INFO - Batch 1600, Loss: 2.2760, LR: 0.000016
2025-10-23 14:59:59,570 - INFO - Batch 1700, Loss: 2.1864, LR: 0.000016
2025-10-23 15:00:15,060 - INFO - Batch 1800, Loss: 2.3684, LR: 0.000016
2025-10-23 15:00:35,356 - INFO - Batch 1900, Loss: 2.1848, LR: 0.000016
2025-10-23 15:00:57,481 - INFO - Batch 2000, Loss: 2.2654, LR: 0.000016
2025-10-23 15:01:19,534 - INFO - Batch 2100, Loss: 2.3911, LR: 0.000016
2025-10-23 15:01:39,002 - INFO - Batch 2200, Loss: 1.9786, LR: 0.000016
2025-10-23 15:01:57,619 - INFO - Batch 2300, Loss: 1.9396, LR: 0.000015
2025-10-23 15:02:15,700 - INFO - Batch 2400, Loss: 2.2216, LR: 0.000015
2025-10-23 15:02:31,866 - INFO - Batch 2500, Loss: 2.3863, LR: 0.000015
2025-10-23 15:02:55,344 - INFO - Batch 2600, Loss: 1.8318, LR: 0.000015
2025-10-23 15:03:14,216 - INFO - Batch 2700, Loss: 2.0753, LR: 0.000015
2025-10-23 15:03:34,063 - INFO - Batch 2800, Loss: 2.5916, LR: 0.000015
2025-10-23 15:03:52,930 - INFO - Batch 2900, Loss: 2.2270, LR: 0.000015
2025-10-23 15:04:10,521 - INFO - Batch 3000, Loss: 2.2387, LR: 0.000015
2025-10-23 15:04:25,983 - INFO - Batch 3100, Loss: 2.0220, LR: 0.000015
2025-10-23 15:04:42,988 - INFO - Batch 3200, Loss: 2.3991, LR: 0.000015
2025-10-23 15:05:01,487 - INFO - Batch 3300, Loss: 2.2231, LR: 0.000015
2025-10-23 15:05:19,887 - INFO - Batch 3400, Loss: 2.3685, LR: 0.000015
2025-10-23 15:05:38,025 - INFO - Batch 3500, Loss: 2.2225, LR: 0.000015
2025-10-23 15:05:55,466 - INFO - Batch 3600, Loss: 2.2553, LR: 0.000015
2025-10-23 15:06:15,379 - INFO - Batch 3700, Loss: 2.1557, LR: 0.000015
2025-10-23 15:06:35,564 - INFO - Batch 3800, Loss: 2.2463, LR: 0.000015
2025-10-23 15:06:54,373 - INFO - Batch 3900, Loss: 2.2275, LR: 0.000015
2025-10-23 15:07:15,818 - INFO - Batch 4000, Loss: 2.4407, LR: 0.000014
2025-10-23 15:07:33,181 - INFO - Batch 4100, Loss: 2.6708, LR: 0.000014
2025-10-23 15:07:51,234 - INFO - Batch 4200, Loss: 2.2227, LR: 0.000014
2025-10-23 15:08:09,524 - INFO - Batch 4300, Loss: 2.3136, LR: 0.000014
2025-10-23 15:08:32,653 - INFO - Batch 4400, Loss: 2.3211, LR: 0.000014
2025-10-23 15:08:49,515 - INFO - Batch 4500, Loss: 2.3019, LR: 0.000014
2025-10-23 15:09:07,951 - INFO - Batch 4600, Loss: 2.3545, LR: 0.000014
2025-10-23 15:09:21,263 - INFO - Batch 4700, Loss: 2.2460, LR: 0.000014
2025-10-23 15:09:33,963 - INFO - Batch 4800, Loss: 2.0628, LR: 0.000014
2025-10-23 15:09:46,811 - INFO - Batch 4900, Loss: 2.2655, LR: 0.000014
2025-10-23 15:10:00,737 - INFO - Batch 5000, Loss: 2.2244, LR: 0.000014
2025-10-23 15:10:14,528 - INFO - Batch 5100, Loss: 2.2176, LR: 0.000014
2025-10-23 15:10:28,321 - INFO - Batch 5200, Loss: 2.3398, LR: 0.000014
2025-10-23 15:10:41,386 - INFO - Batch 5300, Loss: 2.2088, LR: 0.000014
2025-10-23 15:10:54,150 - INFO - Batch 5400, Loss: 2.2652, LR: 0.000014
2025-10-23 15:11:09,908 - INFO - Batch 5500, Loss: 2.2606, LR: 0.000014
2025-10-23 15:11:28,450 - INFO - Batch 5600, Loss: 1.9834, LR: 0.000014
2025-10-23 15:11:45,644 - INFO - Batch 5700, Loss: 2.1525, LR: 0.000014
2025-10-23 15:12:07,254 - INFO - Batch 5800, Loss: 2.2002, LR: 0.000013
2025-10-23 15:12:27,582 - INFO - Batch 5900, Loss: 2.2364, LR: 0.000013
2025-10-23 15:12:48,834 - INFO - Batch 6000, Loss: 2.1450, LR: 0.000013
2025-10-23 15:13:07,231 - INFO - Batch 6100, Loss: 2.3268, LR: 0.000013
2025-10-23 15:13:28,685 - INFO - Batch 6200, Loss: 2.3830, LR: 0.000013
2025-10-23 15:13:46,875 - INFO - Batch 6300, Loss: 2.1208, LR: 0.000013
2025-10-23 15:14:03,931 - INFO - Batch 6400, Loss: 2.2015, LR: 0.000013
2025-10-23 15:14:10,674 - INFO - Epoch 23/30: Train Loss: 2.2530, Val Loss: 2.1782, LR: 0.000013
2025-10-23 15:14:11,477 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 15:14:11,656 - INFO - Batch 0, Loss: 2.3291, LR: 0.000013
2025-10-23 15:14:30,683 - INFO - Batch 100, Loss: 2.0434, LR: 0.000013
2025-10-23 15:14:48,584 - INFO - Batch 200, Loss: 2.0844, LR: 0.000013
2025-10-23 15:15:04,780 - INFO - Batch 300, Loss: 2.0698, LR: 0.000013
2025-10-23 15:15:25,114 - INFO - Batch 400, Loss: 2.3078, LR: 0.000013
2025-10-23 15:15:44,795 - INFO - Batch 500, Loss: 2.0524, LR: 0.000013
2025-10-23 15:16:04,155 - INFO - Batch 600, Loss: 2.2108, LR: 0.000013
2025-10-23 15:16:26,697 - INFO - Batch 700, Loss: 2.0295, LR: 0.000013
2025-10-23 15:16:48,022 - INFO - Batch 800, Loss: 2.2328, LR: 0.000013
2025-10-23 15:17:05,119 - INFO - Batch 900, Loss: 2.1936, LR: 0.000013
2025-10-23 15:17:24,147 - INFO - Batch 1000, Loss: 2.0514, LR: 0.000013
2025-10-23 15:17:42,230 - INFO - Batch 1100, Loss: 2.4702, LR: 0.000012
2025-10-23 15:18:00,302 - INFO - Batch 1200, Loss: 2.2576, LR: 0.000012
2025-10-23 15:18:19,367 - INFO - Batch 1300, Loss: 2.2681, LR: 0.000012
2025-10-23 15:18:39,368 - INFO - Batch 1400, Loss: 2.1889, LR: 0.000012
2025-10-23 15:18:58,978 - INFO - Batch 1500, Loss: 2.0771, LR: 0.000012
2025-10-23 15:19:17,205 - INFO - Batch 1600, Loss: 2.0186, LR: 0.000012
2025-10-23 15:19:32,708 - INFO - Batch 1700, Loss: 2.1079, LR: 0.000012
2025-10-23 15:19:53,059 - INFO - Batch 1800, Loss: 2.1486, LR: 0.000012
2025-10-23 15:20:13,478 - INFO - Batch 1900, Loss: 2.1538, LR: 0.000012
2025-10-23 15:20:31,997 - INFO - Batch 2000, Loss: 2.1634, LR: 0.000012
2025-10-23 15:20:49,971 - INFO - Batch 2100, Loss: 2.3424, LR: 0.000012
2025-10-23 15:21:07,799 - INFO - Batch 2200, Loss: 2.3343, LR: 0.000012
2025-10-23 15:21:32,362 - INFO - Batch 2300, Loss: 2.3513, LR: 0.000012
2025-10-23 15:21:53,013 - INFO - Batch 2400, Loss: 2.1334, LR: 0.000012
2025-10-23 15:22:12,720 - INFO - Batch 2500, Loss: 2.0206, LR: 0.000012
2025-10-23 15:22:34,945 - INFO - Batch 2600, Loss: 2.2703, LR: 0.000012
2025-10-23 15:22:57,728 - INFO - Batch 2700, Loss: 2.2816, LR: 0.000012
2025-10-23 15:23:15,634 - INFO - Batch 2800, Loss: 2.1729, LR: 0.000012
2025-10-23 15:23:33,503 - INFO - Batch 2900, Loss: 1.9707, LR: 0.000012
2025-10-23 15:23:55,401 - INFO - Batch 3000, Loss: 2.3106, LR: 0.000011
2025-10-23 15:24:13,850 - INFO - Batch 3100, Loss: 2.3823, LR: 0.000011
2025-10-23 15:24:32,775 - INFO - Batch 3200, Loss: 2.4555, LR: 0.000011
2025-10-23 15:24:51,072 - INFO - Batch 3300, Loss: 2.2571, LR: 0.000011
2025-10-23 15:25:10,586 - INFO - Batch 3400, Loss: 2.0266, LR: 0.000011
2025-10-23 15:25:32,997 - INFO - Batch 3500, Loss: 2.3881, LR: 0.000011
2025-10-23 15:25:55,101 - INFO - Batch 3600, Loss: 1.8617, LR: 0.000011
2025-10-23 15:26:15,201 - INFO - Batch 3700, Loss: 2.2785, LR: 0.000011
2025-10-23 15:26:34,657 - INFO - Batch 3800, Loss: 2.4131, LR: 0.000011
2025-10-23 15:26:55,854 - INFO - Batch 3900, Loss: 2.1762, LR: 0.000011
2025-10-23 15:27:16,082 - INFO - Batch 4000, Loss: 2.0323, LR: 0.000011
2025-10-23 15:27:35,374 - INFO - Batch 4100, Loss: 2.3776, LR: 0.000011
2025-10-23 15:27:53,681 - INFO - Batch 4200, Loss: 2.1434, LR: 0.000011
2025-10-23 15:28:12,775 - INFO - Batch 4300, Loss: 2.3223, LR: 0.000011
2025-10-23 15:28:28,802 - INFO - Batch 4400, Loss: 2.0829, LR: 0.000011
2025-10-23 15:28:45,551 - INFO - Batch 4500, Loss: 2.3179, LR: 0.000011
2025-10-23 15:29:03,515 - INFO - Batch 4600, Loss: 2.2244, LR: 0.000011
2025-10-23 15:29:24,904 - INFO - Batch 4700, Loss: 2.0851, LR: 0.000011
2025-10-23 15:29:42,566 - INFO - Batch 4800, Loss: 2.0946, LR: 0.000011
2025-10-23 15:30:02,594 - INFO - Batch 4900, Loss: 2.2279, LR: 0.000011
2025-10-23 15:30:19,949 - INFO - Batch 5000, Loss: 2.1170, LR: 0.000010
2025-10-23 15:30:41,276 - INFO - Batch 5100, Loss: 2.1939, LR: 0.000010
2025-10-23 15:30:54,965 - INFO - Batch 5200, Loss: 2.1483, LR: 0.000010
2025-10-23 15:31:08,721 - INFO - Batch 5300, Loss: 2.3123, LR: 0.000010
2025-10-23 15:31:21,419 - INFO - Batch 5400, Loss: 2.1007, LR: 0.000010
2025-10-23 15:31:37,348 - INFO - Batch 5500, Loss: 2.1174, LR: 0.000010
2025-10-23 15:31:58,220 - INFO - Batch 5600, Loss: 2.1278, LR: 0.000010
2025-10-23 15:32:19,905 - INFO - Batch 5700, Loss: 2.3173, LR: 0.000010
2025-10-23 15:32:41,523 - INFO - Batch 5800, Loss: 2.3202, LR: 0.000010
2025-10-23 15:32:58,657 - INFO - Batch 5900, Loss: 2.4996, LR: 0.000010
2025-10-23 15:33:17,926 - INFO - Batch 6000, Loss: 2.1670, LR: 0.000010
2025-10-23 15:33:35,700 - INFO - Batch 6100, Loss: 2.6381, LR: 0.000010
2025-10-23 15:33:55,852 - INFO - Batch 6200, Loss: 2.2820, LR: 0.000010
2025-10-23 15:34:13,532 - INFO - Batch 6300, Loss: 2.2768, LR: 0.000010
2025-10-23 15:34:34,006 - INFO - Batch 6400, Loss: 2.1969, LR: 0.000010
2025-10-23 15:34:43,867 - INFO - Epoch 24/30: Train Loss: 2.2425, Val Loss: 2.1756, LR: 0.000010
2025-10-23 15:34:44,421 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 15:34:44,654 - INFO - Batch 0, Loss: 2.2383, LR: 0.000010
2025-10-23 15:35:06,234 - INFO - Batch 100, Loss: 2.1512, LR: 0.000010
2025-10-23 15:35:24,359 - INFO - Batch 200, Loss: 2.1931, LR: 0.000010
2025-10-23 15:35:44,902 - INFO - Batch 300, Loss: 2.3092, LR: 0.000010
2025-10-23 15:36:03,150 - INFO - Batch 400, Loss: 2.1905, LR: 0.000010
2025-10-23 15:36:24,476 - INFO - Batch 500, Loss: 2.2952, LR: 0.000010
2025-10-23 15:36:41,765 - INFO - Batch 600, Loss: 2.2868, LR: 0.000009
2025-10-23 15:36:59,768 - INFO - Batch 700, Loss: 2.2972, LR: 0.000009
2025-10-23 15:37:21,522 - INFO - Batch 800, Loss: 2.3124, LR: 0.000009
2025-10-23 15:37:40,178 - INFO - Batch 900, Loss: 2.1319, LR: 0.000009
2025-10-23 15:37:59,750 - INFO - Batch 1000, Loss: 2.2810, LR: 0.000009
2025-10-23 15:38:20,753 - INFO - Batch 1100, Loss: 2.2298, LR: 0.000009
2025-10-23 15:38:40,842 - INFO - Batch 1200, Loss: 2.3534, LR: 0.000009
2025-10-23 15:38:56,136 - INFO - Batch 1300, Loss: 2.3025, LR: 0.000009
2025-10-23 15:39:09,814 - INFO - Batch 1400, Loss: 2.0816, LR: 0.000009
2025-10-23 15:39:23,845 - INFO - Batch 1500, Loss: 2.2559, LR: 0.000009
2025-10-23 15:39:42,562 - INFO - Batch 1600, Loss: 2.2914, LR: 0.000009
2025-10-23 15:40:01,048 - INFO - Batch 1700, Loss: 2.3449, LR: 0.000009
2025-10-23 15:40:24,325 - INFO - Batch 1800, Loss: 2.4463, LR: 0.000009
2025-10-23 15:40:42,381 - INFO - Batch 1900, Loss: 2.4357, LR: 0.000009
2025-10-23 15:40:59,546 - INFO - Batch 2000, Loss: 2.3543, LR: 0.000009
2025-10-23 15:41:19,295 - INFO - Batch 2100, Loss: 2.0292, LR: 0.000009
2025-10-23 15:41:40,219 - INFO - Batch 2200, Loss: 2.3337, LR: 0.000009
2025-10-23 15:42:00,594 - INFO - Batch 2300, Loss: 2.5491, LR: 0.000009
2025-10-23 15:42:18,533 - INFO - Batch 2400, Loss: 2.2756, LR: 0.000009
2025-10-23 15:42:34,395 - INFO - Batch 2500, Loss: 2.3628, LR: 0.000009
2025-10-23 15:42:53,839 - INFO - Batch 2600, Loss: 2.0137, LR: 0.000009
2025-10-23 15:43:08,447 - INFO - Batch 2700, Loss: 2.3393, LR: 0.000008
2025-10-23 15:43:25,791 - INFO - Batch 2800, Loss: 2.4237, LR: 0.000008
2025-10-23 15:43:44,390 - INFO - Batch 2900, Loss: 2.1327, LR: 0.000008
2025-10-23 15:44:02,967 - INFO - Batch 3000, Loss: 2.2463, LR: 0.000008
2025-10-23 15:44:23,788 - INFO - Batch 3100, Loss: 2.1166, LR: 0.000008
2025-10-23 15:44:47,109 - INFO - Batch 3200, Loss: 2.3716, LR: 0.000008
2025-10-23 15:45:08,148 - INFO - Batch 3300, Loss: 1.9909, LR: 0.000008
2025-10-23 15:45:25,658 - INFO - Batch 3400, Loss: 2.2490, LR: 0.000008
2025-10-23 15:45:44,755 - INFO - Batch 3500, Loss: 2.4428, LR: 0.000008
2025-10-23 15:46:05,436 - INFO - Batch 3600, Loss: 2.2303, LR: 0.000008
2025-10-23 15:46:23,928 - INFO - Batch 3700, Loss: 2.3110, LR: 0.000008
2025-10-23 15:46:43,657 - INFO - Batch 3800, Loss: 2.0843, LR: 0.000008
2025-10-23 15:47:01,990 - INFO - Batch 3900, Loss: 2.4491, LR: 0.000008
2025-10-23 15:47:20,332 - INFO - Batch 4000, Loss: 2.1758, LR: 0.000008
2025-10-23 15:47:40,565 - INFO - Batch 4100, Loss: 2.2038, LR: 0.000008
2025-10-23 15:48:00,745 - INFO - Batch 4200, Loss: 2.2509, LR: 0.000008
2025-10-23 15:48:21,877 - INFO - Batch 4300, Loss: 2.3705, LR: 0.000008
2025-10-23 15:48:41,079 - INFO - Batch 4400, Loss: 2.0391, LR: 0.000008
2025-10-23 15:49:01,700 - INFO - Batch 4500, Loss: 2.2121, LR: 0.000008
2025-10-23 15:49:19,118 - INFO - Batch 4600, Loss: 2.1507, LR: 0.000008
2025-10-23 15:49:43,065 - INFO - Batch 4700, Loss: 2.6248, LR: 0.000008
2025-10-23 15:50:01,308 - INFO - Batch 4800, Loss: 2.6734, LR: 0.000008
2025-10-23 15:50:21,070 - INFO - Batch 4900, Loss: 2.2583, LR: 0.000007
2025-10-23 15:50:41,946 - INFO - Batch 5000, Loss: 2.2591, LR: 0.000007
2025-10-23 15:51:04,386 - INFO - Batch 5100, Loss: 2.3612, LR: 0.000007
2025-10-23 15:51:24,305 - INFO - Batch 5200, Loss: 2.3770, LR: 0.000007
2025-10-23 15:51:43,502 - INFO - Batch 5300, Loss: 2.4912, LR: 0.000007
2025-10-23 15:52:02,387 - INFO - Batch 5400, Loss: 2.0870, LR: 0.000007
2025-10-23 15:52:24,884 - INFO - Batch 5500, Loss: 2.0422, LR: 0.000007
2025-10-23 15:52:43,691 - INFO - Batch 5600, Loss: 2.2185, LR: 0.000007
2025-10-23 15:53:02,626 - INFO - Batch 5700, Loss: 2.0200, LR: 0.000007
2025-10-23 15:53:22,574 - INFO - Batch 5800, Loss: 2.4310, LR: 0.000007
2025-10-23 15:53:41,624 - INFO - Batch 5900, Loss: 2.1810, LR: 0.000007
2025-10-23 15:53:59,538 - INFO - Batch 6000, Loss: 2.1991, LR: 0.000007
2025-10-23 15:54:20,030 - INFO - Batch 6100, Loss: 2.2635, LR: 0.000007
2025-10-23 15:54:39,628 - INFO - Batch 6200, Loss: 2.4058, LR: 0.000007
2025-10-23 15:54:56,655 - INFO - Batch 6300, Loss: 2.0934, LR: 0.000007
2025-10-23 15:55:17,219 - INFO - Batch 6400, Loss: 2.2972, LR: 0.000007
2025-10-23 15:55:26,231 - INFO - Epoch 25/30: Train Loss: 2.2332, Val Loss: 2.1697, LR: 0.000007
2025-10-23 15:55:26,632 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 15:55:26,831 - INFO - Batch 0, Loss: 2.1354, LR: 0.000007
2025-10-23 15:55:45,116 - INFO - Batch 100, Loss: 2.3723, LR: 0.000007
2025-10-23 15:56:03,767 - INFO - Batch 200, Loss: 2.1780, LR: 0.000007
2025-10-23 15:56:22,365 - INFO - Batch 300, Loss: 2.0040, LR: 0.000007
2025-10-23 15:56:41,381 - INFO - Batch 400, Loss: 2.4415, LR: 0.000007
2025-10-23 15:57:06,758 - INFO - Batch 500, Loss: 2.0703, LR: 0.000007
2025-10-23 15:57:24,630 - INFO - Batch 600, Loss: 2.3914, LR: 0.000007
2025-10-23 15:57:43,906 - INFO - Batch 700, Loss: 2.1890, LR: 0.000007
2025-10-23 15:58:01,760 - INFO - Batch 800, Loss: 2.1156, LR: 0.000007
2025-10-23 15:58:18,990 - INFO - Batch 900, Loss: 2.1038, LR: 0.000006
2025-10-23 15:58:36,760 - INFO - Batch 1000, Loss: 2.1943, LR: 0.000006
2025-10-23 15:58:56,693 - INFO - Batch 1100, Loss: 2.4780, LR: 0.000006
2025-10-23 15:59:15,517 - INFO - Batch 1200, Loss: 2.1068, LR: 0.000006
2025-10-23 15:59:39,578 - INFO - Batch 1300, Loss: 2.1439, LR: 0.000006
2025-10-23 16:00:01,406 - INFO - Batch 1400, Loss: 2.4371, LR: 0.000006
2025-10-23 16:00:19,618 - INFO - Batch 1500, Loss: 2.3301, LR: 0.000006
2025-10-23 16:00:38,967 - INFO - Batch 1600, Loss: 2.2411, LR: 0.000006
2025-10-23 16:00:57,715 - INFO - Batch 1700, Loss: 2.2824, LR: 0.000006
2025-10-23 16:01:16,601 - INFO - Batch 1800, Loss: 2.1176, LR: 0.000006
2025-10-23 16:01:38,320 - INFO - Batch 1900, Loss: 2.0551, LR: 0.000006
2025-10-23 16:01:56,038 - INFO - Batch 2000, Loss: 2.3615, LR: 0.000006
2025-10-23 16:02:15,513 - INFO - Batch 2100, Loss: 2.1694, LR: 0.000006
2025-10-23 16:02:34,872 - INFO - Batch 2200, Loss: 2.0840, LR: 0.000006
2025-10-23 16:02:57,583 - INFO - Batch 2300, Loss: 2.2329, LR: 0.000006
2025-10-23 16:03:15,773 - INFO - Batch 2400, Loss: 2.0970, LR: 0.000006
2025-10-23 16:03:35,605 - INFO - Batch 2500, Loss: 2.3782, LR: 0.000006
2025-10-23 16:03:56,559 - INFO - Batch 2600, Loss: 2.2607, LR: 0.000006
2025-10-23 16:04:20,918 - INFO - Batch 2700, Loss: 2.0110, LR: 0.000006
2025-10-23 16:04:42,182 - INFO - Batch 2800, Loss: 2.2255, LR: 0.000006
2025-10-23 16:05:02,804 - INFO - Batch 2900, Loss: 2.3765, LR: 0.000006
2025-10-23 16:05:21,931 - INFO - Batch 3000, Loss: 2.0320, LR: 0.000006
2025-10-23 16:05:41,718 - INFO - Batch 3100, Loss: 2.3344, LR: 0.000006
2025-10-23 16:06:01,847 - INFO - Batch 3200, Loss: 2.3541, LR: 0.000006
2025-10-23 16:06:22,529 - INFO - Batch 3300, Loss: 2.2240, LR: 0.000006
2025-10-23 16:06:40,362 - INFO - Batch 3400, Loss: 2.0494, LR: 0.000005
2025-10-23 16:06:59,092 - INFO - Batch 3500, Loss: 2.3096, LR: 0.000005
2025-10-23 16:07:18,237 - INFO - Batch 3600, Loss: 2.3106, LR: 0.000005
2025-10-23 16:07:37,142 - INFO - Batch 3700, Loss: 2.3518, LR: 0.000005
2025-10-23 16:07:58,130 - INFO - Batch 3800, Loss: 2.2705, LR: 0.000005
2025-10-23 16:08:17,870 - INFO - Batch 3900, Loss: 2.0406, LR: 0.000005
2025-10-23 16:08:37,198 - INFO - Batch 4000, Loss: 2.2195, LR: 0.000005
2025-10-23 16:08:56,212 - INFO - Batch 4100, Loss: 2.2115, LR: 0.000005
2025-10-23 16:09:18,457 - INFO - Batch 4200, Loss: 2.2057, LR: 0.000005
2025-10-23 16:09:38,816 - INFO - Batch 4300, Loss: 2.3909, LR: 0.000005
2025-10-23 16:09:58,061 - INFO - Batch 4400, Loss: 2.3214, LR: 0.000005
2025-10-23 16:10:16,532 - INFO - Batch 4500, Loss: 2.0913, LR: 0.000005
2025-10-23 16:10:36,515 - INFO - Batch 4600, Loss: 2.1644, LR: 0.000005
2025-10-23 16:10:56,302 - INFO - Batch 4700, Loss: 1.9547, LR: 0.000005
2025-10-23 16:11:16,305 - INFO - Batch 4800, Loss: 2.3193, LR: 0.000005
2025-10-23 16:11:41,934 - INFO - Batch 4900, Loss: 2.2477, LR: 0.000005
2025-10-23 16:11:59,976 - INFO - Batch 5000, Loss: 2.2922, LR: 0.000005
2025-10-23 16:12:17,746 - INFO - Batch 5100, Loss: 2.1081, LR: 0.000005
2025-10-23 16:12:37,193 - INFO - Batch 5200, Loss: 2.1856, LR: 0.000005
2025-10-23 16:12:57,626 - INFO - Batch 5300, Loss: 2.4582, LR: 0.000005
2025-10-23 16:13:18,490 - INFO - Batch 5400, Loss: 2.3823, LR: 0.000005
2025-10-23 16:13:38,543 - INFO - Batch 5500, Loss: 2.3136, LR: 0.000005
2025-10-23 16:13:59,235 - INFO - Batch 5600, Loss: 2.0424, LR: 0.000005
2025-10-23 16:14:20,147 - INFO - Batch 5700, Loss: 2.1139, LR: 0.000005
2025-10-23 16:14:44,336 - INFO - Batch 5800, Loss: 2.2581, LR: 0.000005
2025-10-23 16:15:04,014 - INFO - Batch 5900, Loss: 2.1905, LR: 0.000005
2025-10-23 16:15:22,302 - INFO - Batch 6000, Loss: 2.2839, LR: 0.000005
2025-10-23 16:15:41,749 - INFO - Batch 6100, Loss: 1.8755, LR: 0.000005
2025-10-23 16:16:04,077 - INFO - Batch 6200, Loss: 2.1714, LR: 0.000004
2025-10-23 16:16:24,158 - INFO - Batch 6300, Loss: 2.1244, LR: 0.000004
2025-10-23 16:16:42,632 - INFO - Batch 6400, Loss: 2.4092, LR: 0.000004
2025-10-23 16:16:50,981 - INFO - Epoch 26/30: Train Loss: 2.2257, Val Loss: 2.1671, LR: 0.000004
2025-10-23 16:16:51,394 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 16:16:51,580 - INFO - Batch 0, Loss: 1.9545, LR: 0.000004
2025-10-23 16:17:11,224 - INFO - Batch 100, Loss: 2.2390, LR: 0.000004
2025-10-23 16:17:30,113 - INFO - Batch 200, Loss: 2.2309, LR: 0.000004
2025-10-23 16:17:49,455 - INFO - Batch 300, Loss: 2.0217, LR: 0.000004
2025-10-23 16:18:09,458 - INFO - Batch 400, Loss: 2.4582, LR: 0.000004
2025-10-23 16:18:27,074 - INFO - Batch 500, Loss: 2.3498, LR: 0.000004
2025-10-23 16:18:47,617 - INFO - Batch 600, Loss: 2.2010, LR: 0.000004
2025-10-23 16:19:09,388 - INFO - Batch 700, Loss: 2.0925, LR: 0.000004
2025-10-23 16:19:33,303 - INFO - Batch 800, Loss: 2.2969, LR: 0.000004
2025-10-23 16:19:53,471 - INFO - Batch 900, Loss: 2.2023, LR: 0.000004
2025-10-23 16:20:11,736 - INFO - Batch 1000, Loss: 2.1559, LR: 0.000004
2025-10-23 16:20:29,395 - INFO - Batch 1100, Loss: 2.1422, LR: 0.000004
2025-10-23 16:20:52,852 - INFO - Batch 1200, Loss: 1.8935, LR: 0.000004
2025-10-23 16:21:10,883 - INFO - Batch 1300, Loss: 2.3415, LR: 0.000004
2025-10-23 16:21:29,064 - INFO - Batch 1400, Loss: 2.1044, LR: 0.000004
2025-10-23 16:21:48,719 - INFO - Batch 1500, Loss: 2.2792, LR: 0.000004
2025-10-23 16:22:08,224 - INFO - Batch 1600, Loss: 2.3554, LR: 0.000004
2025-10-23 16:22:26,041 - INFO - Batch 1700, Loss: 2.4612, LR: 0.000004
2025-10-23 16:22:49,231 - INFO - Batch 1800, Loss: 2.1875, LR: 0.000004
2025-10-23 16:23:05,758 - INFO - Batch 1900, Loss: 2.4586, LR: 0.000004
2025-10-23 16:23:25,202 - INFO - Batch 2000, Loss: 2.0844, LR: 0.000004
2025-10-23 16:23:45,488 - INFO - Batch 2100, Loss: 2.1862, LR: 0.000004
2025-10-23 16:24:06,211 - INFO - Batch 2200, Loss: 2.2221, LR: 0.000004
2025-10-23 16:24:26,642 - INFO - Batch 2300, Loss: 2.2878, LR: 0.000004
2025-10-23 16:24:45,928 - INFO - Batch 2400, Loss: 2.2565, LR: 0.000004
2025-10-23 16:25:07,122 - INFO - Batch 2500, Loss: 2.4442, LR: 0.000004
2025-10-23 16:25:27,756 - INFO - Batch 2600, Loss: 1.9448, LR: 0.000004
2025-10-23 16:25:48,309 - INFO - Batch 2700, Loss: 2.1007, LR: 0.000004
2025-10-23 16:26:07,867 - INFO - Batch 2800, Loss: 2.2685, LR: 0.000004
2025-10-23 16:26:27,628 - INFO - Batch 2900, Loss: 2.0541, LR: 0.000003
2025-10-23 16:26:51,985 - INFO - Batch 3000, Loss: 2.4404, LR: 0.000003
2025-10-23 16:27:09,380 - INFO - Batch 3100, Loss: 2.5614, LR: 0.000003
2025-10-23 16:27:28,383 - INFO - Batch 3200, Loss: 2.2814, LR: 0.000003
2025-10-23 16:27:48,390 - INFO - Batch 3300, Loss: 1.9535, LR: 0.000003
2025-10-23 16:28:06,909 - INFO - Batch 3400, Loss: 2.1780, LR: 0.000003
2025-10-23 16:28:25,292 - INFO - Batch 3500, Loss: 2.2169, LR: 0.000003
2025-10-23 16:28:49,908 - INFO - Batch 3600, Loss: 2.3506, LR: 0.000003
2025-10-23 16:29:10,433 - INFO - Batch 3700, Loss: 2.1536, LR: 0.000003
2025-10-23 16:29:29,569 - INFO - Batch 3800, Loss: 2.4333, LR: 0.000003
2025-10-23 16:29:47,602 - INFO - Batch 3900, Loss: 2.4595, LR: 0.000003
2025-10-23 16:30:04,836 - INFO - Batch 4000, Loss: 2.3248, LR: 0.000003
2025-10-23 16:30:25,221 - INFO - Batch 4100, Loss: 2.3834, LR: 0.000003
2025-10-23 16:30:43,827 - INFO - Batch 4200, Loss: 2.2812, LR: 0.000003
2025-10-23 16:31:03,641 - INFO - Batch 4300, Loss: 2.2313, LR: 0.000003
2025-10-23 16:31:21,342 - INFO - Batch 4400, Loss: 2.2081, LR: 0.000003
2025-10-23 16:31:39,598 - INFO - Batch 4500, Loss: 2.1977, LR: 0.000003
2025-10-23 16:32:01,570 - INFO - Batch 4600, Loss: 2.2104, LR: 0.000003
2025-10-23 16:32:21,298 - INFO - Batch 4700, Loss: 2.2342, LR: 0.000003
2025-10-23 16:32:39,319 - INFO - Batch 4800, Loss: 2.3355, LR: 0.000003
2025-10-23 16:32:59,232 - INFO - Batch 4900, Loss: 2.0979, LR: 0.000003
2025-10-23 16:33:18,348 - INFO - Batch 5000, Loss: 2.0182, LR: 0.000003
2025-10-23 16:33:34,016 - INFO - Batch 5100, Loss: 1.9479, LR: 0.000003
2025-10-23 16:33:53,338 - INFO - Batch 5200, Loss: 2.2757, LR: 0.000003
2025-10-23 16:34:13,948 - INFO - Batch 5300, Loss: 2.4067, LR: 0.000003
2025-10-23 16:34:33,090 - INFO - Batch 5400, Loss: 2.0576, LR: 0.000003
2025-10-23 16:34:54,278 - INFO - Batch 5500, Loss: 2.0983, LR: 0.000003
2025-10-23 16:35:13,888 - INFO - Batch 5600, Loss: 2.3888, LR: 0.000003
2025-10-23 16:35:35,458 - INFO - Batch 5700, Loss: 2.2011, LR: 0.000003
2025-10-23 16:35:55,639 - INFO - Batch 5800, Loss: 2.2740, LR: 0.000003
2025-10-23 16:36:12,631 - INFO - Batch 5900, Loss: 2.3904, LR: 0.000003
2025-10-23 16:36:31,591 - INFO - Batch 6000, Loss: 2.0984, LR: 0.000003
2025-10-23 16:36:50,403 - INFO - Batch 6100, Loss: 2.1793, LR: 0.000003
2025-10-23 16:37:12,048 - INFO - Batch 6200, Loss: 2.3165, LR: 0.000003
2025-10-23 16:37:31,605 - INFO - Batch 6300, Loss: 2.3655, LR: 0.000003
2025-10-23 16:37:52,170 - INFO - Batch 6400, Loss: 2.2237, LR: 0.000003
2025-10-23 16:38:00,811 - INFO - Epoch 27/30: Train Loss: 2.2184, Val Loss: 2.1634, LR: 0.000002
2025-10-23 16:38:01,503 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 16:38:01,713 - INFO - Batch 0, Loss: 1.9803, LR: 0.000002
2025-10-23 16:38:21,288 - INFO - Batch 100, Loss: 2.3707, LR: 0.000002
2025-10-23 16:38:38,805 - INFO - Batch 200, Loss: 2.2778, LR: 0.000002
2025-10-23 16:38:53,723 - INFO - Batch 300, Loss: 2.2991, LR: 0.000002
2025-10-23 16:39:16,963 - INFO - Batch 400, Loss: 2.2113, LR: 0.000002
2025-10-23 16:39:38,356 - INFO - Batch 500, Loss: 2.2772, LR: 0.000002
2025-10-23 16:39:57,543 - INFO - Batch 600, Loss: 2.2075, LR: 0.000002
2025-10-23 16:40:18,359 - INFO - Batch 700, Loss: 2.1530, LR: 0.000002
2025-10-23 16:40:35,310 - INFO - Batch 800, Loss: 2.2731, LR: 0.000002
2025-10-23 16:40:52,371 - INFO - Batch 900, Loss: 2.2524, LR: 0.000002
2025-10-23 16:41:15,858 - INFO - Batch 1000, Loss: 2.3594, LR: 0.000002
2025-10-23 16:41:35,102 - INFO - Batch 1100, Loss: 2.2785, LR: 0.000002
2025-10-23 16:41:56,624 - INFO - Batch 1200, Loss: 2.2618, LR: 0.000002
2025-10-23 16:42:20,047 - INFO - Batch 1300, Loss: 2.4178, LR: 0.000002
2025-10-23 16:42:40,106 - INFO - Batch 1400, Loss: 2.0301, LR: 0.000002
2025-10-23 16:42:57,421 - INFO - Batch 1500, Loss: 2.2214, LR: 0.000002
2025-10-23 16:43:17,815 - INFO - Batch 1600, Loss: 2.0559, LR: 0.000002
2025-10-23 16:43:34,198 - INFO - Batch 1700, Loss: 2.2113, LR: 0.000002
2025-10-23 16:43:54,869 - INFO - Batch 1800, Loss: 2.1912, LR: 0.000002
2025-10-23 16:44:15,006 - INFO - Batch 1900, Loss: 1.9052, LR: 0.000002
2025-10-23 16:44:34,213 - INFO - Batch 2000, Loss: 2.1640, LR: 0.000002
2025-10-23 16:44:53,453 - INFO - Batch 2100, Loss: 2.0441, LR: 0.000002
2025-10-23 16:45:12,470 - INFO - Batch 2200, Loss: 2.3162, LR: 0.000002
2025-10-23 16:45:29,899 - INFO - Batch 2300, Loss: 2.3491, LR: 0.000002
2025-10-23 16:45:48,655 - INFO - Batch 2400, Loss: 2.2822, LR: 0.000002
2025-10-23 16:46:07,362 - INFO - Batch 2500, Loss: 2.2245, LR: 0.000002
2025-10-23 16:46:25,298 - INFO - Batch 2600, Loss: 1.9051, LR: 0.000002
2025-10-23 16:46:43,596 - INFO - Batch 2700, Loss: 2.0678, LR: 0.000002
2025-10-23 16:47:03,049 - INFO - Batch 2800, Loss: 2.2566, LR: 0.000002
2025-10-23 16:47:22,947 - INFO - Batch 2900, Loss: 2.0331, LR: 0.000002
2025-10-23 16:47:40,568 - INFO - Batch 3000, Loss: 2.2552, LR: 0.000002
2025-10-23 16:47:58,353 - INFO - Batch 3100, Loss: 2.0473, LR: 0.000002
2025-10-23 16:48:18,196 - INFO - Batch 3200, Loss: 1.9501, LR: 0.000002
2025-10-23 16:48:37,730 - INFO - Batch 3300, Loss: 2.1857, LR: 0.000002
2025-10-23 16:48:56,681 - INFO - Batch 3400, Loss: 2.1026, LR: 0.000002
2025-10-23 16:49:15,725 - INFO - Batch 3500, Loss: 2.0338, LR: 0.000002
2025-10-23 16:49:36,077 - INFO - Batch 3600, Loss: 2.4184, LR: 0.000002
2025-10-23 16:49:52,177 - INFO - Batch 3700, Loss: 2.1402, LR: 0.000002
2025-10-23 16:50:10,764 - INFO - Batch 3800, Loss: 2.1253, LR: 0.000002
2025-10-23 16:50:32,544 - INFO - Batch 3900, Loss: 2.4470, LR: 0.000002
2025-10-23 16:50:50,523 - INFO - Batch 4000, Loss: 2.2636, LR: 0.000002
2025-10-23 16:51:16,782 - INFO - Batch 4100, Loss: 2.1411, LR: 0.000002
2025-10-23 16:51:37,216 - INFO - Batch 4200, Loss: 1.9901, LR: 0.000002
2025-10-23 16:51:56,823 - INFO - Batch 4300, Loss: 2.2107, LR: 0.000002
2025-10-23 16:52:18,826 - INFO - Batch 4400, Loss: 1.9085, LR: 0.000001
2025-10-23 16:52:40,814 - INFO - Batch 4500, Loss: 1.9753, LR: 0.000001
2025-10-23 16:53:01,762 - INFO - Batch 4600, Loss: 2.0256, LR: 0.000001
2025-10-23 16:53:21,268 - INFO - Batch 4700, Loss: 2.2783, LR: 0.000001
2025-10-23 16:53:40,528 - INFO - Batch 4800, Loss: 2.1347, LR: 0.000001
2025-10-23 16:53:59,676 - INFO - Batch 4900, Loss: 2.2734, LR: 0.000001
2025-10-23 16:54:20,353 - INFO - Batch 5000, Loss: 2.5125, LR: 0.000001
2025-10-23 16:54:39,107 - INFO - Batch 5100, Loss: 2.4639, LR: 0.000001
2025-10-23 16:54:59,189 - INFO - Batch 5200, Loss: 2.5904, LR: 0.000001
2025-10-23 16:55:22,393 - INFO - Batch 5300, Loss: 2.4545, LR: 0.000001
2025-10-23 16:55:40,820 - INFO - Batch 5400, Loss: 2.0789, LR: 0.000001
2025-10-23 16:55:59,831 - INFO - Batch 5500, Loss: 2.0880, LR: 0.000001
2025-10-23 16:56:19,217 - INFO - Batch 5600, Loss: 2.3178, LR: 0.000001
2025-10-23 16:56:38,982 - INFO - Batch 5700, Loss: 2.2186, LR: 0.000001
2025-10-23 16:56:59,376 - INFO - Batch 5800, Loss: 2.0591, LR: 0.000001
2025-10-23 16:57:22,450 - INFO - Batch 5900, Loss: 2.0239, LR: 0.000001
2025-10-23 16:57:40,416 - INFO - Batch 6000, Loss: 2.2072, LR: 0.000001
2025-10-23 16:58:00,412 - INFO - Batch 6100, Loss: 2.2741, LR: 0.000001
2025-10-23 16:58:21,610 - INFO - Batch 6200, Loss: 2.2778, LR: 0.000001
2025-10-23 16:58:43,256 - INFO - Batch 6300, Loss: 2.1374, LR: 0.000001
2025-10-23 16:59:01,014 - INFO - Batch 6400, Loss: 2.4333, LR: 0.000001
2025-10-23 16:59:09,600 - INFO - Epoch 28/30: Train Loss: 2.2130, Val Loss: 2.1642, LR: 0.000001
2025-10-23 16:59:09,802 - INFO - Batch 0, Loss: 2.0148, LR: 0.000001
2025-10-23 16:59:29,084 - INFO - Batch 100, Loss: 2.2497, LR: 0.000001
2025-10-23 16:59:46,799 - INFO - Batch 200, Loss: 2.1299, LR: 0.000001
2025-10-23 17:00:08,459 - INFO - Batch 300, Loss: 1.9890, LR: 0.000001
2025-10-23 17:00:26,883 - INFO - Batch 400, Loss: 2.3071, LR: 0.000001
2025-10-23 17:00:46,143 - INFO - Batch 500, Loss: 2.1444, LR: 0.000001
2025-10-23 17:01:03,987 - INFO - Batch 600, Loss: 2.1081, LR: 0.000001
2025-10-23 17:01:27,257 - INFO - Batch 700, Loss: 2.1875, LR: 0.000001
2025-10-23 17:01:44,187 - INFO - Batch 800, Loss: 2.1089, LR: 0.000001
2025-10-23 17:02:00,124 - INFO - Batch 900, Loss: 2.1042, LR: 0.000001
2025-10-23 17:02:18,495 - INFO - Batch 1000, Loss: 2.3143, LR: 0.000001
2025-10-23 17:02:36,636 - INFO - Batch 1100, Loss: 2.1716, LR: 0.000001
2025-10-23 17:02:55,554 - INFO - Batch 1200, Loss: 2.2114, LR: 0.000001
2025-10-23 17:03:13,083 - INFO - Batch 1300, Loss: 2.3273, LR: 0.000001
2025-10-23 17:03:33,829 - INFO - Batch 1400, Loss: 2.3464, LR: 0.000001
2025-10-23 17:03:53,410 - INFO - Batch 1500, Loss: 2.0855, LR: 0.000001
2025-10-23 17:04:12,185 - INFO - Batch 1600, Loss: 2.0435, LR: 0.000001
2025-10-23 17:04:31,622 - INFO - Batch 1700, Loss: 2.3206, LR: 0.000001
2025-10-23 17:04:49,672 - INFO - Batch 1800, Loss: 2.1123, LR: 0.000001
2025-10-23 17:05:08,343 - INFO - Batch 1900, Loss: 2.4419, LR: 0.000001
2025-10-23 17:05:28,450 - INFO - Batch 2000, Loss: 2.1632, LR: 0.000001
2025-10-23 17:05:50,530 - INFO - Batch 2100, Loss: 2.2019, LR: 0.000001
2025-10-23 17:06:09,886 - INFO - Batch 2200, Loss: 2.1904, LR: 0.000001
2025-10-23 17:06:30,417 - INFO - Batch 2300, Loss: 2.3840, LR: 0.000001
2025-10-23 17:06:49,460 - INFO - Batch 2400, Loss: 1.9092, LR: 0.000001
2025-10-23 17:07:07,211 - INFO - Batch 2500, Loss: 2.2924, LR: 0.000001
2025-10-23 17:07:26,446 - INFO - Batch 2600, Loss: 2.1447, LR: 0.000001
2025-10-23 17:07:43,394 - INFO - Batch 2700, Loss: 2.1244, LR: 0.000001
2025-10-23 17:08:01,671 - INFO - Batch 2800, Loss: 1.9951, LR: 0.000001
2025-10-23 17:08:21,642 - INFO - Batch 2900, Loss: 2.0616, LR: 0.000001
2025-10-23 17:08:40,888 - INFO - Batch 3000, Loss: 2.3638, LR: 0.000001
2025-10-23 17:08:59,591 - INFO - Batch 3100, Loss: 2.1951, LR: 0.000001
2025-10-23 17:09:22,158 - INFO - Batch 3200, Loss: 2.0785, LR: 0.000001
2025-10-23 17:09:40,248 - INFO - Batch 3300, Loss: 2.0811, LR: 0.000001
2025-10-23 17:10:00,561 - INFO - Batch 3400, Loss: 1.9740, LR: 0.000001
2025-10-23 17:10:17,914 - INFO - Batch 3500, Loss: 2.1302, LR: 0.000001
2025-10-23 17:10:38,710 - INFO - Batch 3600, Loss: 2.1153, LR: 0.000001
2025-10-23 17:10:56,884 - INFO - Batch 3700, Loss: 1.9315, LR: 0.000001
2025-10-23 17:11:10,445 - INFO - Batch 3800, Loss: 2.3089, LR: 0.000001
2025-10-23 17:11:30,067 - INFO - Batch 3900, Loss: 2.1525, LR: 0.000001
2025-10-23 17:11:48,866 - INFO - Batch 4000, Loss: 2.1665, LR: 0.000001
2025-10-23 17:12:07,652 - INFO - Batch 4100, Loss: 2.3723, LR: 0.000001
2025-10-23 17:12:26,217 - INFO - Batch 4200, Loss: 2.2687, LR: 0.000001
2025-10-23 17:12:43,834 - INFO - Batch 4300, Loss: 2.1956, LR: 0.000000
2025-10-23 17:13:04,006 - INFO - Batch 4400, Loss: 2.1680, LR: 0.000000
2025-10-23 17:13:22,585 - INFO - Batch 4500, Loss: 2.4202, LR: 0.000000
2025-10-23 17:13:44,032 - INFO - Batch 4600, Loss: 2.1420, LR: 0.000000
2025-10-23 17:14:04,318 - INFO - Batch 4700, Loss: 2.1155, LR: 0.000000
2025-10-23 17:14:21,769 - INFO - Batch 4800, Loss: 2.2452, LR: 0.000000
2025-10-23 17:14:40,718 - INFO - Batch 4900, Loss: 2.1071, LR: 0.000000
2025-10-23 17:14:59,467 - INFO - Batch 5000, Loss: 2.2303, LR: 0.000000
2025-10-23 17:15:19,412 - INFO - Batch 5100, Loss: 2.3497, LR: 0.000000
2025-10-23 17:15:41,040 - INFO - Batch 5200, Loss: 2.0294, LR: 0.000000
2025-10-23 17:15:55,565 - INFO - Batch 5300, Loss: 2.1357, LR: 0.000000
2025-10-23 17:16:15,200 - INFO - Batch 5400, Loss: 2.3191, LR: 0.000000
2025-10-23 17:16:32,688 - INFO - Batch 5500, Loss: 2.3258, LR: 0.000000
2025-10-23 17:16:58,672 - INFO - Batch 5600, Loss: 1.9850, LR: 0.000000
2025-10-23 17:17:17,260 - INFO - Batch 5700, Loss: 2.1660, LR: 0.000000
2025-10-23 17:17:39,149 - INFO - Batch 5800, Loss: 2.3013, LR: 0.000000
2025-10-23 17:17:57,604 - INFO - Batch 5900, Loss: 2.2354, LR: 0.000000
2025-10-23 17:18:15,005 - INFO - Batch 6000, Loss: 2.0556, LR: 0.000000
2025-10-23 17:18:35,903 - INFO - Batch 6100, Loss: 2.2025, LR: 0.000000
2025-10-23 17:18:56,110 - INFO - Batch 6200, Loss: 2.1564, LR: 0.000000
2025-10-23 17:19:11,314 - INFO - Batch 6300, Loss: 2.0969, LR: 0.000000
2025-10-23 17:19:30,695 - INFO - Batch 6400, Loss: 2.3099, LR: 0.000000
2025-10-23 17:19:40,234 - INFO - Epoch 29/30: Train Loss: 2.2089, Val Loss: 2.1641, LR: 0.000000
2025-10-23 17:19:40,426 - INFO - Batch 0, Loss: 2.1781, LR: 0.000000
2025-10-23 17:19:58,137 - INFO - Batch 100, Loss: 2.0110, LR: 0.000000
2025-10-23 17:20:15,234 - INFO - Batch 200, Loss: 2.4531, LR: 0.000000
2025-10-23 17:20:34,981 - INFO - Batch 300, Loss: 2.3045, LR: 0.000000
2025-10-23 17:20:54,973 - INFO - Batch 400, Loss: 2.4556, LR: 0.000000
2025-10-23 17:21:19,439 - INFO - Batch 500, Loss: 2.1685, LR: 0.000000
2025-10-23 17:21:40,678 - INFO - Batch 600, Loss: 2.4077, LR: 0.000000
2025-10-23 17:21:58,222 - INFO - Batch 700, Loss: 2.0389, LR: 0.000000
2025-10-23 17:22:17,875 - INFO - Batch 800, Loss: 2.1970, LR: 0.000000
2025-10-23 17:22:35,151 - INFO - Batch 900, Loss: 2.2970, LR: 0.000000
2025-10-23 17:22:54,902 - INFO - Batch 1000, Loss: 2.1695, LR: 0.000000
2025-10-23 17:23:13,406 - INFO - Batch 1100, Loss: 2.1556, LR: 0.000000
2025-10-23 17:23:33,711 - INFO - Batch 1200, Loss: 2.1666, LR: 0.000000
2025-10-23 17:23:53,306 - INFO - Batch 1300, Loss: 2.3926, LR: 0.000000
2025-10-23 17:24:14,831 - INFO - Batch 1400, Loss: 2.1111, LR: 0.000000
2025-10-23 17:24:33,362 - INFO - Batch 1500, Loss: 2.1606, LR: 0.000000
2025-10-23 17:24:52,651 - INFO - Batch 1600, Loss: 2.1258, LR: 0.000000
2025-10-23 17:25:13,175 - INFO - Batch 1700, Loss: 2.1675, LR: 0.000000
2025-10-23 17:25:30,381 - INFO - Batch 1800, Loss: 2.1459, LR: 0.000000
2025-10-23 17:25:47,895 - INFO - Batch 1900, Loss: 2.1137, LR: 0.000000
2025-10-23 17:26:07,961 - INFO - Batch 2000, Loss: 2.2537, LR: 0.000000
2025-10-23 17:26:27,492 - INFO - Batch 2100, Loss: 2.2980, LR: 0.000000
2025-10-23 17:26:44,601 - INFO - Batch 2200, Loss: 1.9838, LR: 0.000000
2025-10-23 17:27:02,841 - INFO - Batch 2300, Loss: 1.9797, LR: 0.000000
2025-10-23 17:27:25,452 - INFO - Batch 2400, Loss: 2.0880, LR: 0.000000
2025-10-23 17:27:44,404 - INFO - Batch 2500, Loss: 2.2499, LR: 0.000000
2025-10-23 17:28:06,474 - INFO - Batch 2600, Loss: 1.9069, LR: 0.000000
2025-10-23 17:28:25,738 - INFO - Batch 2700, Loss: 2.0378, LR: 0.000000
2025-10-23 17:28:44,474 - INFO - Batch 2800, Loss: 2.3486, LR: 0.000000
2025-10-23 17:29:02,822 - INFO - Batch 2900, Loss: 2.2579, LR: 0.000000
2025-10-23 17:29:23,062 - INFO - Batch 3000, Loss: 2.4520, LR: 0.000000
2025-10-23 17:29:47,899 - INFO - Batch 3100, Loss: 1.9484, LR: 0.000000
2025-10-23 17:30:08,164 - INFO - Batch 3200, Loss: 2.2124, LR: 0.000000
2025-10-23 17:30:26,518 - INFO - Batch 3300, Loss: 2.0186, LR: 0.000000
2025-10-23 17:30:42,582 - INFO - Batch 3400, Loss: 2.1839, LR: 0.000000
2025-10-23 17:30:55,199 - INFO - Batch 3500, Loss: 2.1755, LR: 0.000000
2025-10-23 17:31:08,876 - INFO - Batch 3600, Loss: 2.4194, LR: 0.000000
2025-10-23 17:31:26,703 - INFO - Batch 3700, Loss: 2.0952, LR: 0.000000
2025-10-23 17:31:49,122 - INFO - Batch 3800, Loss: 2.2523, LR: 0.000000
2025-10-23 17:32:11,671 - INFO - Batch 3900, Loss: 2.2270, LR: 0.000000
2025-10-23 17:32:36,939 - INFO - Batch 4000, Loss: 2.0895, LR: 0.000000
2025-10-23 17:32:54,581 - INFO - Batch 4100, Loss: 2.1481, LR: 0.000000
2025-10-23 17:33:14,757 - INFO - Batch 4200, Loss: 2.0394, LR: 0.000000
2025-10-23 17:33:33,565 - INFO - Batch 4300, Loss: 2.2901, LR: 0.000000
2025-10-23 17:33:50,595 - INFO - Batch 4400, Loss: 2.5216, LR: 0.000000
2025-10-23 17:34:08,709 - INFO - Batch 4500, Loss: 2.2764, LR: 0.000000
2025-10-23 17:34:24,810 - INFO - Batch 4600, Loss: 2.3334, LR: 0.000000
2025-10-23 17:34:44,124 - INFO - Batch 4700, Loss: 2.0840, LR: 0.000000
2025-10-23 17:35:00,879 - INFO - Batch 4800, Loss: 2.1169, LR: 0.000000
2025-10-23 17:35:21,027 - INFO - Batch 4900, Loss: 2.1717, LR: 0.000000
2025-10-23 17:35:38,630 - INFO - Batch 5000, Loss: 2.2709, LR: 0.000000
2025-10-23 17:35:55,037 - INFO - Batch 5100, Loss: 2.2205, LR: 0.000000
2025-10-23 17:36:14,004 - INFO - Batch 5200, Loss: 2.2265, LR: 0.000000
2025-10-23 17:36:32,450 - INFO - Batch 5300, Loss: 2.0624, LR: 0.000000
2025-10-23 17:36:53,603 - INFO - Batch 5400, Loss: 2.2639, LR: 0.000000
2025-10-23 17:37:13,052 - INFO - Batch 5500, Loss: 2.1202, LR: 0.000000
2025-10-23 17:37:33,363 - INFO - Batch 5600, Loss: 2.0804, LR: 0.000000
2025-10-23 17:37:57,095 - INFO - Batch 5700, Loss: 2.2146, LR: 0.000000
2025-10-23 17:38:18,197 - INFO - Batch 5800, Loss: 2.2250, LR: 0.000000
2025-10-23 17:38:37,991 - INFO - Batch 5900, Loss: 2.2070, LR: 0.000000
2025-10-23 17:38:58,830 - INFO - Batch 6000, Loss: 2.3069, LR: 0.000000
2025-10-23 17:39:17,368 - INFO - Batch 6100, Loss: 2.1270, LR: 0.000000
2025-10-23 17:39:36,056 - INFO - Batch 6200, Loss: 2.2850, LR: 0.000000
2025-10-23 17:39:52,057 - INFO - Batch 6300, Loss: 2.4622, LR: 0.000000
2025-10-23 17:40:12,248 - INFO - Batch 6400, Loss: 2.0483, LR: 0.000000
2025-10-23 17:40:21,246 - INFO - Epoch 30/30: Train Loss: 2.2070, Val Loss: 2.1634, LR: 0.000000
2025-10-23 17:40:21,556 - INFO - 模型已保存到: ./checkpoints/best_model.pth
2025-10-23 17:40:22,285 - INFO - 模型已保存到: ./checkpoints/checkpoint_epoch_30.pth
2025-10-23 17:40:22,526 - INFO - 模型已保存到: ./checkpoints/final_model.pth
2025-10-23 17:40:23,736 - INFO - 训练完成!
